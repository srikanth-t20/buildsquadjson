{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Roll#2019900090 Project (BERT fine-tuned using Tensorflow 2.0 model) ",
      "provenance": [],
      "collapsed_sections": [
        "PNq9e4OS6cRf",
        "YzECrzeIYb9q"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t4PVfgAGuida",
        "colab_type": "text"
      },
      "source": [
        "# **This Google colab notebook is the project work of:**\n",
        "\n",
        "> **Srikanth Thirumalasetti**, **Roll# 2019900090**, **PGSSP Student, IIITH, Gachibowli, Hyderabad**\n",
        "\n",
        "> **Course: CSE 573 (Spring 2020) - Natural Language Processing Applications (NLA)** "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pmlNNku3vHgX",
        "colab_type": "text"
      },
      "source": [
        "# **About the project**\n",
        "*   The project is a pilot implementation of BERT (*Base, Uncased, 12-layer, 768-hidden, 12-heads, 110M parameters*) for a closed-domain Q&A system.\n",
        "*   The above pre-trained BERT model is fine-tuned on SQuAD 1.1 dataset.\n",
        "*   Evaluation of the model is done by running SQuAD v1.1 evaluation script that compares the predictions made by the fine-tuned model and the SQuAD dev set for evaluation.\n",
        "*   Additionally, an external test dataset used is from the textual content from Proxzar.\n",
        "*   A *qualititative* comparision of the current implementation of QnA systems @Proxzar.ai *vis-a-vis* this BERT implementation was also planned in the final report on performance summary.\n",
        "*   The final fine-tuned model is planned to be saved as a Tensorflow 2.0 model in saved model format.\n",
        "*   The model is trained on a single GPU provided by Google Colab runtime environment."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZDHojXnkVKBg",
        "colab_type": "text"
      },
      "source": [
        "# **Project Status**\n",
        "\n",
        "\n",
        "> **Total 4 Action Items**\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AB8W9vD-VXCi",
        "colab_type": "text"
      },
      "source": [
        "---\n",
        "**Action Item 1:** \n",
        "\n",
        "> Build BERT model after fine-tuning pre-trained BERT base, uncased, 12-layer model using SQuAD 1.1 dataset and save it in **TF2 saved model** format.\n",
        "\n",
        "> **Status**: 100% completed\n",
        "\n",
        "> **ETC**: *April 18th 2020*\n",
        "\n",
        "---\n",
        "\n",
        "**Action Item 2:** \n",
        "> Performance Metrics Of Fine-tuned Model.\n",
        "\n",
        "> **Status**: 100% completed\n",
        "\n",
        "> **ETC**: *April 22nd 2020*\n",
        "\n",
        "---\n",
        "\n",
        "**Action Item 3:** \n",
        "> Generate qualitative comparision report by comparing existing implementation of Q&A systems @Proxzar.ai with this BERT implementation.\n",
        "\n",
        "> **Status**: Partially Completed (Errored out due to bug)\n",
        "\n",
        "> **ETC**: *April 24th 2020*\n",
        "\n",
        "---\n",
        "\n",
        "**Action Item 4:** \n",
        "> Submit the project as per the institution guidelines.\n",
        "\n",
        "> **Status**: TBD\n",
        "\n",
        "> **ETC**: *May 1st 2020*\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rvFpaXkIZ9uU",
        "colab_type": "text"
      },
      "source": [
        "# **Action Item 1**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bH5hNJ-_Rwfx",
        "colab_type": "text"
      },
      "source": [
        "# Map Google Drive Locally\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j4M9o-eJR2AI",
        "colab_type": "code",
        "outputId": "24d3cdf5-19cf-4107-faae-2ccbfbb3b909",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "77yqeQtMyyR6",
        "colab_type": "code",
        "outputId": "dc4e59c7-2dff-4ecb-e063-36c2662f9a3e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "### IMP #####################################################\n",
        "# Run the below symlink mapping only once per session i.e. \n",
        "# when runtime is started or re-started for the first time.\n",
        "#############################################################\n",
        "!rm /mydrive\n",
        "!ln -s \"/content/drive/My Drive\" /mydrive"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "rm: cannot remove '/mydrive': No such file or directory\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lyqteTQ52K97",
        "colab_type": "text"
      },
      "source": [
        "# Install external libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "bzRY1B2T9zBj",
        "outputId": "e1c38b07-9ac6-4df9-a47b-ebd430069c71",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "!pip install tqdm\n",
        "# Colab changed to Tensorflow 2 version on March'27th'2020. Hence commenting out the below lines\n",
        "!pip uninstall tensorflow # default colab version is 1.*\n",
        "!pip install tensorflow==2.1.0 # install tensorflow version 2 as it has tight integration with Keras"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (4.38.0)\n",
            "Uninstalling tensorflow-2.2.0rc3:\n",
            "  Would remove:\n",
            "    /usr/local/bin/estimator_ckpt_converter\n",
            "    /usr/local/bin/saved_model_cli\n",
            "    /usr/local/bin/tensorboard\n",
            "    /usr/local/bin/tf_upgrade_v2\n",
            "    /usr/local/bin/tflite_convert\n",
            "    /usr/local/bin/toco\n",
            "    /usr/local/bin/toco_from_protos\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow-2.2.0rc3.dist-info/*\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/*\n",
            "Proceed (y/n)? y\n",
            "  Successfully uninstalled tensorflow-2.2.0rc3\n",
            "Collecting tensorflow==2.1.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/85/d4/c0cd1057b331bc38b65478302114194bd8e1b9c2bbc06e300935c0e93d90/tensorflow-2.1.0-cp36-cp36m-manylinux2010_x86_64.whl (421.8MB)\n",
            "\u001b[K     |████████████████████████████████| 421.8MB 23kB/s \n",
            "\u001b[?25hRequirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.1.0) (1.12.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.1.0) (1.1.0)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.1.0) (0.9.0)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.1.0) (1.12.1)\n",
            "Requirement already satisfied: wheel>=0.26; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.1.0) (0.34.2)\n",
            "Requirement already satisfied: scipy==1.4.1; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.1.0) (1.4.1)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.1.0) (0.8.1)\n",
            "Requirement already satisfied: keras-preprocessing>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.1.0) (1.1.0)\n",
            "Requirement already satisfied: numpy<2.0,>=1.16.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.1.0) (1.18.2)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.1.0) (3.2.0)\n",
            "Requirement already satisfied: protobuf>=3.8.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.1.0) (3.10.0)\n",
            "Collecting tensorboard<2.2.0,>=2.1.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d9/41/bbf49b61370e4f4d245d4c6051dfb6db80cec672605c91b1652ac8cc3d38/tensorboard-2.1.1-py3-none-any.whl (3.8MB)\n",
            "\u001b[K     |████████████████████████████████| 3.9MB 43.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.1.0) (1.28.1)\n",
            "Requirement already satisfied: keras-applications>=1.0.8 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.1.0) (1.0.8)\n",
            "Collecting tensorflow-estimator<2.2.0,>=2.1.0rc0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/18/90/b77c328a1304437ab1310b463e533fa7689f4bfc41549593056d812fab8e/tensorflow_estimator-2.1.0-py2.py3-none-any.whl (448kB)\n",
            "\u001b[K     |████████████████████████████████| 450kB 51.8MB/s \n",
            "\u001b[?25hCollecting gast==0.2.2\n",
            "  Downloading https://files.pythonhosted.org/packages/4e/35/11749bf99b2d4e3cceb4d55ca22590b0d7c2c62b9de38ac4a4a7f4687421/gast-0.2.2.tar.gz\n",
            "Requirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.1.0) (0.2.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.8.0->tensorflow==2.1.0) (46.1.3)\n",
            "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0) (1.7.2)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0) (3.2.1)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0) (1.0.1)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0) (0.4.1)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0) (2.21.0)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras-applications>=1.0.8->tensorflow==2.1.0) (2.10.0)\n",
            "Requirement already satisfied: cachetools<3.2,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0) (3.1.1)\n",
            "Requirement already satisfied: rsa<4.1,>=3.1.4 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0) (4.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0) (0.2.8)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0) (1.3.0)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0) (2.8)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0) (3.0.4)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0) (2020.4.5.1)\n",
            "Requirement already satisfied: pyasn1>=0.1.3 in /usr/local/lib/python3.6/dist-packages (from rsa<4.1,>=3.1.4->google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0) (0.4.8)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0) (3.1.0)\n",
            "Building wheels for collected packages: gast\n",
            "  Building wheel for gast (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gast: filename=gast-0.2.2-cp36-none-any.whl size=7540 sha256=09193b7acddce66c545d696f41dfc6c84bdb022051d30e8691393910d6dd4d20\n",
            "  Stored in directory: /root/.cache/pip/wheels/5c/2e/7e/a1d4d4fcebe6c381f378ce7743a3ced3699feb89bcfbdadadd\n",
            "Successfully built gast\n",
            "Installing collected packages: tensorboard, tensorflow-estimator, gast, tensorflow\n",
            "  Found existing installation: tensorboard 2.2.0\n",
            "    Uninstalling tensorboard-2.2.0:\n",
            "      Successfully uninstalled tensorboard-2.2.0\n",
            "  Found existing installation: tensorflow-estimator 2.2.0rc0\n",
            "    Uninstalling tensorflow-estimator-2.2.0rc0:\n",
            "      Successfully uninstalled tensorflow-estimator-2.2.0rc0\n",
            "  Found existing installation: gast 0.3.3\n",
            "    Uninstalling gast-0.3.3:\n",
            "      Successfully uninstalled gast-0.3.3\n",
            "Successfully installed gast-0.2.2 tensorboard-2.1.1 tensorflow-2.1.0 tensorflow-estimator-2.1.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y18lhOx92Vpr",
        "colab_type": "text"
      },
      "source": [
        "# Import external modules"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BNRCYDBb2esA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import math\n",
        "import datetime\n",
        "from tqdm import tqdm\n",
        "import tensorflow as tf"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0kA34LEg3dWD",
        "colab_type": "text"
      },
      "source": [
        "# Define global variables"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9L7fwldM3jiK",
        "colab_type": "code",
        "outputId": "f5d92005-e14f-4af8-a09f-19df80f25b8a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 245
        }
      },
      "source": [
        "################################################################################################################\n",
        "# The reason why we've the below variables saved as ENV variables is because we run the training script in a shell\n",
        "# and using GPU / TPU for parallel processing. This means that we need to maintain these variables across \n",
        "# new independent processes to run python scripts in the bash / command prompt.\n",
        "# Also, to give info on all those variables that are used in BERT training / prediction scripts, another set of \n",
        "# \"local\" variables are listed in the succeeding section (for general info of various variables used in BERT).\n",
        "################################################################################################################\n",
        "os.environ['LOCAL_DIR']=\"/mydrive\"\n",
        "\n",
        "#os.environ['NIGHTLY_BUILD_DIR']='/usr/local/lib/python3.6/dist-packages/official' # use TF_SQUAD_DIR instead\n",
        "\n",
        "# SQuAD specific:\n",
        "os.environ['SQUAD_TRG_DATA_FILE']=os.path.join(os.environ['LOCAL_DIR'],'bert_finetuning_outputs','train-v1.1.json')\n",
        "!echo \"(SQUAD_TRG_DATA_FILE) SQuAD training data file with full path is: \" ${SQUAD_TRG_DATA_FILE}\n",
        "\n",
        "os.environ['SQUAD_PRED_FILE']=os.path.join(os.environ['LOCAL_DIR'],'bert_finetuning_outputs','dev-v1.1.json')\n",
        "!echo \"(SQUAD_PRED_FILE) SQuAD prediction file with full path is: \" ${SQUAD_PRED_FILE}\n",
        "\n",
        "############################################################\n",
        "#Below vars are when the project is cloned via gitbub\n",
        "#os.environ['TF_SQUAD_DIR']=os.path.join(os.environ['LOCAL_DIR'],'bert_tf')\n",
        "\n",
        "os.environ['TF_SQUAD_DIR']='/usr/local/lib/python3.6/dist-packages/official'\n",
        "!echo \"(TF_SQUAD_DIR) Tensforflow SQuAD parent directory with Python scripts used for fine-tuning is: \" ${TF_SQUAD_DIR}\n",
        "############################################################\n",
        "\n",
        "os.environ['SQUAD_VERSION']='v1.1'\n",
        "!echo \"SQuAD version being used for fine-tuning is: \" ${SQUAD_VERSION}\n",
        "\n",
        "# BERT specific:\n",
        "os.environ['BERT_MODEL_TO_FINE_TUNE_ON_TF_HUB']='https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/1'\n",
        "!echo \"(BERT_MODEL_TO_FINE_TUNE_ON_TF_HUB) BERT model being fine-tuned is: \" ${BERT_MODEL_TO_FINE_TUNE_ON_TF_HUB}\n",
        "\n",
        "os.environ['BERT_DIR']=os.path.join(os.environ['LOCAL_DIR'],'(REL_VER)_bert_tf_OLD', 'BERT_Pretrained_Models','bert_uncased_base')\n",
        "!echo \"(BERT_DIR) BERT directory that has pre-trained model being fine-tuned is: \" ${BERT_DIR}\n",
        "\n",
        "os.environ['OUTPUT_DIR']=os.path.join(os.environ['LOCAL_DIR'],'bert_finetuning_outputs')\n",
        "!echo \"(OUTPUT_DIR) Output directory to save model and predictions is: \" ${OUTPUT_DIR}\n",
        "\n",
        "os.environ['KERAS_SAVED_MODEL_OUTPUT_DIR']=os.path.join(os.environ['LOCAL_DIR'],'bert_finetuning_outputs','keras_saved_model')\n",
        "!echo \"(KERAS_SAVED_MODEL_OUTPUT_DIR) Output directory to save fine-tuned model as Keras model is: \" ${KERAS_SAVED_MODEL_OUTPUT_DIR}\n",
        "\n",
        "############################################################\n",
        "# Uncomment below lines when the project external scripts are cloned via github\n",
        "#os.environ['CREATE_FINETUNING_DATA_SCRIPT']=os.path.join('models','official', 'nlp', 'data', 'create_finetuning_data.py')\n",
        "#!echo \"(CREATE_FINETUNING_DATA_SCRIPT) Python script (with relative path to project root) that tokenizes and prepares data for fine tuning is: \" ${CREATE_FINETUNING_DATA_SCRIPT}\n",
        "\n",
        "#os.environ['FINETUNING_TRG_SCRIPT']=os.path.join('models','official', 'nlp', 'bert' , 'run_squad.py')\n",
        "#!echo \"(FINETUNING_TRG_SCRIPT) Python script (with relative path to project root) that trains or fine tunes is: \" ${FINETUNING_TRG_SCRIPT}\n",
        "\n",
        "#os.environ['FINAL_EVALUATION_SCRIPT']=os.path.join('models','official', 'nlp', 'bert' , 'evaluate-v1.1.py')\n",
        "#!echo \"(FINAL_EVALUATION_SCRIPT) Python script (with relative path to project root) that trains or fine tunes is: \" ${FINAL_EVALUATION_SCRIPT}\n",
        "\n",
        "#os.environ['MODEL_SAVE_SCRIPT']=os.path.join('models','official', 'nlp', 'bert' , 'model_saving_utils.py')\n",
        "#!echo \"(MODEL_SAVE_SCRIPT) Python script used to save the fine-tuned model to Keras model.\" ${MODEL_SAVE_SCRIPT}\n",
        "\n",
        "# Comment below lines when the project external scripts are cloned via github\n",
        "os.environ['CREATE_FINETUNING_DATA_SCRIPT']=os.path.join(os.environ['TF_SQUAD_DIR'], 'nlp', 'data', 'create_finetuning_data.py')\n",
        "!echo \"(CREATE_FINETUNING_DATA_SCRIPT) Python script (with absolute path) that tokenizes and prepares data for fine tuning is: \" ${CREATE_FINETUNING_DATA_SCRIPT}\n",
        "\n",
        "os.environ['FINETUNING_TRG_SCRIPT']=os.path.join(os.environ['TF_SQUAD_DIR'], 'nlp', 'bert' , 'run_squad.py')\n",
        "!echo \"(FINETUNING_TRG_SCRIPT) Python script (with absolute path) that trains or fine tunes is: \" ${FINETUNING_TRG_SCRIPT}\n",
        "\n",
        "os.environ['FINAL_EVALUATION_SCRIPT']=os.path.join(os.environ['OUTPUT_DIR'], 'evaluate-v1.1.py')\n",
        "!echo \"(FINAL_EVALUATION_SCRIPT) Python script (with absolute path) that trains or fine tunes is: \" ${FINAL_EVALUATION_SCRIPT}\n",
        "\n",
        "os.environ['MODEL_SAVE_SCRIPT']=os.path.join(os.environ['TF_SQUAD_DIR'], 'nlp', 'bert' , 'model_saving_utils.py')\n",
        "!echo \"(MODEL_SAVE_SCRIPT) Python script used to save the fine-tuned model to Keras model.\" ${MODEL_SAVE_SCRIPT}\n",
        "\n",
        "############################################################\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(SQUAD_TRG_DATA_FILE) SQuAD training data file with full path is:  /mydrive/bert_finetuning_outputs/train-v1.1.json\n",
            "(SQUAD_PRED_FILE) SQuAD prediction file with full path is:  /mydrive/bert_finetuning_outputs/dev-v1.1.json\n",
            "(TF_SQUAD_DIR) Tensforflow SQuAD parent directory with Python scripts used for fine-tuning is:  /usr/local/lib/python3.6/dist-packages/official\n",
            "SQuAD version being used for fine-tuning is:  v1.1\n",
            "(BERT_MODEL_TO_FINE_TUNE_ON_TF_HUB) BERT model being fine-tuned is:  https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/1\n",
            "(BERT_DIR) BERT directory that has pre-trained model being fine-tuned is:  /mydrive/(REL_VER)_bert_tf_OLD/BERT_Pretrained_Models/bert_uncased_base\n",
            "(OUTPUT_DIR) Output directory to save model and predictions is:  /mydrive/bert_finetuning_outputs\n",
            "(KERAS_SAVED_MODEL_OUTPUT_DIR) Output directory to save fine-tuned model as Keras model is:  /mydrive/bert_finetuning_outputs/keras_saved_model\n",
            "(CREATE_FINETUNING_DATA_SCRIPT) Python script (with absolute path) that tokenizes and prepares data for fine tuning is:  /usr/local/lib/python3.6/dist-packages/official/nlp/data/create_finetuning_data.py\n",
            "(FINETUNING_TRG_SCRIPT) Python script (with absolute path) that trains or fine tunes is:  /usr/local/lib/python3.6/dist-packages/official/nlp/bert/run_squad.py\n",
            "(FINAL_EVALUATION_SCRIPT) Python script (with absolute path) that trains or fine tunes is:  /mydrive/bert_finetuning_outputs/evaluate-v1.1.py\n",
            "(MODEL_SAVE_SCRIPT) Python script used to save the fine-tuned model to Keras model. /usr/local/lib/python3.6/dist-packages/official/nlp/bert/model_saving_utils.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PNq9e4OS6cRf",
        "colab_type": "text"
      },
      "source": [
        "# Get Tensorflow Models from github"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bx-RAsLx6ikG",
        "colab_type": "code",
        "outputId": "3922551c-474b-482b-cc9c-98c9e7c08561",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "### IMP ####################################################################################################\n",
        "# Installing Tensorflow Models API via github is giving issues with missing attributes, like: CallbackList.\n",
        "# Hence, commenting out the below lines to install nightly build as suggested in TF models readme.\n",
        "#os.chdir(os.environ['TF_SQUAD_DIR'])\n",
        "#print(\"\\nCurrent working directory is: \" + os.getcwd() + \"\\n\")\n",
        "#!git clone 'https://github.com/tensorflow/models.git'\n",
        "#os.environ['PYTHONPATH'] += \":\" + os.path.join(os.environ['TF_SQUAD_DIR'], 'models')\n",
        "#!pip3 install --user -r models/official/requirements.txt\n",
        "############################################################################################################\n",
        "\n",
        "# Install ONLY ONCE (check if the folder exists: os.environ['TF_SQUAD_DIR'] and was not reset in new VM)\n",
        "# If folder doesn't exists, uncomment the below line and install tf-models.nightly package.\n",
        "#!pip install tf-models-nightly\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting tf-models-nightly\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/0f/8f/ceccfb078dc3ef3e2b8c0b3b682f864e35e995bd712736e5c490b9bcf253/tf_models_nightly-2.2.0.dev20200420-py2.py3-none-any.whl (765kB)\n",
            "\r\u001b[K     |▍                               | 10kB 27.5MB/s eta 0:00:01\r\u001b[K     |▉                               | 20kB 6.1MB/s eta 0:00:01\r\u001b[K     |█▎                              | 30kB 8.6MB/s eta 0:00:01\r\u001b[K     |█▊                              | 40kB 11.0MB/s eta 0:00:01\r\u001b[K     |██▏                             | 51kB 7.1MB/s eta 0:00:01\r\u001b[K     |██▋                             | 61kB 8.3MB/s eta 0:00:01\r\u001b[K     |███                             | 71kB 9.4MB/s eta 0:00:01\r\u001b[K     |███▍                            | 81kB 10.4MB/s eta 0:00:01\r\u001b[K     |███▉                            | 92kB 8.2MB/s eta 0:00:01\r\u001b[K     |████▎                           | 102kB 8.9MB/s eta 0:00:01\r\u001b[K     |████▊                           | 112kB 8.9MB/s eta 0:00:01\r\u001b[K     |█████▏                          | 122kB 8.9MB/s eta 0:00:01\r\u001b[K     |█████▋                          | 133kB 8.9MB/s eta 0:00:01\r\u001b[K     |██████                          | 143kB 8.9MB/s eta 0:00:01\r\u001b[K     |██████▍                         | 153kB 8.9MB/s eta 0:00:01\r\u001b[K     |██████▉                         | 163kB 8.9MB/s eta 0:00:01\r\u001b[K     |███████▎                        | 174kB 8.9MB/s eta 0:00:01\r\u001b[K     |███████▊                        | 184kB 8.9MB/s eta 0:00:01\r\u001b[K     |████████▏                       | 194kB 8.9MB/s eta 0:00:01\r\u001b[K     |████████▋                       | 204kB 8.9MB/s eta 0:00:01\r\u001b[K     |█████████                       | 215kB 8.9MB/s eta 0:00:01\r\u001b[K     |█████████▍                      | 225kB 8.9MB/s eta 0:00:01\r\u001b[K     |█████████▉                      | 235kB 8.9MB/s eta 0:00:01\r\u001b[K     |██████████▎                     | 245kB 8.9MB/s eta 0:00:01\r\u001b[K     |██████████▊                     | 256kB 8.9MB/s eta 0:00:01\r\u001b[K     |███████████▏                    | 266kB 8.9MB/s eta 0:00:01\r\u001b[K     |███████████▋                    | 276kB 8.9MB/s eta 0:00:01\r\u001b[K     |████████████                    | 286kB 8.9MB/s eta 0:00:01\r\u001b[K     |████████████▍                   | 296kB 8.9MB/s eta 0:00:01\r\u001b[K     |████████████▉                   | 307kB 8.9MB/s eta 0:00:01\r\u001b[K     |█████████████▎                  | 317kB 8.9MB/s eta 0:00:01\r\u001b[K     |█████████████▊                  | 327kB 8.9MB/s eta 0:00:01\r\u001b[K     |██████████████▏                 | 337kB 8.9MB/s eta 0:00:01\r\u001b[K     |██████████████▌                 | 348kB 8.9MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 358kB 8.9MB/s eta 0:00:01\r\u001b[K     |███████████████▍                | 368kB 8.9MB/s eta 0:00:01\r\u001b[K     |███████████████▉                | 378kB 8.9MB/s eta 0:00:01\r\u001b[K     |████████████████▎               | 389kB 8.9MB/s eta 0:00:01\r\u001b[K     |████████████████▊               | 399kB 8.9MB/s eta 0:00:01\r\u001b[K     |█████████████████▏              | 409kB 8.9MB/s eta 0:00:01\r\u001b[K     |█████████████████▌              | 419kB 8.9MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 430kB 8.9MB/s eta 0:00:01\r\u001b[K     |██████████████████▍             | 440kB 8.9MB/s eta 0:00:01\r\u001b[K     |██████████████████▉             | 450kB 8.9MB/s eta 0:00:01\r\u001b[K     |███████████████████▎            | 460kB 8.9MB/s eta 0:00:01\r\u001b[K     |███████████████████▊            | 471kB 8.9MB/s eta 0:00:01\r\u001b[K     |████████████████████▏           | 481kB 8.9MB/s eta 0:00:01\r\u001b[K     |████████████████████▌           | 491kB 8.9MB/s eta 0:00:01\r\u001b[K     |█████████████████████           | 501kB 8.9MB/s eta 0:00:01\r\u001b[K     |█████████████████████▍          | 512kB 8.9MB/s eta 0:00:01\r\u001b[K     |█████████████████████▉          | 522kB 8.9MB/s eta 0:00:01\r\u001b[K     |██████████████████████▎         | 532kB 8.9MB/s eta 0:00:01\r\u001b[K     |██████████████████████▊         | 542kB 8.9MB/s eta 0:00:01\r\u001b[K     |███████████████████████▏        | 552kB 8.9MB/s eta 0:00:01\r\u001b[K     |███████████████████████▌        | 563kB 8.9MB/s eta 0:00:01\r\u001b[K     |████████████████████████        | 573kB 8.9MB/s eta 0:00:01\r\u001b[K     |████████████████████████▍       | 583kB 8.9MB/s eta 0:00:01\r\u001b[K     |████████████████████████▉       | 593kB 8.9MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▎      | 604kB 8.9MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▊      | 614kB 8.9MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 624kB 8.9MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▌     | 634kB 8.9MB/s eta 0:00:01\r\u001b[K     |███████████████████████████     | 645kB 8.9MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▍    | 655kB 8.9MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▉    | 665kB 8.9MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▎   | 675kB 8.9MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▊   | 686kB 8.9MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████   | 696kB 8.9MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▌  | 706kB 8.9MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 716kB 8.9MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▍ | 727kB 8.9MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▉ | 737kB 8.9MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▎| 747kB 8.9MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▊| 757kB 8.9MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 768kB 8.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: oauth2client>=4.1.2 in /usr/local/lib/python3.6/dist-packages (from tf-models-nightly) (4.1.3)\n",
            "Requirement already satisfied: tensorflow-datasets in /usr/local/lib/python3.6/dist-packages (from tf-models-nightly) (2.1.0)\n",
            "Requirement already satisfied: typing in /usr/local/lib/python3.6/dist-packages (from tf-models-nightly) (3.6.6)\n",
            "Requirement already satisfied: scipy>=0.19.1 in /usr/local/lib/python3.6/dist-packages (from tf-models-nightly) (1.4.1)\n",
            "Requirement already satisfied: pandas>=0.22.0 in /usr/local/lib/python3.6/dist-packages (from tf-models-nightly) (1.0.3)\n",
            "Requirement already satisfied: gin-config in /usr/local/lib/python3.6/dist-packages (from tf-models-nightly) (0.3.0)\n",
            "Collecting mlperf-compliance==0.0.10\n",
            "  Downloading https://files.pythonhosted.org/packages/f4/08/f2febd8cbd5c9371f7dab311e90400d83238447ba7609b3bf0145b4cb2a2/mlperf_compliance-0.0.10-py3-none-any.whl\n",
            "Requirement already satisfied: kaggle>=1.3.9 in /usr/local/lib/python3.6/dist-packages (from tf-models-nightly) (1.5.6)\n",
            "Collecting tf-nightly\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/45/25/d3dd2946cd6ae36475be19e41c87f1d47590ab75ea22267bd95a58b7b5b9/tf_nightly-2.2.0.dev20200420-cp36-cp36m-manylinux2010_x86_64.whl (519.0MB)\n",
            "\u001b[K     |████████████████████████████████| 519.0MB 33kB/s \n",
            "\u001b[?25hRequirement already satisfied: matplotlib in /usr/local/lib/python3.6/dist-packages (from tf-models-nightly) (3.2.1)\n",
            "Requirement already satisfied: tensorflow-hub>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tf-models-nightly) (0.8.0)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from tf-models-nightly) (3.13)\n",
            "Collecting opencv-python-headless\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/43/2c/909a04b07360516953beaf6f66480bb6b84b817c6b300c1235bfb2901ad8/opencv_python_headless-4.2.0.34-cp36-cp36m-manylinux1_x86_64.whl (21.6MB)\n",
            "\u001b[K     |████████████████████████████████| 21.6MB 60kB/s \n",
            "\u001b[?25hRequirement already satisfied: google-api-python-client>=1.6.7 in /usr/local/lib/python3.6/dist-packages (from tf-models-nightly) (1.7.12)\n",
            "Collecting py-cpuinfo>=3.3.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/42/60/63f28a5401da733043abe7053e7d9591491b4784c4f87c339bf51215aa0a/py-cpuinfo-5.0.0.tar.gz (82kB)\n",
            "\u001b[K     |████████████████████████████████| 92kB 13.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.15.4 in /usr/local/lib/python3.6/dist-packages (from tf-models-nightly) (1.18.2)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.6/dist-packages (from tf-models-nightly) (7.0.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from tf-models-nightly) (1.12.0)\n",
            "Requirement already satisfied: dataclasses in /usr/local/lib/python3.6/dist-packages (from tf-models-nightly) (0.7)\n",
            "Collecting sentencepiece\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/74/f4/2d5214cbf13d06e7cb2c20d84115ca25b53ea76fa1f0ade0e3c9749de214/sentencepiece-0.1.85-cp36-cp36m-manylinux1_x86_64.whl (1.0MB)\n",
            "\u001b[K     |████████████████████████████████| 1.0MB 48.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: psutil>=5.4.3 in /usr/local/lib/python3.6/dist-packages (from tf-models-nightly) (5.4.8)\n",
            "Requirement already satisfied: tensorflow-addons in /usr/local/lib/python3.6/dist-packages (from tf-models-nightly) (0.8.3)\n",
            "Collecting tensorflow-model-optimization>=0.2.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/09/7e/e94aa029999ec30951e8129fa992fecbbaffda66eba97c65d5a83f8ea96d/tensorflow_model_optimization-0.3.0-py2.py3-none-any.whl (165kB)\n",
            "\u001b[K     |████████████████████████████████| 174kB 53.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: google-cloud-bigquery>=0.31.0 in /usr/local/lib/python3.6/dist-packages (from tf-models-nightly) (1.21.0)\n",
            "Requirement already satisfied: Cython in /usr/local/lib/python3.6/dist-packages (from tf-models-nightly) (0.29.16)\n",
            "Requirement already satisfied: httplib2>=0.9.1 in /usr/local/lib/python3.6/dist-packages (from oauth2client>=4.1.2->tf-models-nightly) (0.17.2)\n",
            "Requirement already satisfied: rsa>=3.1.4 in /usr/local/lib/python3.6/dist-packages (from oauth2client>=4.1.2->tf-models-nightly) (4.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.0.5 in /usr/local/lib/python3.6/dist-packages (from oauth2client>=4.1.2->tf-models-nightly) (0.2.8)\n",
            "Requirement already satisfied: pyasn1>=0.1.7 in /usr/local/lib/python3.6/dist-packages (from oauth2client>=4.1.2->tf-models-nightly) (0.4.8)\n",
            "Requirement already satisfied: promise in /usr/local/lib/python3.6/dist-packages (from tensorflow-datasets->tf-models-nightly) (2.3)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from tensorflow-datasets->tf-models-nightly) (0.16.0)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.6/dist-packages (from tensorflow-datasets->tf-models-nightly) (0.9.0)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow-datasets->tf-models-nightly) (3.10.0)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-datasets->tf-models-nightly) (2.21.0)\n",
            "Requirement already satisfied: tensorflow-metadata in /usr/local/lib/python3.6/dist-packages (from tensorflow-datasets->tf-models-nightly) (0.21.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from tensorflow-datasets->tf-models-nightly) (4.38.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.6/dist-packages (from tensorflow-datasets->tf-models-nightly) (1.12.1)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.6/dist-packages (from tensorflow-datasets->tf-models-nightly) (0.3.1.1)\n",
            "Requirement already satisfied: attrs>=18.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-datasets->tf-models-nightly) (19.3.0)\n",
            "Requirement already satisfied: termcolor in /usr/local/lib/python3.6/dist-packages (from tensorflow-datasets->tf-models-nightly) (1.1.0)\n",
            "Requirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.6/dist-packages (from pandas>=0.22.0->tf-models-nightly) (2.8.1)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas>=0.22.0->tf-models-nightly) (2018.9)\n",
            "Requirement already satisfied: python-slugify in /usr/local/lib/python3.6/dist-packages (from kaggle>=1.3.9->tf-models-nightly) (4.0.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.6/dist-packages (from kaggle>=1.3.9->tf-models-nightly) (2020.4.5.1)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from kaggle>=1.3.9->tf-models-nightly) (1.24.3)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.6/dist-packages (from tf-nightly->tf-models-nightly) (3.2.0)\n",
            "Collecting gast==0.3.3\n",
            "  Downloading https://files.pythonhosted.org/packages/d6/84/759f5dd23fec8ba71952d97bcc7e2c9d7d63bdc582421f3cd4be845f0c98/gast-0.3.3-py2.py3-none-any.whl\n",
            "Collecting tb-nightly<2.4.0a0,>=2.3.0a0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f1/fa/629906397ae403334368891bf480d74f4422f83fc3a8c17eb912f00fdad8/tb_nightly-2.3.0a20200420-py3-none-any.whl (3.0MB)\n",
            "\u001b[K     |████████████████████████████████| 3.0MB 46.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tf-nightly->tf-models-nightly) (1.28.1)\n",
            "Requirement already satisfied: h5py<2.11.0,>=2.10.0 in /usr/local/lib/python3.6/dist-packages (from tf-nightly->tf-models-nightly) (2.10.0)\n",
            "Collecting tf-estimator-nightly\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/93/b7/a2c74be1599c40e6ff608d7643193797745f5c0b7422debe36c9579e5513/tf_estimator_nightly-2.3.0.dev2020042001-py2.py3-none-any.whl (455kB)\n",
            "\u001b[K     |████████████████████████████████| 460kB 54.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: keras-preprocessing>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tf-nightly->tf-models-nightly) (1.1.0)\n",
            "Requirement already satisfied: wheel>=0.26; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from tf-nightly->tf-models-nightly) (0.34.2)\n",
            "Requirement already satisfied: google-pasta>=0.1.8 in /usr/local/lib/python3.6/dist-packages (from tf-nightly->tf-models-nightly) (0.2.0)\n",
            "Requirement already satisfied: astunparse==1.6.3 in /usr/local/lib/python3.6/dist-packages (from tf-nightly->tf-models-nightly) (1.6.3)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->tf-models-nightly) (2.4.7)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->tf-models-nightly) (1.2.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib->tf-models-nightly) (0.10.0)\n",
            "Requirement already satisfied: google-auth>=1.4.1 in /usr/local/lib/python3.6/dist-packages (from google-api-python-client>=1.6.7->tf-models-nightly) (1.7.2)\n",
            "Requirement already satisfied: google-auth-httplib2>=0.0.3 in /usr/local/lib/python3.6/dist-packages (from google-api-python-client>=1.6.7->tf-models-nightly) (0.0.3)\n",
            "Requirement already satisfied: uritemplate<4dev,>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from google-api-python-client>=1.6.7->tf-models-nightly) (3.0.1)\n",
            "Requirement already satisfied: typeguard in /usr/local/lib/python3.6/dist-packages (from tensorflow-addons->tf-models-nightly) (2.7.1)\n",
            "Collecting dm-tree~=0.1.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ee/47/948602fe82595056eb7f14b5005ee525c62a73218ffffe2aabb6b9e3ed42/dm_tree-0.1.4-cp36-cp36m-manylinux1_x86_64.whl (293kB)\n",
            "\u001b[K     |████████████████████████████████| 296kB 59.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: google-cloud-core<2.0dev,>=1.0.3 in /usr/local/lib/python3.6/dist-packages (from google-cloud-bigquery>=0.31.0->tf-models-nightly) (1.0.3)\n",
            "Requirement already satisfied: google-resumable-media!=0.4.0,<0.5.0dev,>=0.3.1 in /usr/local/lib/python3.6/dist-packages (from google-cloud-bigquery>=0.31.0->tf-models-nightly) (0.4.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.6.1->tensorflow-datasets->tf-models-nightly) (46.1.3)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests>=2.19.0->tensorflow-datasets->tf-models-nightly) (3.0.4)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests>=2.19.0->tensorflow-datasets->tf-models-nightly) (2.8)\n",
            "Requirement already satisfied: googleapis-common-protos in /usr/local/lib/python3.6/dist-packages (from tensorflow-metadata->tensorflow-datasets->tf-models-nightly) (1.51.0)\n",
            "Requirement already satisfied: text-unidecode>=1.3 in /usr/local/lib/python3.6/dist-packages (from python-slugify->kaggle>=1.3.9->tf-models-nightly) (1.3)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.6/dist-packages (from tb-nightly<2.4.0a0,>=2.3.0a0->tf-nightly->tf-models-nightly) (1.6.0.post3)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tb-nightly<2.4.0a0,>=2.3.0a0->tf-nightly->tf-models-nightly) (1.0.1)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from tb-nightly<2.4.0a0,>=2.3.0a0->tf-nightly->tf-models-nightly) (0.4.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tb-nightly<2.4.0a0,>=2.3.0a0->tf-nightly->tf-models-nightly) (3.2.1)\n",
            "Requirement already satisfied: cachetools<3.2,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from google-auth>=1.4.1->google-api-python-client>=1.6.7->tf-models-nightly) (3.1.1)\n",
            "Requirement already satisfied: google-api-core<2.0.0dev,>=1.14.0 in /usr/local/lib/python3.6/dist-packages (from google-cloud-core<2.0dev,>=1.0.3->google-cloud-bigquery>=0.31.0->tf-models-nightly) (1.16.0)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tb-nightly<2.4.0a0,>=2.3.0a0->tf-nightly->tf-models-nightly) (1.3.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tb-nightly<2.4.0a0,>=2.3.0a0->tf-nightly->tf-models-nightly) (3.1.0)\n",
            "Building wheels for collected packages: py-cpuinfo\n",
            "  Building wheel for py-cpuinfo (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for py-cpuinfo: filename=py_cpuinfo-5.0.0-cp36-none-any.whl size=18684 sha256=0cd61d538a18ae2d654570fadc084e899745ba21e724556dd243a183159d04b8\n",
            "  Stored in directory: /root/.cache/pip/wheels/01/7e/a9/b982d0fea22b7e4ae5619de949570cde5ad55420cec16e86a5\n",
            "Successfully built py-cpuinfo\n",
            "\u001b[31mERROR: tensorflow 2.1.0 has requirement gast==0.2.2, but you'll have gast 0.3.3 which is incompatible.\u001b[0m\n",
            "Installing collected packages: mlperf-compliance, gast, tb-nightly, tf-estimator-nightly, tf-nightly, opencv-python-headless, py-cpuinfo, sentencepiece, dm-tree, tensorflow-model-optimization, tf-models-nightly\n",
            "  Found existing installation: gast 0.2.2\n",
            "    Uninstalling gast-0.2.2:\n",
            "      Successfully uninstalled gast-0.2.2\n",
            "Successfully installed dm-tree-0.1.4 gast-0.3.3 mlperf-compliance-0.0.10 opencv-python-headless-4.2.0.34 py-cpuinfo-5.0.0 sentencepiece-0.1.85 tb-nightly-2.3.0a20200420 tensorflow-model-optimization-0.3.0 tf-estimator-nightly-2.3.0.dev2020042001 tf-models-nightly-2.2.0.dev20200420 tf-nightly-2.2.0.dev20200420\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "gast",
                  "tensorboard",
                  "tensorflow",
                  "tensorflow_estimator"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YzECrzeIYb9q",
        "colab_type": "text"
      },
      "source": [
        "# Variables used for training / fine-tuning BERT model (for info only)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yG9ZOh3CYmDk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "### BEGIN #########################################################################################################\n",
        "# The below variables are NOT required as we are using Tensorflow hub published model of BERT. These\n",
        "# variables are left here for future experiments to import BERT embeddings and weights via checkpoint file.\n",
        "\n",
        "# The config json file corresponding to the pre-trained BERT model and that specifies the model architecture.\n",
        "#bert_config_file = os.path.join(LOCAL_DIR,'bertmodel','uncased_L-12_H-768_A-12','bert_config.json')\n",
        "\n",
        "# The vocabulary file that the BERT model was trained on.\n",
        "#vocab_file = os.path.join($BERT_DIR,'vocab.txt')\n",
        "\n",
        "# Get weights and other variables from the pre-trained BERT saved model file.\n",
        "#init_checkpoint = os.path.join(LOCAL_DIR,$BERT_DIR, 'bert_model.ckpt.data-00000-of-00001')\n",
        "### END #########################################################################################################\n",
        "\n",
        "# The output directory where the model checkpoints will be written.\n",
        "#output_dir = os.path.join($OUTPUT_DIR, 'model_out')\n",
        "\n",
        "#### BEGIN ##################################################################################################################################\n",
        "# The way 'run_squad.py' is used in TF for fine-tuning of QA task using BERT embeddings works differently when doing predictions.\n",
        "#   - During fine-tuning, a predictions file as set in the variable 'output_prediction_file' is generated in the folder given in the variable 'output_dir'.\n",
        "#   - After completing training / fine-tuning using 'run_squad.py', we do evaluation by running another script 'squad_evaluate_v1_1.py'.\n",
        "#   - The above evaluations script takes two args viz., 1) SQuAD's dev_v1.1.json, and 2) predictions.json (generated during fine-tuning).\n",
        "\n",
        "# SQuAD json for training / fine-tuning\n",
        "#train_file = $SQUAD_TRG_DATA_FILE\n",
        "\n",
        "# SQuAD json for evaluating predictions generated in the file 'predictions.json' during fine-tuning\n",
        "#predict_file = $SQUAD_PRED_FILE\n",
        "\n",
        "# Output file to log predictions.\n",
        "#output_prediction_file = os.path.join(output_dir,'predictions.json')\n",
        "#### END ##################################################################################################################################\n",
        "\n",
        "# Whether to lower case the input - True for uncased models / False for cased models.\n",
        "#do_lower_case = True\n",
        "\n",
        "#### BEGIN ##################################################################################################################################\n",
        "# The maximum total input sequence length after WordPiece tokenization.\n",
        "# Sequences longer than this will be truncated, and sequences shorter than this will be padded.\n",
        "#   - Internally, padding is taken care of by the script 'create_finetuning_data.py', which is run \n",
        "#     before running the main training script 'run_squad.py' by setting \n",
        "#     input mask to 0 for those tokens that doesn't need to compute attention.\n",
        "#max_seq_length = $MAX_SEQ_LEN\n",
        "\n",
        "# When splitting up a long document into chunks, how much stride to take between chunks.\n",
        "#doc_stride = 128\n",
        "#### END ##################################################################################################################################\n",
        "\n",
        "# The maximum number of tokens for the question. Questions longer than this will be truncated to this length.\n",
        "#max_query_length = 64\n",
        "\n",
        "# Whether to run training / fine-tuning\n",
        "#do_train = True \n",
        "\n",
        "# Whether to run eval on the dev set.\n",
        "#do_predict = True\n",
        "\n",
        "# Total batch size for training. \n",
        "#train_batch_size = 32 # not applicable in our case\n",
        "\n",
        "# Total batch size for predictions\n",
        "#predict_batch_size = 8\n",
        "\n",
        "# The initial learning rate for Adam.\n",
        "#learning_rate = 5e-5\n",
        "\n",
        "# Total number of training epochs to perform.\n",
        "#num_train_epochs = 3.0\n",
        "\n",
        "# Proportion of training to perform linear learning rate warmup for E.g., 0.1 = 10% of training.\n",
        "#warmup_proportion = 0.1 # not applicable in our case\n",
        "\n",
        "# How often to save the model checkpoint.\n",
        "#save_checkpoints_steps = 1000\n",
        "\n",
        "# How many steps to make in each estimator call.\n",
        "#iterations_per_loop = 1000\n",
        "\n",
        "# The total number of n-best predictions to generate in the nbest_predictions.json output file.\n",
        "#n_best_size = 10\n",
        "\n",
        "# The maximum length of an answer that can be generated. \n",
        "# This is needed because the start and end predictions are not conditioned on one another.\n",
        "#max_answer_length = 30\n",
        "\n",
        "# Whether to use TPU or GPU/CPU (we are using GPU for training as this is the most practical scenario for us in future)\n",
        "# It is estimated to take a few hours for BERT large. Since, we are fine-tuning on GPU, the BERT model has been\n",
        "# changed to BERT (base) - instead of BERT (large) as originally planned.\n",
        "#use_tpu = false\n",
        "\n",
        "# The Cloud TPU to use for training. This should be either the name \n",
        "# used when creating the Cloud TPU, or a grpc://ip.address.of.tpu:8470 url.\n",
        "#tpu_name = None\n",
        "\n",
        "# [Optional] GCE zone where the Cloud TPU is located in. If not\n",
        "# specified, we will attempt to automatically detect the GCE project from metadata.\n",
        "#tpu_zone = None\n",
        "# [Optional] Project name for the Cloud TPU-enabled project. If not \n",
        "# specified, we will attempt to automatically detect the GCE project from metadata.\n",
        "#gcp_project = None\n",
        "\n",
        "# [Optional] TensorFlow master URL.\n",
        "#master = None\n",
        "\n",
        "# Only used if `use_tpu` is True. Total number of TPU cores to use.\n",
        "#num_tpu_cores = None\n",
        "\n",
        "# If true, all of the warnings related to data processing will be printed. \n",
        "# A number of warnings are expected for a normal SQuAD evaluation.\n",
        "#verbose_logging = False\n",
        "\n",
        "# If true, the SQuAD examples contain some that do not have an answer (SQuAD 2.0).\n",
        "#version_2_with_negative = False\n",
        "\n",
        "# If null_score - best_non_null is greater than the threshold predict null.\n",
        "#null_score_diff_threshold = 0.0\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JzF1eqFu3aCB",
        "colab_type": "text"
      },
      "source": [
        "# Import SQuAD 2.0 dataset to LOCAL_DIR"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QhCUCMmkdQoq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# SQuAD 2.0 dataset is downloaded into Google drive (/content/drive/My Drive/bert_finetuned_model)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "0DX6itdPdEwn"
      },
      "source": [
        "# Load, pre-process and tokenize (train) dataset for fine-tuning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y_mUz95000z7",
        "colab_type": "code",
        "outputId": "e8bc6df1-c389-4335-f5d6-eee41f5b8f60",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "#################################################################################################\n",
        "# The below python script tokenizes the input data in the file ${SQUAD_TRG_DATA_FILE} and \n",
        "# writes inputs to a tensorflow record that is eventually written to the drive (${OUTPUT_DIR}).\n",
        "# This tf.record is read by the next script 'run_squad.py', which actually does fine-tuning.\n",
        "#################################################################################################\n",
        "print(\"\\nCurrent working directory is: \" + os.getcwd() + \"\\n\")\n",
        "!python ${CREATE_FINETUNING_DATA_SCRIPT} \\\n",
        " --squad_data_file=${SQUAD_TRG_DATA_FILE} \\\n",
        " --vocab_file=${OUTPUT_DIR}/vocab.txt \\\n",
        " --train_data_output_path=${OUTPUT_DIR}/squad_${SQUAD_VERSION}_train.tf_record \\\n",
        " --meta_data_file_path=${OUTPUT_DIR}/squad_${SQUAD_VERSION}_meta_data \\\n",
        " --fine_tuning_task_type=squad --max_seq_length=512"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Current working directory is: /content\n",
            "\n",
            "I0420 19:06:20.358043 140407150372736 squad_lib.py:353] *** Example ***\n",
            "I0420 19:06:20.358261 140407150372736 squad_lib.py:354] unique_id: 1000000000\n",
            "I0420 19:06:20.358927 140407150372736 squad_lib.py:355] example_index: 0\n",
            "I0420 19:06:20.358995 140407150372736 squad_lib.py:356] doc_span_index: 0\n",
            "I0420 19:06:20.359112 140407150372736 squad_lib.py:358] tokens: [CLS] to whom did the virgin mary allegedly appear in 1858 in lou ##rdes france ? [SEP] architectural ##ly , the school has a catholic character . atop the main building ' s gold dome is a golden statue of the virgin mary . immediately in front of the main building and facing it , is a copper statue of christ with arms up ##rai ##sed with the legend \" ve ##ni ##te ad me om ##nes \" . next to the main building is the basilica of the sacred heart . immediately behind the basilica is the gr ##otto , a marian place of prayer and reflection . it is a replica of the gr ##otto at lou ##rdes , france where the virgin mary reputed ##ly appeared to saint bern ##ade ##tte so ##ub ##iro ##us in 1858 . at the end of the main drive ( and in a direct line that connects through 3 statues and the gold dome ) , is a simple , modern stone statue of mary . [SEP]\n",
            "I0420 19:06:20.359509 140407150372736 squad_lib.py:361] token_to_orig_map: 17:0 18:0 19:0 20:1 21:2 22:3 23:4 24:5 25:6 26:6 27:7 28:8 29:9 30:10 31:10 32:10 33:11 34:12 35:13 36:14 37:15 38:16 39:17 40:18 41:19 42:20 43:20 44:21 45:22 46:23 47:24 48:25 49:26 50:27 51:28 52:29 53:30 54:30 55:31 56:32 57:33 58:34 59:35 60:36 61:37 62:38 63:39 64:39 65:39 66:40 67:41 68:42 69:43 70:43 71:43 72:43 73:44 74:45 75:46 76:46 77:46 78:46 79:47 80:48 81:49 82:50 83:51 84:52 85:53 86:54 87:55 88:56 89:57 90:58 91:58 92:59 93:60 94:61 95:62 96:63 97:64 98:65 99:65 100:65 101:66 102:67 103:68 104:69 105:70 106:71 107:72 108:72 109:73 110:74 111:75 112:76 113:77 114:78 115:79 116:79 117:80 118:81 119:81 120:81 121:82 122:83 123:84 124:85 125:86 126:87 127:87 128:88 129:89 130:90 131:91 132:91 133:91 134:92 135:92 136:92 137:92 138:93 139:94 140:94 141:95 142:96 143:97 144:98 145:99 146:100 147:101 148:102 149:102 150:103 151:104 152:105 153:106 154:107 155:108 156:109 157:110 158:111 159:112 160:113 161:114 162:115 163:115 164:115 165:116 166:117 167:118 168:118 169:119 170:120 171:121 172:122 173:123 174:123\n",
            "I0420 19:06:20.359633 140407150372736 squad_lib.py:366] token_is_max_context: 17:True 18:True 19:True 20:True 21:True 22:True 23:True 24:True 25:True 26:True 27:True 28:True 29:True 30:True 31:True 32:True 33:True 34:True 35:True 36:True 37:True 38:True 39:True 40:True 41:True 42:True 43:True 44:True 45:True 46:True 47:True 48:True 49:True 50:True 51:True 52:True 53:True 54:True 55:True 56:True 57:True 58:True 59:True 60:True 61:True 62:True 63:True 64:True 65:True 66:True 67:True 68:True 69:True 70:True 71:True 72:True 73:True 74:True 75:True 76:True 77:True 78:True 79:True 80:True 81:True 82:True 83:True 84:True 85:True 86:True 87:True 88:True 89:True 90:True 91:True 92:True 93:True 94:True 95:True 96:True 97:True 98:True 99:True 100:True 101:True 102:True 103:True 104:True 105:True 106:True 107:True 108:True 109:True 110:True 111:True 112:True 113:True 114:True 115:True 116:True 117:True 118:True 119:True 120:True 121:True 122:True 123:True 124:True 125:True 126:True 127:True 128:True 129:True 130:True 131:True 132:True 133:True 134:True 135:True 136:True 137:True 138:True 139:True 140:True 141:True 142:True 143:True 144:True 145:True 146:True 147:True 148:True 149:True 150:True 151:True 152:True 153:True 154:True 155:True 156:True 157:True 158:True 159:True 160:True 161:True 162:True 163:True 164:True 165:True 166:True 167:True 168:True 169:True 170:True 171:True 172:True 173:True 174:True\n",
            "I0420 19:06:20.359817 140407150372736 squad_lib.py:368] input_ids: 101 2000 3183 2106 1996 6261 2984 9382 3711 1999 8517 1999 10223 26371 2605 1029 102 6549 2135 1010 1996 2082 2038 1037 3234 2839 1012 10234 1996 2364 2311 1005 1055 2751 8514 2003 1037 3585 6231 1997 1996 6261 2984 1012 3202 1999 2392 1997 1996 2364 2311 1998 5307 2009 1010 2003 1037 6967 6231 1997 4828 2007 2608 2039 14995 6924 2007 1996 5722 1000 2310 3490 2618 4748 2033 18168 5267 1000 1012 2279 2000 1996 2364 2311 2003 1996 13546 1997 1996 6730 2540 1012 3202 2369 1996 13546 2003 1996 24665 23052 1010 1037 14042 2173 1997 7083 1998 9185 1012 2009 2003 1037 15059 1997 1996 24665 23052 2012 10223 26371 1010 2605 2073 1996 6261 2984 22353 2135 2596 2000 3002 16595 9648 4674 2061 12083 9711 2271 1999 8517 1012 2012 1996 2203 1997 1996 2364 3298 1006 1998 1999 1037 3622 2240 2008 8539 2083 1017 11342 1998 1996 2751 8514 1007 1010 2003 1037 3722 1010 2715 2962 6231 1997 2984 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0420 19:06:20.359987 140407150372736 squad_lib.py:369] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0420 19:06:20.360157 140407150372736 squad_lib.py:370] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0420 19:06:20.360218 140407150372736 squad_lib.py:375] start_position: 130\n",
            "I0420 19:06:20.360292 140407150372736 squad_lib.py:376] end_position: 137\n",
            "I0420 19:06:20.360349 140407150372736 squad_lib.py:377] answer: saint bern ##ade ##tte so ##ub ##iro ##us\n",
            "I0420 19:06:20.363731 140407150372736 squad_lib.py:353] *** Example ***\n",
            "I0420 19:06:20.363852 140407150372736 squad_lib.py:354] unique_id: 1000000001\n",
            "I0420 19:06:20.363930 140407150372736 squad_lib.py:355] example_index: 1\n",
            "I0420 19:06:20.363991 140407150372736 squad_lib.py:356] doc_span_index: 0\n",
            "I0420 19:06:20.364112 140407150372736 squad_lib.py:358] tokens: [CLS] what is in front of the notre dame main building ? [SEP] architectural ##ly , the school has a catholic character . atop the main building ' s gold dome is a golden statue of the virgin mary . immediately in front of the main building and facing it , is a copper statue of christ with arms up ##rai ##sed with the legend \" ve ##ni ##te ad me om ##nes \" . next to the main building is the basilica of the sacred heart . immediately behind the basilica is the gr ##otto , a marian place of prayer and reflection . it is a replica of the gr ##otto at lou ##rdes , france where the virgin mary reputed ##ly appeared to saint bern ##ade ##tte so ##ub ##iro ##us in 1858 . at the end of the main drive ( and in a direct line that connects through 3 statues and the gold dome ) , is a simple , modern stone statue of mary . [SEP]\n",
            "I0420 19:06:20.364233 140407150372736 squad_lib.py:361] token_to_orig_map: 13:0 14:0 15:0 16:1 17:2 18:3 19:4 20:5 21:6 22:6 23:7 24:8 25:9 26:10 27:10 28:10 29:11 30:12 31:13 32:14 33:15 34:16 35:17 36:18 37:19 38:20 39:20 40:21 41:22 42:23 43:24 44:25 45:26 46:27 47:28 48:29 49:30 50:30 51:31 52:32 53:33 54:34 55:35 56:36 57:37 58:38 59:39 60:39 61:39 62:40 63:41 64:42 65:43 66:43 67:43 68:43 69:44 70:45 71:46 72:46 73:46 74:46 75:47 76:48 77:49 78:50 79:51 80:52 81:53 82:54 83:55 84:56 85:57 86:58 87:58 88:59 89:60 90:61 91:62 92:63 93:64 94:65 95:65 96:65 97:66 98:67 99:68 100:69 101:70 102:71 103:72 104:72 105:73 106:74 107:75 108:76 109:77 110:78 111:79 112:79 113:80 114:81 115:81 116:81 117:82 118:83 119:84 120:85 121:86 122:87 123:87 124:88 125:89 126:90 127:91 128:91 129:91 130:92 131:92 132:92 133:92 134:93 135:94 136:94 137:95 138:96 139:97 140:98 141:99 142:100 143:101 144:102 145:102 146:103 147:104 148:105 149:106 150:107 151:108 152:109 153:110 154:111 155:112 156:113 157:114 158:115 159:115 160:115 161:116 162:117 163:118 164:118 165:119 166:120 167:121 168:122 169:123 170:123\n",
            "I0420 19:06:20.364349 140407150372736 squad_lib.py:366] token_is_max_context: 13:True 14:True 15:True 16:True 17:True 18:True 19:True 20:True 21:True 22:True 23:True 24:True 25:True 26:True 27:True 28:True 29:True 30:True 31:True 32:True 33:True 34:True 35:True 36:True 37:True 38:True 39:True 40:True 41:True 42:True 43:True 44:True 45:True 46:True 47:True 48:True 49:True 50:True 51:True 52:True 53:True 54:True 55:True 56:True 57:True 58:True 59:True 60:True 61:True 62:True 63:True 64:True 65:True 66:True 67:True 68:True 69:True 70:True 71:True 72:True 73:True 74:True 75:True 76:True 77:True 78:True 79:True 80:True 81:True 82:True 83:True 84:True 85:True 86:True 87:True 88:True 89:True 90:True 91:True 92:True 93:True 94:True 95:True 96:True 97:True 98:True 99:True 100:True 101:True 102:True 103:True 104:True 105:True 106:True 107:True 108:True 109:True 110:True 111:True 112:True 113:True 114:True 115:True 116:True 117:True 118:True 119:True 120:True 121:True 122:True 123:True 124:True 125:True 126:True 127:True 128:True 129:True 130:True 131:True 132:True 133:True 134:True 135:True 136:True 137:True 138:True 139:True 140:True 141:True 142:True 143:True 144:True 145:True 146:True 147:True 148:True 149:True 150:True 151:True 152:True 153:True 154:True 155:True 156:True 157:True 158:True 159:True 160:True 161:True 162:True 163:True 164:True 165:True 166:True 167:True 168:True 169:True 170:True\n",
            "I0420 19:06:20.364518 140407150372736 squad_lib.py:368] input_ids: 101 2054 2003 1999 2392 1997 1996 10289 8214 2364 2311 1029 102 6549 2135 1010 1996 2082 2038 1037 3234 2839 1012 10234 1996 2364 2311 1005 1055 2751 8514 2003 1037 3585 6231 1997 1996 6261 2984 1012 3202 1999 2392 1997 1996 2364 2311 1998 5307 2009 1010 2003 1037 6967 6231 1997 4828 2007 2608 2039 14995 6924 2007 1996 5722 1000 2310 3490 2618 4748 2033 18168 5267 1000 1012 2279 2000 1996 2364 2311 2003 1996 13546 1997 1996 6730 2540 1012 3202 2369 1996 13546 2003 1996 24665 23052 1010 1037 14042 2173 1997 7083 1998 9185 1012 2009 2003 1037 15059 1997 1996 24665 23052 2012 10223 26371 1010 2605 2073 1996 6261 2984 22353 2135 2596 2000 3002 16595 9648 4674 2061 12083 9711 2271 1999 8517 1012 2012 1996 2203 1997 1996 2364 3298 1006 1998 1999 1037 3622 2240 2008 8539 2083 1017 11342 1998 1996 2751 8514 1007 1010 2003 1037 3722 1010 2715 2962 6231 1997 2984 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0420 19:06:20.364691 140407150372736 squad_lib.py:369] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0420 19:06:20.364851 140407150372736 squad_lib.py:370] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0420 19:06:20.364913 140407150372736 squad_lib.py:375] start_position: 52\n",
            "I0420 19:06:20.364972 140407150372736 squad_lib.py:376] end_position: 56\n",
            "I0420 19:06:20.365028 140407150372736 squad_lib.py:377] answer: a copper statue of christ\n",
            "I0420 19:06:20.368129 140407150372736 squad_lib.py:353] *** Example ***\n",
            "I0420 19:06:20.368222 140407150372736 squad_lib.py:354] unique_id: 1000000002\n",
            "I0420 19:06:20.368294 140407150372736 squad_lib.py:355] example_index: 2\n",
            "I0420 19:06:20.368366 140407150372736 squad_lib.py:356] doc_span_index: 0\n",
            "I0420 19:06:20.368505 140407150372736 squad_lib.py:358] tokens: [CLS] the basilica of the sacred heart at notre dame is beside to which structure ? [SEP] architectural ##ly , the school has a catholic character . atop the main building ' s gold dome is a golden statue of the virgin mary . immediately in front of the main building and facing it , is a copper statue of christ with arms up ##rai ##sed with the legend \" ve ##ni ##te ad me om ##nes \" . next to the main building is the basilica of the sacred heart . immediately behind the basilica is the gr ##otto , a marian place of prayer and reflection . it is a replica of the gr ##otto at lou ##rdes , france where the virgin mary reputed ##ly appeared to saint bern ##ade ##tte so ##ub ##iro ##us in 1858 . at the end of the main drive ( and in a direct line that connects through 3 statues and the gold dome ) , is a simple , modern stone statue of mary . [SEP]\n",
            "I0420 19:06:20.368647 140407150372736 squad_lib.py:361] token_to_orig_map: 17:0 18:0 19:0 20:1 21:2 22:3 23:4 24:5 25:6 26:6 27:7 28:8 29:9 30:10 31:10 32:10 33:11 34:12 35:13 36:14 37:15 38:16 39:17 40:18 41:19 42:20 43:20 44:21 45:22 46:23 47:24 48:25 49:26 50:27 51:28 52:29 53:30 54:30 55:31 56:32 57:33 58:34 59:35 60:36 61:37 62:38 63:39 64:39 65:39 66:40 67:41 68:42 69:43 70:43 71:43 72:43 73:44 74:45 75:46 76:46 77:46 78:46 79:47 80:48 81:49 82:50 83:51 84:52 85:53 86:54 87:55 88:56 89:57 90:58 91:58 92:59 93:60 94:61 95:62 96:63 97:64 98:65 99:65 100:65 101:66 102:67 103:68 104:69 105:70 106:71 107:72 108:72 109:73 110:74 111:75 112:76 113:77 114:78 115:79 116:79 117:80 118:81 119:81 120:81 121:82 122:83 123:84 124:85 125:86 126:87 127:87 128:88 129:89 130:90 131:91 132:91 133:91 134:92 135:92 136:92 137:92 138:93 139:94 140:94 141:95 142:96 143:97 144:98 145:99 146:100 147:101 148:102 149:102 150:103 151:104 152:105 153:106 154:107 155:108 156:109 157:110 158:111 159:112 160:113 161:114 162:115 163:115 164:115 165:116 166:117 167:118 168:118 169:119 170:120 171:121 172:122 173:123 174:123\n",
            "I0420 19:06:20.397953 140407150372736 squad_lib.py:366] token_is_max_context: 17:True 18:True 19:True 20:True 21:True 22:True 23:True 24:True 25:True 26:True 27:True 28:True 29:True 30:True 31:True 32:True 33:True 34:True 35:True 36:True 37:True 38:True 39:True 40:True 41:True 42:True 43:True 44:True 45:True 46:True 47:True 48:True 49:True 50:True 51:True 52:True 53:True 54:True 55:True 56:True 57:True 58:True 59:True 60:True 61:True 62:True 63:True 64:True 65:True 66:True 67:True 68:True 69:True 70:True 71:True 72:True 73:True 74:True 75:True 76:True 77:True 78:True 79:True 80:True 81:True 82:True 83:True 84:True 85:True 86:True 87:True 88:True 89:True 90:True 91:True 92:True 93:True 94:True 95:True 96:True 97:True 98:True 99:True 100:True 101:True 102:True 103:True 104:True 105:True 106:True 107:True 108:True 109:True 110:True 111:True 112:True 113:True 114:True 115:True 116:True 117:True 118:True 119:True 120:True 121:True 122:True 123:True 124:True 125:True 126:True 127:True 128:True 129:True 130:True 131:True 132:True 133:True 134:True 135:True 136:True 137:True 138:True 139:True 140:True 141:True 142:True 143:True 144:True 145:True 146:True 147:True 148:True 149:True 150:True 151:True 152:True 153:True 154:True 155:True 156:True 157:True 158:True 159:True 160:True 161:True 162:True 163:True 164:True 165:True 166:True 167:True 168:True 169:True 170:True 171:True 172:True 173:True 174:True\n",
            "I0420 19:06:20.398208 140407150372736 squad_lib.py:368] input_ids: 101 1996 13546 1997 1996 6730 2540 2012 10289 8214 2003 3875 2000 2029 3252 1029 102 6549 2135 1010 1996 2082 2038 1037 3234 2839 1012 10234 1996 2364 2311 1005 1055 2751 8514 2003 1037 3585 6231 1997 1996 6261 2984 1012 3202 1999 2392 1997 1996 2364 2311 1998 5307 2009 1010 2003 1037 6967 6231 1997 4828 2007 2608 2039 14995 6924 2007 1996 5722 1000 2310 3490 2618 4748 2033 18168 5267 1000 1012 2279 2000 1996 2364 2311 2003 1996 13546 1997 1996 6730 2540 1012 3202 2369 1996 13546 2003 1996 24665 23052 1010 1037 14042 2173 1997 7083 1998 9185 1012 2009 2003 1037 15059 1997 1996 24665 23052 2012 10223 26371 1010 2605 2073 1996 6261 2984 22353 2135 2596 2000 3002 16595 9648 4674 2061 12083 9711 2271 1999 8517 1012 2012 1996 2203 1997 1996 2364 3298 1006 1998 1999 1037 3622 2240 2008 8539 2083 1017 11342 1998 1996 2751 8514 1007 1010 2003 1037 3722 1010 2715 2962 6231 1997 2984 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0420 19:06:20.398447 140407150372736 squad_lib.py:369] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0420 19:06:20.398700 140407150372736 squad_lib.py:370] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0420 19:06:20.398815 140407150372736 squad_lib.py:375] start_position: 81\n",
            "I0420 19:06:20.398899 140407150372736 squad_lib.py:376] end_position: 83\n",
            "I0420 19:06:20.398979 140407150372736 squad_lib.py:377] answer: the main building\n",
            "I0420 19:06:20.402404 140407150372736 squad_lib.py:353] *** Example ***\n",
            "I0420 19:06:20.402509 140407150372736 squad_lib.py:354] unique_id: 1000000003\n",
            "I0420 19:06:20.402597 140407150372736 squad_lib.py:355] example_index: 3\n",
            "I0420 19:06:20.402661 140407150372736 squad_lib.py:356] doc_span_index: 0\n",
            "I0420 19:06:20.402790 140407150372736 squad_lib.py:358] tokens: [CLS] what is the gr ##otto at notre dame ? [SEP] architectural ##ly , the school has a catholic character . atop the main building ' s gold dome is a golden statue of the virgin mary . immediately in front of the main building and facing it , is a copper statue of christ with arms up ##rai ##sed with the legend \" ve ##ni ##te ad me om ##nes \" . next to the main building is the basilica of the sacred heart . immediately behind the basilica is the gr ##otto , a marian place of prayer and reflection . it is a replica of the gr ##otto at lou ##rdes , france where the virgin mary reputed ##ly appeared to saint bern ##ade ##tte so ##ub ##iro ##us in 1858 . at the end of the main drive ( and in a direct line that connects through 3 statues and the gold dome ) , is a simple , modern stone statue of mary . [SEP]\n",
            "I0420 19:06:20.402917 140407150372736 squad_lib.py:361] token_to_orig_map: 11:0 12:0 13:0 14:1 15:2 16:3 17:4 18:5 19:6 20:6 21:7 22:8 23:9 24:10 25:10 26:10 27:11 28:12 29:13 30:14 31:15 32:16 33:17 34:18 35:19 36:20 37:20 38:21 39:22 40:23 41:24 42:25 43:26 44:27 45:28 46:29 47:30 48:30 49:31 50:32 51:33 52:34 53:35 54:36 55:37 56:38 57:39 58:39 59:39 60:40 61:41 62:42 63:43 64:43 65:43 66:43 67:44 68:45 69:46 70:46 71:46 72:46 73:47 74:48 75:49 76:50 77:51 78:52 79:53 80:54 81:55 82:56 83:57 84:58 85:58 86:59 87:60 88:61 89:62 90:63 91:64 92:65 93:65 94:65 95:66 96:67 97:68 98:69 99:70 100:71 101:72 102:72 103:73 104:74 105:75 106:76 107:77 108:78 109:79 110:79 111:80 112:81 113:81 114:81 115:82 116:83 117:84 118:85 119:86 120:87 121:87 122:88 123:89 124:90 125:91 126:91 127:91 128:92 129:92 130:92 131:92 132:93 133:94 134:94 135:95 136:96 137:97 138:98 139:99 140:100 141:101 142:102 143:102 144:103 145:104 146:105 147:106 148:107 149:108 150:109 151:110 152:111 153:112 154:113 155:114 156:115 157:115 158:115 159:116 160:117 161:118 162:118 163:119 164:120 165:121 166:122 167:123 168:123\n",
            "I0420 19:06:20.403048 140407150372736 squad_lib.py:366] token_is_max_context: 11:True 12:True 13:True 14:True 15:True 16:True 17:True 18:True 19:True 20:True 21:True 22:True 23:True 24:True 25:True 26:True 27:True 28:True 29:True 30:True 31:True 32:True 33:True 34:True 35:True 36:True 37:True 38:True 39:True 40:True 41:True 42:True 43:True 44:True 45:True 46:True 47:True 48:True 49:True 50:True 51:True 52:True 53:True 54:True 55:True 56:True 57:True 58:True 59:True 60:True 61:True 62:True 63:True 64:True 65:True 66:True 67:True 68:True 69:True 70:True 71:True 72:True 73:True 74:True 75:True 76:True 77:True 78:True 79:True 80:True 81:True 82:True 83:True 84:True 85:True 86:True 87:True 88:True 89:True 90:True 91:True 92:True 93:True 94:True 95:True 96:True 97:True 98:True 99:True 100:True 101:True 102:True 103:True 104:True 105:True 106:True 107:True 108:True 109:True 110:True 111:True 112:True 113:True 114:True 115:True 116:True 117:True 118:True 119:True 120:True 121:True 122:True 123:True 124:True 125:True 126:True 127:True 128:True 129:True 130:True 131:True 132:True 133:True 134:True 135:True 136:True 137:True 138:True 139:True 140:True 141:True 142:True 143:True 144:True 145:True 146:True 147:True 148:True 149:True 150:True 151:True 152:True 153:True 154:True 155:True 156:True 157:True 158:True 159:True 160:True 161:True 162:True 163:True 164:True 165:True 166:True 167:True 168:True\n",
            "I0420 19:06:20.403225 140407150372736 squad_lib.py:368] input_ids: 101 2054 2003 1996 24665 23052 2012 10289 8214 1029 102 6549 2135 1010 1996 2082 2038 1037 3234 2839 1012 10234 1996 2364 2311 1005 1055 2751 8514 2003 1037 3585 6231 1997 1996 6261 2984 1012 3202 1999 2392 1997 1996 2364 2311 1998 5307 2009 1010 2003 1037 6967 6231 1997 4828 2007 2608 2039 14995 6924 2007 1996 5722 1000 2310 3490 2618 4748 2033 18168 5267 1000 1012 2279 2000 1996 2364 2311 2003 1996 13546 1997 1996 6730 2540 1012 3202 2369 1996 13546 2003 1996 24665 23052 1010 1037 14042 2173 1997 7083 1998 9185 1012 2009 2003 1037 15059 1997 1996 24665 23052 2012 10223 26371 1010 2605 2073 1996 6261 2984 22353 2135 2596 2000 3002 16595 9648 4674 2061 12083 9711 2271 1999 8517 1012 2012 1996 2203 1997 1996 2364 3298 1006 1998 1999 1037 3622 2240 2008 8539 2083 1017 11342 1998 1996 2751 8514 1007 1010 2003 1037 3722 1010 2715 2962 6231 1997 2984 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0420 19:06:20.403415 140407150372736 squad_lib.py:369] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0420 19:06:20.403613 140407150372736 squad_lib.py:370] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0420 19:06:20.403676 140407150372736 squad_lib.py:375] start_position: 95\n",
            "I0420 19:06:20.403734 140407150372736 squad_lib.py:376] end_position: 101\n",
            "I0420 19:06:20.403793 140407150372736 squad_lib.py:377] answer: a marian place of prayer and reflection\n",
            "I0420 19:06:20.406926 140407150372736 squad_lib.py:353] *** Example ***\n",
            "I0420 19:06:20.407015 140407150372736 squad_lib.py:354] unique_id: 1000000004\n",
            "I0420 19:06:20.407087 140407150372736 squad_lib.py:355] example_index: 4\n",
            "I0420 19:06:20.407145 140407150372736 squad_lib.py:356] doc_span_index: 0\n",
            "I0420 19:06:20.407271 140407150372736 squad_lib.py:358] tokens: [CLS] what sits on top of the main building at notre dame ? [SEP] architectural ##ly , the school has a catholic character . atop the main building ' s gold dome is a golden statue of the virgin mary . immediately in front of the main building and facing it , is a copper statue of christ with arms up ##rai ##sed with the legend \" ve ##ni ##te ad me om ##nes \" . next to the main building is the basilica of the sacred heart . immediately behind the basilica is the gr ##otto , a marian place of prayer and reflection . it is a replica of the gr ##otto at lou ##rdes , france where the virgin mary reputed ##ly appeared to saint bern ##ade ##tte so ##ub ##iro ##us in 1858 . at the end of the main drive ( and in a direct line that connects through 3 statues and the gold dome ) , is a simple , modern stone statue of mary . [SEP]\n",
            "I0420 19:06:20.407396 140407150372736 squad_lib.py:361] token_to_orig_map: 14:0 15:0 16:0 17:1 18:2 19:3 20:4 21:5 22:6 23:6 24:7 25:8 26:9 27:10 28:10 29:10 30:11 31:12 32:13 33:14 34:15 35:16 36:17 37:18 38:19 39:20 40:20 41:21 42:22 43:23 44:24 45:25 46:26 47:27 48:28 49:29 50:30 51:30 52:31 53:32 54:33 55:34 56:35 57:36 58:37 59:38 60:39 61:39 62:39 63:40 64:41 65:42 66:43 67:43 68:43 69:43 70:44 71:45 72:46 73:46 74:46 75:46 76:47 77:48 78:49 79:50 80:51 81:52 82:53 83:54 84:55 85:56 86:57 87:58 88:58 89:59 90:60 91:61 92:62 93:63 94:64 95:65 96:65 97:65 98:66 99:67 100:68 101:69 102:70 103:71 104:72 105:72 106:73 107:74 108:75 109:76 110:77 111:78 112:79 113:79 114:80 115:81 116:81 117:81 118:82 119:83 120:84 121:85 122:86 123:87 124:87 125:88 126:89 127:90 128:91 129:91 130:91 131:92 132:92 133:92 134:92 135:93 136:94 137:94 138:95 139:96 140:97 141:98 142:99 143:100 144:101 145:102 146:102 147:103 148:104 149:105 150:106 151:107 152:108 153:109 154:110 155:111 156:112 157:113 158:114 159:115 160:115 161:115 162:116 163:117 164:118 165:118 166:119 167:120 168:121 169:122 170:123 171:123\n",
            "I0420 19:06:20.407515 140407150372736 squad_lib.py:366] token_is_max_context: 14:True 15:True 16:True 17:True 18:True 19:True 20:True 21:True 22:True 23:True 24:True 25:True 26:True 27:True 28:True 29:True 30:True 31:True 32:True 33:True 34:True 35:True 36:True 37:True 38:True 39:True 40:True 41:True 42:True 43:True 44:True 45:True 46:True 47:True 48:True 49:True 50:True 51:True 52:True 53:True 54:True 55:True 56:True 57:True 58:True 59:True 60:True 61:True 62:True 63:True 64:True 65:True 66:True 67:True 68:True 69:True 70:True 71:True 72:True 73:True 74:True 75:True 76:True 77:True 78:True 79:True 80:True 81:True 82:True 83:True 84:True 85:True 86:True 87:True 88:True 89:True 90:True 91:True 92:True 93:True 94:True 95:True 96:True 97:True 98:True 99:True 100:True 101:True 102:True 103:True 104:True 105:True 106:True 107:True 108:True 109:True 110:True 111:True 112:True 113:True 114:True 115:True 116:True 117:True 118:True 119:True 120:True 121:True 122:True 123:True 124:True 125:True 126:True 127:True 128:True 129:True 130:True 131:True 132:True 133:True 134:True 135:True 136:True 137:True 138:True 139:True 140:True 141:True 142:True 143:True 144:True 145:True 146:True 147:True 148:True 149:True 150:True 151:True 152:True 153:True 154:True 155:True 156:True 157:True 158:True 159:True 160:True 161:True 162:True 163:True 164:True 165:True 166:True 167:True 168:True 169:True 170:True 171:True\n",
            "I0420 19:06:20.407685 140407150372736 squad_lib.py:368] input_ids: 101 2054 7719 2006 2327 1997 1996 2364 2311 2012 10289 8214 1029 102 6549 2135 1010 1996 2082 2038 1037 3234 2839 1012 10234 1996 2364 2311 1005 1055 2751 8514 2003 1037 3585 6231 1997 1996 6261 2984 1012 3202 1999 2392 1997 1996 2364 2311 1998 5307 2009 1010 2003 1037 6967 6231 1997 4828 2007 2608 2039 14995 6924 2007 1996 5722 1000 2310 3490 2618 4748 2033 18168 5267 1000 1012 2279 2000 1996 2364 2311 2003 1996 13546 1997 1996 6730 2540 1012 3202 2369 1996 13546 2003 1996 24665 23052 1010 1037 14042 2173 1997 7083 1998 9185 1012 2009 2003 1037 15059 1997 1996 24665 23052 2012 10223 26371 1010 2605 2073 1996 6261 2984 22353 2135 2596 2000 3002 16595 9648 4674 2061 12083 9711 2271 1999 8517 1012 2012 1996 2203 1997 1996 2364 3298 1006 1998 1999 1037 3622 2240 2008 8539 2083 1017 11342 1998 1996 2751 8514 1007 1010 2003 1037 3722 1010 2715 2962 6231 1997 2984 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0420 19:06:20.407848 140407150372736 squad_lib.py:369] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0420 19:06:20.408009 140407150372736 squad_lib.py:370] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0420 19:06:20.408068 140407150372736 squad_lib.py:375] start_position: 33\n",
            "I0420 19:06:20.408127 140407150372736 squad_lib.py:376] end_position: 39\n",
            "I0420 19:06:20.408182 140407150372736 squad_lib.py:377] answer: a golden statue of the virgin mary\n",
            "I0420 19:06:20.413781 140407150372736 squad_lib.py:353] *** Example ***\n",
            "I0420 19:06:20.413871 140407150372736 squad_lib.py:354] unique_id: 1000000005\n",
            "I0420 19:06:20.413950 140407150372736 squad_lib.py:355] example_index: 5\n",
            "I0420 19:06:20.414009 140407150372736 squad_lib.py:356] doc_span_index: 0\n",
            "I0420 19:06:20.414151 140407150372736 squad_lib.py:358] tokens: [CLS] when did the scholastic magazine of notre dame begin publishing ? [SEP] as at most other universities , notre dame ' s students run a number of news media outlets . the nine student - run outlets include three newspapers , both a radio and television station , and several magazines and journals . begun as a one - page journal in september 1876 , the scholastic magazine is issued twice monthly and claims to be the oldest continuous collegiate publication in the united states . the other magazine , the jug ##gler , is released twice a year and focuses on student literature and artwork . the dome yearbook is published annually . the newspapers have varying publication interests , with the observer published daily and mainly reporting university and other news , and staffed by students from both notre dame and saint mary ' s college . unlike scholastic and the dome , the observer is an independent publication and does not have a faculty advisor or any editorial oversight from the university . in 1987 , when some students believed that the observer began to show a conservative bias , a liberal newspaper , common sense was published . likewise , in 2003 , when other students believed that the paper showed a liberal bias , the conservative paper irish rover went into production . neither paper is published as often as the observer ; however , all three are distributed to all students . finally , in spring 2008 an undergraduate journal for political science research , beyond politics , made its debut . [SEP]\n",
            "I0420 19:06:20.499629 140407150372736 squad_lib.py:361] token_to_orig_map: 13:0 14:1 15:2 16:3 17:4 18:4 19:5 20:6 21:6 22:6 23:7 24:8 25:9 26:10 27:11 28:12 29:13 30:14 31:14 32:15 33:16 34:17 35:17 36:17 37:18 38:19 39:20 40:21 41:21 42:22 43:23 44:24 45:25 46:26 47:27 48:27 49:28 50:29 51:30 52:31 53:32 54:32 55:33 56:34 57:35 58:36 59:36 60:36 61:37 62:38 63:39 64:40 65:40 66:41 67:42 68:43 69:44 70:45 71:46 72:47 73:48 74:49 75:50 76:51 77:52 78:53 79:54 80:55 81:56 82:57 83:58 84:59 85:60 86:60 87:61 88:62 89:63 90:63 91:64 92:65 93:65 94:65 95:66 96:67 97:68 98:69 99:70 100:71 101:72 102:73 103:74 104:75 105:76 106:77 107:77 108:78 109:79 110:80 111:81 112:82 113:83 114:83 115:84 116:85 117:86 118:87 119:88 120:89 121:89 122:90 123:91 124:92 125:93 126:94 127:95 128:96 129:97 130:98 131:99 132:100 133:101 134:101 135:102 136:103 137:104 138:105 139:106 140:107 141:108 142:109 143:110 144:111 145:112 146:112 147:112 148:113 149:113 150:114 151:115 152:116 153:117 154:118 155:118 156:119 157:120 158:121 159:122 160:123 161:124 162:125 163:126 164:127 165:128 166:129 167:130 168:131 169:132 170:133 171:134 172:135 173:136 174:137 175:138 176:138 177:139 178:140 179:140 180:141 181:142 182:143 183:144 184:145 185:146 186:147 187:148 188:149 189:150 190:151 191:152 192:153 193:153 194:154 195:155 196:156 197:156 198:157 199:158 200:159 201:160 202:160 203:161 204:161 205:162 206:163 207:163 208:164 209:165 210:166 211:167 212:168 213:169 214:170 215:171 216:172 217:173 218:174 219:174 220:175 221:176 222:177 223:178 224:179 225:180 226:181 227:182 228:182 229:183 230:184 231:185 232:186 233:187 234:188 235:189 236:190 237:191 238:191 239:192 240:192 241:193 242:194 243:195 244:196 245:197 246:198 247:199 248:199 249:200 250:200 251:201 252:202 253:203 254:204 255:205 256:206 257:207 258:208 259:209 260:210 261:210 262:211 263:212 264:212 265:213 266:214 267:215 268:215\n",
            "I0420 19:06:20.499875 140407150372736 squad_lib.py:366] token_is_max_context: 13:True 14:True 15:True 16:True 17:True 18:True 19:True 20:True 21:True 22:True 23:True 24:True 25:True 26:True 27:True 28:True 29:True 30:True 31:True 32:True 33:True 34:True 35:True 36:True 37:True 38:True 39:True 40:True 41:True 42:True 43:True 44:True 45:True 46:True 47:True 48:True 49:True 50:True 51:True 52:True 53:True 54:True 55:True 56:True 57:True 58:True 59:True 60:True 61:True 62:True 63:True 64:True 65:True 66:True 67:True 68:True 69:True 70:True 71:True 72:True 73:True 74:True 75:True 76:True 77:True 78:True 79:True 80:True 81:True 82:True 83:True 84:True 85:True 86:True 87:True 88:True 89:True 90:True 91:True 92:True 93:True 94:True 95:True 96:True 97:True 98:True 99:True 100:True 101:True 102:True 103:True 104:True 105:True 106:True 107:True 108:True 109:True 110:True 111:True 112:True 113:True 114:True 115:True 116:True 117:True 118:True 119:True 120:True 121:True 122:True 123:True 124:True 125:True 126:True 127:True 128:True 129:True 130:True 131:True 132:True 133:True 134:True 135:True 136:True 137:True 138:True 139:True 140:True 141:True 142:True 143:True 144:True 145:True 146:True 147:True 148:True 149:True 150:True 151:True 152:True 153:True 154:True 155:True 156:True 157:True 158:True 159:True 160:True 161:True 162:True 163:True 164:True 165:True 166:True 167:True 168:True 169:True 170:True 171:True 172:True 173:True 174:True 175:True 176:True 177:True 178:True 179:True 180:True 181:True 182:True 183:True 184:True 185:True 186:True 187:True 188:True 189:True 190:True 191:True 192:True 193:True 194:True 195:True 196:True 197:True 198:True 199:True 200:True 201:True 202:True 203:True 204:True 205:True 206:True 207:True 208:True 209:True 210:True 211:True 212:True 213:True 214:True 215:True 216:True 217:True 218:True 219:True 220:True 221:True 222:True 223:True 224:True 225:True 226:True 227:True 228:True 229:True 230:True 231:True 232:True 233:True 234:True 235:True 236:True 237:True 238:True 239:True 240:True 241:True 242:True 243:True 244:True 245:True 246:True 247:True 248:True 249:True 250:True 251:True 252:True 253:True 254:True 255:True 256:True 257:True 258:True 259:True 260:True 261:True 262:True 263:True 264:True 265:True 266:True 267:True 268:True\n",
            "I0420 19:06:20.500146 140407150372736 squad_lib.py:368] input_ids: 101 2043 2106 1996 24105 2932 1997 10289 8214 4088 4640 1029 102 2004 2012 2087 2060 5534 1010 10289 8214 1005 1055 2493 2448 1037 2193 1997 2739 2865 11730 1012 1996 3157 3076 1011 2448 11730 2421 2093 6399 1010 2119 1037 2557 1998 2547 2276 1010 1998 2195 7298 1998 9263 1012 5625 2004 1037 2028 1011 3931 3485 1999 2244 7326 1010 1996 24105 2932 2003 3843 3807 7058 1998 4447 2000 2022 1996 4587 7142 9234 4772 1999 1996 2142 2163 1012 1996 2060 2932 1010 1996 26536 17420 1010 2003 2207 3807 1037 2095 1998 7679 2006 3076 3906 1998 8266 1012 1996 8514 24803 2003 2405 6604 1012 1996 6399 2031 9671 4772 5426 1010 2007 1996 9718 2405 3679 1998 3701 7316 2118 1998 2060 2739 1010 1998 21121 2011 2493 2013 2119 10289 8214 1998 3002 2984 1005 1055 2267 1012 4406 24105 1998 1996 8514 1010 1996 9718 2003 2019 2981 4772 1998 2515 2025 2031 1037 4513 8619 2030 2151 8368 15709 2013 1996 2118 1012 1999 3055 1010 2043 2070 2493 3373 2008 1996 9718 2211 2000 2265 1037 4603 13827 1010 1037 4314 3780 1010 2691 3168 2001 2405 1012 10655 1010 1999 2494 1010 2043 2060 2493 3373 2008 1996 3259 3662 1037 4314 13827 1010 1996 4603 3259 3493 13631 2253 2046 2537 1012 4445 3259 2003 2405 2004 2411 2004 1996 9718 1025 2174 1010 2035 2093 2024 5500 2000 2035 2493 1012 2633 1010 1999 3500 2263 2019 8324 3485 2005 2576 2671 2470 1010 3458 4331 1010 2081 2049 2834 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0420 19:06:20.500410 140407150372736 squad_lib.py:369] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0420 19:06:20.500638 140407150372736 squad_lib.py:370] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0420 19:06:20.500725 140407150372736 squad_lib.py:375] start_position: 63\n",
            "I0420 19:06:20.500789 140407150372736 squad_lib.py:376] end_position: 64\n",
            "I0420 19:06:20.500854 140407150372736 squad_lib.py:377] answer: september 1876\n",
            "I0420 19:06:20.505698 140407150372736 squad_lib.py:353] *** Example ***\n",
            "I0420 19:06:20.505800 140407150372736 squad_lib.py:354] unique_id: 1000000006\n",
            "I0420 19:06:20.505888 140407150372736 squad_lib.py:355] example_index: 6\n",
            "I0420 19:06:20.505951 140407150372736 squad_lib.py:356] doc_span_index: 0\n",
            "I0420 19:06:20.506116 140407150372736 squad_lib.py:358] tokens: [CLS] how often is notre dame ' s the jug ##gler published ? [SEP] as at most other universities , notre dame ' s students run a number of news media outlets . the nine student - run outlets include three newspapers , both a radio and television station , and several magazines and journals . begun as a one - page journal in september 1876 , the scholastic magazine is issued twice monthly and claims to be the oldest continuous collegiate publication in the united states . the other magazine , the jug ##gler , is released twice a year and focuses on student literature and artwork . the dome yearbook is published annually . the newspapers have varying publication interests , with the observer published daily and mainly reporting university and other news , and staffed by students from both notre dame and saint mary ' s college . unlike scholastic and the dome , the observer is an independent publication and does not have a faculty advisor or any editorial oversight from the university . in 1987 , when some students believed that the observer began to show a conservative bias , a liberal newspaper , common sense was published . likewise , in 2003 , when other students believed that the paper showed a liberal bias , the conservative paper irish rover went into production . neither paper is published as often as the observer ; however , all three are distributed to all students . finally , in spring 2008 an undergraduate journal for political science research , beyond politics , made its debut . [SEP]\n",
            "I0420 19:06:20.506258 140407150372736 squad_lib.py:361] token_to_orig_map: 14:0 15:1 16:2 17:3 18:4 19:4 20:5 21:6 22:6 23:6 24:7 25:8 26:9 27:10 28:11 29:12 30:13 31:14 32:14 33:15 34:16 35:17 36:17 37:17 38:18 39:19 40:20 41:21 42:21 43:22 44:23 45:24 46:25 47:26 48:27 49:27 50:28 51:29 52:30 53:31 54:32 55:32 56:33 57:34 58:35 59:36 60:36 61:36 62:37 63:38 64:39 65:40 66:40 67:41 68:42 69:43 70:44 71:45 72:46 73:47 74:48 75:49 76:50 77:51 78:52 79:53 80:54 81:55 82:56 83:57 84:58 85:59 86:60 87:60 88:61 89:62 90:63 91:63 92:64 93:65 94:65 95:65 96:66 97:67 98:68 99:69 100:70 101:71 102:72 103:73 104:74 105:75 106:76 107:77 108:77 109:78 110:79 111:80 112:81 113:82 114:83 115:83 116:84 117:85 118:86 119:87 120:88 121:89 122:89 123:90 124:91 125:92 126:93 127:94 128:95 129:96 130:97 131:98 132:99 133:100 134:101 135:101 136:102 137:103 138:104 139:105 140:106 141:107 142:108 143:109 144:110 145:111 146:112 147:112 148:112 149:113 150:113 151:114 152:115 153:116 154:117 155:118 156:118 157:119 158:120 159:121 160:122 161:123 162:124 163:125 164:126 165:127 166:128 167:129 168:130 169:131 170:132 171:133 172:134 173:135 174:136 175:137 176:138 177:138 178:139 179:140 180:140 181:141 182:142 183:143 184:144 185:145 186:146 187:147 188:148 189:149 190:150 191:151 192:152 193:153 194:153 195:154 196:155 197:156 198:156 199:157 200:158 201:159 202:160 203:160 204:161 205:161 206:162 207:163 208:163 209:164 210:165 211:166 212:167 213:168 214:169 215:170 216:171 217:172 218:173 219:174 220:174 221:175 222:176 223:177 224:178 225:179 226:180 227:181 228:182 229:182 230:183 231:184 232:185 233:186 234:187 235:188 236:189 237:190 238:191 239:191 240:192 241:192 242:193 243:194 244:195 245:196 246:197 247:198 248:199 249:199 250:200 251:200 252:201 253:202 254:203 255:204 256:205 257:206 258:207 259:208 260:209 261:210 262:210 263:211 264:212 265:212 266:213 267:214 268:215 269:215\n",
            "I0420 19:06:20.506421 140407150372736 squad_lib.py:366] token_is_max_context: 14:True 15:True 16:True 17:True 18:True 19:True 20:True 21:True 22:True 23:True 24:True 25:True 26:True 27:True 28:True 29:True 30:True 31:True 32:True 33:True 34:True 35:True 36:True 37:True 38:True 39:True 40:True 41:True 42:True 43:True 44:True 45:True 46:True 47:True 48:True 49:True 50:True 51:True 52:True 53:True 54:True 55:True 56:True 57:True 58:True 59:True 60:True 61:True 62:True 63:True 64:True 65:True 66:True 67:True 68:True 69:True 70:True 71:True 72:True 73:True 74:True 75:True 76:True 77:True 78:True 79:True 80:True 81:True 82:True 83:True 84:True 85:True 86:True 87:True 88:True 89:True 90:True 91:True 92:True 93:True 94:True 95:True 96:True 97:True 98:True 99:True 100:True 101:True 102:True 103:True 104:True 105:True 106:True 107:True 108:True 109:True 110:True 111:True 112:True 113:True 114:True 115:True 116:True 117:True 118:True 119:True 120:True 121:True 122:True 123:True 124:True 125:True 126:True 127:True 128:True 129:True 130:True 131:True 132:True 133:True 134:True 135:True 136:True 137:True 138:True 139:True 140:True 141:True 142:True 143:True 144:True 145:True 146:True 147:True 148:True 149:True 150:True 151:True 152:True 153:True 154:True 155:True 156:True 157:True 158:True 159:True 160:True 161:True 162:True 163:True 164:True 165:True 166:True 167:True 168:True 169:True 170:True 171:True 172:True 173:True 174:True 175:True 176:True 177:True 178:True 179:True 180:True 181:True 182:True 183:True 184:True 185:True 186:True 187:True 188:True 189:True 190:True 191:True 192:True 193:True 194:True 195:True 196:True 197:True 198:True 199:True 200:True 201:True 202:True 203:True 204:True 205:True 206:True 207:True 208:True 209:True 210:True 211:True 212:True 213:True 214:True 215:True 216:True 217:True 218:True 219:True 220:True 221:True 222:True 223:True 224:True 225:True 226:True 227:True 228:True 229:True 230:True 231:True 232:True 233:True 234:True 235:True 236:True 237:True 238:True 239:True 240:True 241:True 242:True 243:True 244:True 245:True 246:True 247:True 248:True 249:True 250:True 251:True 252:True 253:True 254:True 255:True 256:True 257:True 258:True 259:True 260:True 261:True 262:True 263:True 264:True 265:True 266:True 267:True 268:True 269:True\n",
            "I0420 19:06:20.506612 140407150372736 squad_lib.py:368] input_ids: 101 2129 2411 2003 10289 8214 1005 1055 1996 26536 17420 2405 1029 102 2004 2012 2087 2060 5534 1010 10289 8214 1005 1055 2493 2448 1037 2193 1997 2739 2865 11730 1012 1996 3157 3076 1011 2448 11730 2421 2093 6399 1010 2119 1037 2557 1998 2547 2276 1010 1998 2195 7298 1998 9263 1012 5625 2004 1037 2028 1011 3931 3485 1999 2244 7326 1010 1996 24105 2932 2003 3843 3807 7058 1998 4447 2000 2022 1996 4587 7142 9234 4772 1999 1996 2142 2163 1012 1996 2060 2932 1010 1996 26536 17420 1010 2003 2207 3807 1037 2095 1998 7679 2006 3076 3906 1998 8266 1012 1996 8514 24803 2003 2405 6604 1012 1996 6399 2031 9671 4772 5426 1010 2007 1996 9718 2405 3679 1998 3701 7316 2118 1998 2060 2739 1010 1998 21121 2011 2493 2013 2119 10289 8214 1998 3002 2984 1005 1055 2267 1012 4406 24105 1998 1996 8514 1010 1996 9718 2003 2019 2981 4772 1998 2515 2025 2031 1037 4513 8619 2030 2151 8368 15709 2013 1996 2118 1012 1999 3055 1010 2043 2070 2493 3373 2008 1996 9718 2211 2000 2265 1037 4603 13827 1010 1037 4314 3780 1010 2691 3168 2001 2405 1012 10655 1010 1999 2494 1010 2043 2060 2493 3373 2008 1996 3259 3662 1037 4314 13827 1010 1996 4603 3259 3493 13631 2253 2046 2537 1012 4445 3259 2003 2405 2004 2411 2004 1996 9718 1025 2174 1010 2035 2093 2024 5500 2000 2035 2493 1012 2633 1010 1999 3500 2263 2019 8324 3485 2005 2576 2671 2470 1010 3458 4331 1010 2081 2049 2834 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0420 19:06:20.506803 140407150372736 squad_lib.py:369] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0420 19:06:20.506970 140407150372736 squad_lib.py:370] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0420 19:06:20.507040 140407150372736 squad_lib.py:375] start_position: 98\n",
            "I0420 19:06:20.507099 140407150372736 squad_lib.py:376] end_position: 98\n",
            "I0420 19:06:20.507154 140407150372736 squad_lib.py:377] answer: twice\n",
            "I0420 19:06:20.512073 140407150372736 squad_lib.py:353] *** Example ***\n",
            "I0420 19:06:20.512163 140407150372736 squad_lib.py:354] unique_id: 1000000007\n",
            "I0420 19:06:20.512233 140407150372736 squad_lib.py:355] example_index: 7\n",
            "I0420 19:06:20.512290 140407150372736 squad_lib.py:356] doc_span_index: 0\n",
            "I0420 19:06:20.512438 140407150372736 squad_lib.py:358] tokens: [CLS] what is the daily student paper at notre dame called ? [SEP] as at most other universities , notre dame ' s students run a number of news media outlets . the nine student - run outlets include three newspapers , both a radio and television station , and several magazines and journals . begun as a one - page journal in september 1876 , the scholastic magazine is issued twice monthly and claims to be the oldest continuous collegiate publication in the united states . the other magazine , the jug ##gler , is released twice a year and focuses on student literature and artwork . the dome yearbook is published annually . the newspapers have varying publication interests , with the observer published daily and mainly reporting university and other news , and staffed by students from both notre dame and saint mary ' s college . unlike scholastic and the dome , the observer is an independent publication and does not have a faculty advisor or any editorial oversight from the university . in 1987 , when some students believed that the observer began to show a conservative bias , a liberal newspaper , common sense was published . likewise , in 2003 , when other students believed that the paper showed a liberal bias , the conservative paper irish rover went into production . neither paper is published as often as the observer ; however , all three are distributed to all students . finally , in spring 2008 an undergraduate journal for political science research , beyond politics , made its debut . [SEP]\n",
            "I0420 19:06:20.602555 140407150372736 squad_lib.py:361] token_to_orig_map: 13:0 14:1 15:2 16:3 17:4 18:4 19:5 20:6 21:6 22:6 23:7 24:8 25:9 26:10 27:11 28:12 29:13 30:14 31:14 32:15 33:16 34:17 35:17 36:17 37:18 38:19 39:20 40:21 41:21 42:22 43:23 44:24 45:25 46:26 47:27 48:27 49:28 50:29 51:30 52:31 53:32 54:32 55:33 56:34 57:35 58:36 59:36 60:36 61:37 62:38 63:39 64:40 65:40 66:41 67:42 68:43 69:44 70:45 71:46 72:47 73:48 74:49 75:50 76:51 77:52 78:53 79:54 80:55 81:56 82:57 83:58 84:59 85:60 86:60 87:61 88:62 89:63 90:63 91:64 92:65 93:65 94:65 95:66 96:67 97:68 98:69 99:70 100:71 101:72 102:73 103:74 104:75 105:76 106:77 107:77 108:78 109:79 110:80 111:81 112:82 113:83 114:83 115:84 116:85 117:86 118:87 119:88 120:89 121:89 122:90 123:91 124:92 125:93 126:94 127:95 128:96 129:97 130:98 131:99 132:100 133:101 134:101 135:102 136:103 137:104 138:105 139:106 140:107 141:108 142:109 143:110 144:111 145:112 146:112 147:112 148:113 149:113 150:114 151:115 152:116 153:117 154:118 155:118 156:119 157:120 158:121 159:122 160:123 161:124 162:125 163:126 164:127 165:128 166:129 167:130 168:131 169:132 170:133 171:134 172:135 173:136 174:137 175:138 176:138 177:139 178:140 179:140 180:141 181:142 182:143 183:144 184:145 185:146 186:147 187:148 188:149 189:150 190:151 191:152 192:153 193:153 194:154 195:155 196:156 197:156 198:157 199:158 200:159 201:160 202:160 203:161 204:161 205:162 206:163 207:163 208:164 209:165 210:166 211:167 212:168 213:169 214:170 215:171 216:172 217:173 218:174 219:174 220:175 221:176 222:177 223:178 224:179 225:180 226:181 227:182 228:182 229:183 230:184 231:185 232:186 233:187 234:188 235:189 236:190 237:191 238:191 239:192 240:192 241:193 242:194 243:195 244:196 245:197 246:198 247:199 248:199 249:200 250:200 251:201 252:202 253:203 254:204 255:205 256:206 257:207 258:208 259:209 260:210 261:210 262:211 263:212 264:212 265:213 266:214 267:215 268:215\n",
            "I0420 19:06:20.602871 140407150372736 squad_lib.py:366] token_is_max_context: 13:True 14:True 15:True 16:True 17:True 18:True 19:True 20:True 21:True 22:True 23:True 24:True 25:True 26:True 27:True 28:True 29:True 30:True 31:True 32:True 33:True 34:True 35:True 36:True 37:True 38:True 39:True 40:True 41:True 42:True 43:True 44:True 45:True 46:True 47:True 48:True 49:True 50:True 51:True 52:True 53:True 54:True 55:True 56:True 57:True 58:True 59:True 60:True 61:True 62:True 63:True 64:True 65:True 66:True 67:True 68:True 69:True 70:True 71:True 72:True 73:True 74:True 75:True 76:True 77:True 78:True 79:True 80:True 81:True 82:True 83:True 84:True 85:True 86:True 87:True 88:True 89:True 90:True 91:True 92:True 93:True 94:True 95:True 96:True 97:True 98:True 99:True 100:True 101:True 102:True 103:True 104:True 105:True 106:True 107:True 108:True 109:True 110:True 111:True 112:True 113:True 114:True 115:True 116:True 117:True 118:True 119:True 120:True 121:True 122:True 123:True 124:True 125:True 126:True 127:True 128:True 129:True 130:True 131:True 132:True 133:True 134:True 135:True 136:True 137:True 138:True 139:True 140:True 141:True 142:True 143:True 144:True 145:True 146:True 147:True 148:True 149:True 150:True 151:True 152:True 153:True 154:True 155:True 156:True 157:True 158:True 159:True 160:True 161:True 162:True 163:True 164:True 165:True 166:True 167:True 168:True 169:True 170:True 171:True 172:True 173:True 174:True 175:True 176:True 177:True 178:True 179:True 180:True 181:True 182:True 183:True 184:True 185:True 186:True 187:True 188:True 189:True 190:True 191:True 192:True 193:True 194:True 195:True 196:True 197:True 198:True 199:True 200:True 201:True 202:True 203:True 204:True 205:True 206:True 207:True 208:True 209:True 210:True 211:True 212:True 213:True 214:True 215:True 216:True 217:True 218:True 219:True 220:True 221:True 222:True 223:True 224:True 225:True 226:True 227:True 228:True 229:True 230:True 231:True 232:True 233:True 234:True 235:True 236:True 237:True 238:True 239:True 240:True 241:True 242:True 243:True 244:True 245:True 246:True 247:True 248:True 249:True 250:True 251:True 252:True 253:True 254:True 255:True 256:True 257:True 258:True 259:True 260:True 261:True 262:True 263:True 264:True 265:True 266:True 267:True 268:True\n",
            "I0420 19:06:20.603205 140407150372736 squad_lib.py:368] input_ids: 101 2054 2003 1996 3679 3076 3259 2012 10289 8214 2170 1029 102 2004 2012 2087 2060 5534 1010 10289 8214 1005 1055 2493 2448 1037 2193 1997 2739 2865 11730 1012 1996 3157 3076 1011 2448 11730 2421 2093 6399 1010 2119 1037 2557 1998 2547 2276 1010 1998 2195 7298 1998 9263 1012 5625 2004 1037 2028 1011 3931 3485 1999 2244 7326 1010 1996 24105 2932 2003 3843 3807 7058 1998 4447 2000 2022 1996 4587 7142 9234 4772 1999 1996 2142 2163 1012 1996 2060 2932 1010 1996 26536 17420 1010 2003 2207 3807 1037 2095 1998 7679 2006 3076 3906 1998 8266 1012 1996 8514 24803 2003 2405 6604 1012 1996 6399 2031 9671 4772 5426 1010 2007 1996 9718 2405 3679 1998 3701 7316 2118 1998 2060 2739 1010 1998 21121 2011 2493 2013 2119 10289 8214 1998 3002 2984 1005 1055 2267 1012 4406 24105 1998 1996 8514 1010 1996 9718 2003 2019 2981 4772 1998 2515 2025 2031 1037 4513 8619 2030 2151 8368 15709 2013 1996 2118 1012 1999 3055 1010 2043 2070 2493 3373 2008 1996 9718 2211 2000 2265 1037 4603 13827 1010 1037 4314 3780 1010 2691 3168 2001 2405 1012 10655 1010 1999 2494 1010 2043 2060 2493 3373 2008 1996 3259 3662 1037 4314 13827 1010 1996 4603 3259 3493 13631 2253 2046 2537 1012 4445 3259 2003 2405 2004 2411 2004 1996 9718 1025 2174 1010 2035 2093 2024 5500 2000 2035 2493 1012 2633 1010 1999 3500 2263 2019 8324 3485 2005 2576 2671 2470 1010 3458 4331 1010 2081 2049 2834 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0420 19:06:20.603453 140407150372736 squad_lib.py:369] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0420 19:06:20.603777 140407150372736 squad_lib.py:370] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0420 19:06:20.603912 140407150372736 squad_lib.py:375] start_position: 123\n",
            "I0420 19:06:20.603993 140407150372736 squad_lib.py:376] end_position: 124\n",
            "I0420 19:06:20.604053 140407150372736 squad_lib.py:377] answer: the observer\n",
            "I0420 19:06:20.613746 140407150372736 squad_lib.py:353] *** Example ***\n",
            "I0420 19:06:20.613882 140407150372736 squad_lib.py:354] unique_id: 1000000008\n",
            "I0420 19:06:20.613978 140407150372736 squad_lib.py:355] example_index: 8\n",
            "I0420 19:06:20.614051 140407150372736 squad_lib.py:356] doc_span_index: 0\n",
            "I0420 19:06:20.614244 140407150372736 squad_lib.py:358] tokens: [CLS] how many student news papers are found at notre dame ? [SEP] as at most other universities , notre dame ' s students run a number of news media outlets . the nine student - run outlets include three newspapers , both a radio and television station , and several magazines and journals . begun as a one - page journal in september 1876 , the scholastic magazine is issued twice monthly and claims to be the oldest continuous collegiate publication in the united states . the other magazine , the jug ##gler , is released twice a year and focuses on student literature and artwork . the dome yearbook is published annually . the newspapers have varying publication interests , with the observer published daily and mainly reporting university and other news , and staffed by students from both notre dame and saint mary ' s college . unlike scholastic and the dome , the observer is an independent publication and does not have a faculty advisor or any editorial oversight from the university . in 1987 , when some students believed that the observer began to show a conservative bias , a liberal newspaper , common sense was published . likewise , in 2003 , when other students believed that the paper showed a liberal bias , the conservative paper irish rover went into production . neither paper is published as often as the observer ; however , all three are distributed to all students . finally , in spring 2008 an undergraduate journal for political science research , beyond politics , made its debut . [SEP]\n",
            "I0420 19:06:20.614451 140407150372736 squad_lib.py:361] token_to_orig_map: 13:0 14:1 15:2 16:3 17:4 18:4 19:5 20:6 21:6 22:6 23:7 24:8 25:9 26:10 27:11 28:12 29:13 30:14 31:14 32:15 33:16 34:17 35:17 36:17 37:18 38:19 39:20 40:21 41:21 42:22 43:23 44:24 45:25 46:26 47:27 48:27 49:28 50:29 51:30 52:31 53:32 54:32 55:33 56:34 57:35 58:36 59:36 60:36 61:37 62:38 63:39 64:40 65:40 66:41 67:42 68:43 69:44 70:45 71:46 72:47 73:48 74:49 75:50 76:51 77:52 78:53 79:54 80:55 81:56 82:57 83:58 84:59 85:60 86:60 87:61 88:62 89:63 90:63 91:64 92:65 93:65 94:65 95:66 96:67 97:68 98:69 99:70 100:71 101:72 102:73 103:74 104:75 105:76 106:77 107:77 108:78 109:79 110:80 111:81 112:82 113:83 114:83 115:84 116:85 117:86 118:87 119:88 120:89 121:89 122:90 123:91 124:92 125:93 126:94 127:95 128:96 129:97 130:98 131:99 132:100 133:101 134:101 135:102 136:103 137:104 138:105 139:106 140:107 141:108 142:109 143:110 144:111 145:112 146:112 147:112 148:113 149:113 150:114 151:115 152:116 153:117 154:118 155:118 156:119 157:120 158:121 159:122 160:123 161:124 162:125 163:126 164:127 165:128 166:129 167:130 168:131 169:132 170:133 171:134 172:135 173:136 174:137 175:138 176:138 177:139 178:140 179:140 180:141 181:142 182:143 183:144 184:145 185:146 186:147 187:148 188:149 189:150 190:151 191:152 192:153 193:153 194:154 195:155 196:156 197:156 198:157 199:158 200:159 201:160 202:160 203:161 204:161 205:162 206:163 207:163 208:164 209:165 210:166 211:167 212:168 213:169 214:170 215:171 216:172 217:173 218:174 219:174 220:175 221:176 222:177 223:178 224:179 225:180 226:181 227:182 228:182 229:183 230:184 231:185 232:186 233:187 234:188 235:189 236:190 237:191 238:191 239:192 240:192 241:193 242:194 243:195 244:196 245:197 246:198 247:199 248:199 249:200 250:200 251:201 252:202 253:203 254:204 255:205 256:206 257:207 258:208 259:209 260:210 261:210 262:211 263:212 264:212 265:213 266:214 267:215 268:215\n",
            "I0420 19:06:20.614662 140407150372736 squad_lib.py:366] token_is_max_context: 13:True 14:True 15:True 16:True 17:True 18:True 19:True 20:True 21:True 22:True 23:True 24:True 25:True 26:True 27:True 28:True 29:True 30:True 31:True 32:True 33:True 34:True 35:True 36:True 37:True 38:True 39:True 40:True 41:True 42:True 43:True 44:True 45:True 46:True 47:True 48:True 49:True 50:True 51:True 52:True 53:True 54:True 55:True 56:True 57:True 58:True 59:True 60:True 61:True 62:True 63:True 64:True 65:True 66:True 67:True 68:True 69:True 70:True 71:True 72:True 73:True 74:True 75:True 76:True 77:True 78:True 79:True 80:True 81:True 82:True 83:True 84:True 85:True 86:True 87:True 88:True 89:True 90:True 91:True 92:True 93:True 94:True 95:True 96:True 97:True 98:True 99:True 100:True 101:True 102:True 103:True 104:True 105:True 106:True 107:True 108:True 109:True 110:True 111:True 112:True 113:True 114:True 115:True 116:True 117:True 118:True 119:True 120:True 121:True 122:True 123:True 124:True 125:True 126:True 127:True 128:True 129:True 130:True 131:True 132:True 133:True 134:True 135:True 136:True 137:True 138:True 139:True 140:True 141:True 142:True 143:True 144:True 145:True 146:True 147:True 148:True 149:True 150:True 151:True 152:True 153:True 154:True 155:True 156:True 157:True 158:True 159:True 160:True 161:True 162:True 163:True 164:True 165:True 166:True 167:True 168:True 169:True 170:True 171:True 172:True 173:True 174:True 175:True 176:True 177:True 178:True 179:True 180:True 181:True 182:True 183:True 184:True 185:True 186:True 187:True 188:True 189:True 190:True 191:True 192:True 193:True 194:True 195:True 196:True 197:True 198:True 199:True 200:True 201:True 202:True 203:True 204:True 205:True 206:True 207:True 208:True 209:True 210:True 211:True 212:True 213:True 214:True 215:True 216:True 217:True 218:True 219:True 220:True 221:True 222:True 223:True 224:True 225:True 226:True 227:True 228:True 229:True 230:True 231:True 232:True 233:True 234:True 235:True 236:True 237:True 238:True 239:True 240:True 241:True 242:True 243:True 244:True 245:True 246:True 247:True 248:True 249:True 250:True 251:True 252:True 253:True 254:True 255:True 256:True 257:True 258:True 259:True 260:True 261:True 262:True 263:True 264:True 265:True 266:True 267:True 268:True\n",
            "I0420 19:06:20.701942 140407150372736 squad_lib.py:368] input_ids: 101 2129 2116 3076 2739 4981 2024 2179 2012 10289 8214 1029 102 2004 2012 2087 2060 5534 1010 10289 8214 1005 1055 2493 2448 1037 2193 1997 2739 2865 11730 1012 1996 3157 3076 1011 2448 11730 2421 2093 6399 1010 2119 1037 2557 1998 2547 2276 1010 1998 2195 7298 1998 9263 1012 5625 2004 1037 2028 1011 3931 3485 1999 2244 7326 1010 1996 24105 2932 2003 3843 3807 7058 1998 4447 2000 2022 1996 4587 7142 9234 4772 1999 1996 2142 2163 1012 1996 2060 2932 1010 1996 26536 17420 1010 2003 2207 3807 1037 2095 1998 7679 2006 3076 3906 1998 8266 1012 1996 8514 24803 2003 2405 6604 1012 1996 6399 2031 9671 4772 5426 1010 2007 1996 9718 2405 3679 1998 3701 7316 2118 1998 2060 2739 1010 1998 21121 2011 2493 2013 2119 10289 8214 1998 3002 2984 1005 1055 2267 1012 4406 24105 1998 1996 8514 1010 1996 9718 2003 2019 2981 4772 1998 2515 2025 2031 1037 4513 8619 2030 2151 8368 15709 2013 1996 2118 1012 1999 3055 1010 2043 2070 2493 3373 2008 1996 9718 2211 2000 2265 1037 4603 13827 1010 1037 4314 3780 1010 2691 3168 2001 2405 1012 10655 1010 1999 2494 1010 2043 2060 2493 3373 2008 1996 3259 3662 1037 4314 13827 1010 1996 4603 3259 3493 13631 2253 2046 2537 1012 4445 3259 2003 2405 2004 2411 2004 1996 9718 1025 2174 1010 2035 2093 2024 5500 2000 2035 2493 1012 2633 1010 1999 3500 2263 2019 8324 3485 2005 2576 2671 2470 1010 3458 4331 1010 2081 2049 2834 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0420 19:06:20.702340 140407150372736 squad_lib.py:369] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0420 19:06:20.702552 140407150372736 squad_lib.py:370] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0420 19:06:20.702658 140407150372736 squad_lib.py:375] start_position: 39\n",
            "I0420 19:06:20.702728 140407150372736 squad_lib.py:376] end_position: 39\n",
            "I0420 19:06:20.702792 140407150372736 squad_lib.py:377] answer: three\n",
            "I0420 19:06:20.708280 140407150372736 squad_lib.py:353] *** Example ***\n",
            "I0420 19:06:20.708385 140407150372736 squad_lib.py:354] unique_id: 1000000009\n",
            "I0420 19:06:20.708460 140407150372736 squad_lib.py:355] example_index: 9\n",
            "I0420 19:06:20.708519 140407150372736 squad_lib.py:356] doc_span_index: 0\n",
            "I0420 19:06:20.708673 140407150372736 squad_lib.py:358] tokens: [CLS] in what year did the student paper common sense begin publication at notre dame ? [SEP] as at most other universities , notre dame ' s students run a number of news media outlets . the nine student - run outlets include three newspapers , both a radio and television station , and several magazines and journals . begun as a one - page journal in september 1876 , the scholastic magazine is issued twice monthly and claims to be the oldest continuous collegiate publication in the united states . the other magazine , the jug ##gler , is released twice a year and focuses on student literature and artwork . the dome yearbook is published annually . the newspapers have varying publication interests , with the observer published daily and mainly reporting university and other news , and staffed by students from both notre dame and saint mary ' s college . unlike scholastic and the dome , the observer is an independent publication and does not have a faculty advisor or any editorial oversight from the university . in 1987 , when some students believed that the observer began to show a conservative bias , a liberal newspaper , common sense was published . likewise , in 2003 , when other students believed that the paper showed a liberal bias , the conservative paper irish rover went into production . neither paper is published as often as the observer ; however , all three are distributed to all students . finally , in spring 2008 an undergraduate journal for political science research , beyond politics , made its debut . [SEP]\n",
            "I0420 19:06:20.708816 140407150372736 squad_lib.py:361] token_to_orig_map: 17:0 18:1 19:2 20:3 21:4 22:4 23:5 24:6 25:6 26:6 27:7 28:8 29:9 30:10 31:11 32:12 33:13 34:14 35:14 36:15 37:16 38:17 39:17 40:17 41:18 42:19 43:20 44:21 45:21 46:22 47:23 48:24 49:25 50:26 51:27 52:27 53:28 54:29 55:30 56:31 57:32 58:32 59:33 60:34 61:35 62:36 63:36 64:36 65:37 66:38 67:39 68:40 69:40 70:41 71:42 72:43 73:44 74:45 75:46 76:47 77:48 78:49 79:50 80:51 81:52 82:53 83:54 84:55 85:56 86:57 87:58 88:59 89:60 90:60 91:61 92:62 93:63 94:63 95:64 96:65 97:65 98:65 99:66 100:67 101:68 102:69 103:70 104:71 105:72 106:73 107:74 108:75 109:76 110:77 111:77 112:78 113:79 114:80 115:81 116:82 117:83 118:83 119:84 120:85 121:86 122:87 123:88 124:89 125:89 126:90 127:91 128:92 129:93 130:94 131:95 132:96 133:97 134:98 135:99 136:100 137:101 138:101 139:102 140:103 141:104 142:105 143:106 144:107 145:108 146:109 147:110 148:111 149:112 150:112 151:112 152:113 153:113 154:114 155:115 156:116 157:117 158:118 159:118 160:119 161:120 162:121 163:122 164:123 165:124 166:125 167:126 168:127 169:128 170:129 171:130 172:131 173:132 174:133 175:134 176:135 177:136 178:137 179:138 180:138 181:139 182:140 183:140 184:141 185:142 186:143 187:144 188:145 189:146 190:147 191:148 192:149 193:150 194:151 195:152 196:153 197:153 198:154 199:155 200:156 201:156 202:157 203:158 204:159 205:160 206:160 207:161 208:161 209:162 210:163 211:163 212:164 213:165 214:166 215:167 216:168 217:169 218:170 219:171 220:172 221:173 222:174 223:174 224:175 225:176 226:177 227:178 228:179 229:180 230:181 231:182 232:182 233:183 234:184 235:185 236:186 237:187 238:188 239:189 240:190 241:191 242:191 243:192 244:192 245:193 246:194 247:195 248:196 249:197 250:198 251:199 252:199 253:200 254:200 255:201 256:202 257:203 258:204 259:205 260:206 261:207 262:208 263:209 264:210 265:210 266:211 267:212 268:212 269:213 270:214 271:215 272:215\n",
            "I0420 19:06:20.708952 140407150372736 squad_lib.py:366] token_is_max_context: 17:True 18:True 19:True 20:True 21:True 22:True 23:True 24:True 25:True 26:True 27:True 28:True 29:True 30:True 31:True 32:True 33:True 34:True 35:True 36:True 37:True 38:True 39:True 40:True 41:True 42:True 43:True 44:True 45:True 46:True 47:True 48:True 49:True 50:True 51:True 52:True 53:True 54:True 55:True 56:True 57:True 58:True 59:True 60:True 61:True 62:True 63:True 64:True 65:True 66:True 67:True 68:True 69:True 70:True 71:True 72:True 73:True 74:True 75:True 76:True 77:True 78:True 79:True 80:True 81:True 82:True 83:True 84:True 85:True 86:True 87:True 88:True 89:True 90:True 91:True 92:True 93:True 94:True 95:True 96:True 97:True 98:True 99:True 100:True 101:True 102:True 103:True 104:True 105:True 106:True 107:True 108:True 109:True 110:True 111:True 112:True 113:True 114:True 115:True 116:True 117:True 118:True 119:True 120:True 121:True 122:True 123:True 124:True 125:True 126:True 127:True 128:True 129:True 130:True 131:True 132:True 133:True 134:True 135:True 136:True 137:True 138:True 139:True 140:True 141:True 142:True 143:True 144:True 145:True 146:True 147:True 148:True 149:True 150:True 151:True 152:True 153:True 154:True 155:True 156:True 157:True 158:True 159:True 160:True 161:True 162:True 163:True 164:True 165:True 166:True 167:True 168:True 169:True 170:True 171:True 172:True 173:True 174:True 175:True 176:True 177:True 178:True 179:True 180:True 181:True 182:True 183:True 184:True 185:True 186:True 187:True 188:True 189:True 190:True 191:True 192:True 193:True 194:True 195:True 196:True 197:True 198:True 199:True 200:True 201:True 202:True 203:True 204:True 205:True 206:True 207:True 208:True 209:True 210:True 211:True 212:True 213:True 214:True 215:True 216:True 217:True 218:True 219:True 220:True 221:True 222:True 223:True 224:True 225:True 226:True 227:True 228:True 229:True 230:True 231:True 232:True 233:True 234:True 235:True 236:True 237:True 238:True 239:True 240:True 241:True 242:True 243:True 244:True 245:True 246:True 247:True 248:True 249:True 250:True 251:True 252:True 253:True 254:True 255:True 256:True 257:True 258:True 259:True 260:True 261:True 262:True 263:True 264:True 265:True 266:True 267:True 268:True 269:True 270:True 271:True 272:True\n",
            "I0420 19:06:20.709125 140407150372736 squad_lib.py:368] input_ids: 101 1999 2054 2095 2106 1996 3076 3259 2691 3168 4088 4772 2012 10289 8214 1029 102 2004 2012 2087 2060 5534 1010 10289 8214 1005 1055 2493 2448 1037 2193 1997 2739 2865 11730 1012 1996 3157 3076 1011 2448 11730 2421 2093 6399 1010 2119 1037 2557 1998 2547 2276 1010 1998 2195 7298 1998 9263 1012 5625 2004 1037 2028 1011 3931 3485 1999 2244 7326 1010 1996 24105 2932 2003 3843 3807 7058 1998 4447 2000 2022 1996 4587 7142 9234 4772 1999 1996 2142 2163 1012 1996 2060 2932 1010 1996 26536 17420 1010 2003 2207 3807 1037 2095 1998 7679 2006 3076 3906 1998 8266 1012 1996 8514 24803 2003 2405 6604 1012 1996 6399 2031 9671 4772 5426 1010 2007 1996 9718 2405 3679 1998 3701 7316 2118 1998 2060 2739 1010 1998 21121 2011 2493 2013 2119 10289 8214 1998 3002 2984 1005 1055 2267 1012 4406 24105 1998 1996 8514 1010 1996 9718 2003 2019 2981 4772 1998 2515 2025 2031 1037 4513 8619 2030 2151 8368 15709 2013 1996 2118 1012 1999 3055 1010 2043 2070 2493 3373 2008 1996 9718 2211 2000 2265 1037 4603 13827 1010 1037 4314 3780 1010 2691 3168 2001 2405 1012 10655 1010 1999 2494 1010 2043 2060 2493 3373 2008 1996 3259 3662 1037 4314 13827 1010 1996 4603 3259 3493 13631 2253 2046 2537 1012 4445 3259 2003 2405 2004 2411 2004 1996 9718 1025 2174 1010 2035 2093 2024 5500 2000 2035 2493 1012 2633 1010 1999 3500 2263 2019 8324 3485 2005 2576 2671 2470 1010 3458 4331 1010 2081 2049 2834 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0420 19:06:20.709284 140407150372736 squad_lib.py:369] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0420 19:06:20.709436 140407150372736 squad_lib.py:370] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0420 19:06:20.709491 140407150372736 squad_lib.py:375] start_position: 182\n",
            "I0420 19:06:20.709578 140407150372736 squad_lib.py:376] end_position: 182\n",
            "I0420 19:06:20.709656 140407150372736 squad_lib.py:377] answer: 1987\n",
            "I0420 19:06:20.714391 140407150372736 squad_lib.py:353] *** Example ***\n",
            "I0420 19:06:20.714489 140407150372736 squad_lib.py:354] unique_id: 1000000010\n",
            "I0420 19:06:20.714559 140407150372736 squad_lib.py:355] example_index: 10\n",
            "I0420 19:06:20.714629 140407150372736 squad_lib.py:356] doc_span_index: 0\n",
            "I0420 19:06:20.714747 140407150372736 squad_lib.py:358] tokens: [CLS] where is the headquarters of the congregation of the holy cross ? [SEP] the university is the major seat of the congregation of holy cross ( albeit not its official headquarters , which are in rome ) . its main seminary , more ##au seminary , is located on the campus across st . joseph lake from the main building . old college , the oldest building on campus and located near the shore of st . mary lake , houses undergraduate seminar ##ians . retired priests and brothers reside in fatima house ( a former retreat center ) , holy cross house , as well as col ##umb ##a hall near the gr ##otto . the university through the more ##au seminary has ties to theologian frederick bu ##ech ##ner . while not catholic , bu ##ech ##ner has praised writers from notre dame and more ##au seminary created a bu ##ech ##ner prize for preaching . [SEP]\n",
            "I0420 19:06:20.714863 140407150372736 squad_lib.py:361] token_to_orig_map: 14:0 15:1 16:2 17:3 18:4 19:5 20:6 21:7 22:8 23:9 24:10 25:11 26:12 27:12 28:13 29:14 30:15 31:16 32:16 33:17 34:18 35:19 36:20 37:20 38:20 39:21 40:22 41:23 42:23 43:24 44:24 45:25 46:25 47:26 48:27 49:28 50:29 51:30 52:31 53:32 54:32 55:33 56:34 57:35 58:36 59:37 60:38 61:38 62:39 63:40 64:40 65:41 66:42 67:43 68:44 69:45 70:46 71:47 72:48 73:49 74:50 75:51 76:52 77:52 78:53 79:54 80:54 81:55 82:56 83:57 84:57 85:57 86:58 87:59 88:60 89:61 90:62 91:63 92:64 93:65 94:66 95:66 96:67 97:68 98:69 99:69 100:69 101:70 102:71 103:72 104:72 105:73 106:74 107:75 108:76 109:76 110:76 111:77 112:78 113:79 114:80 115:80 116:80 117:81 118:82 119:83 120:84 121:85 122:85 123:86 124:87 125:88 126:89 127:90 128:91 129:92 130:92 131:92 132:92 133:93 134:94 135:95 136:95 137:96 138:96 139:96 140:97 141:98 142:99 143:100 144:101 145:102 146:103 147:104 148:104 149:105 150:106 151:107 152:108 153:108 154:108 155:109 156:110 157:111 158:111\n",
            "I0420 19:06:20.803330 140407150372736 squad_lib.py:366] token_is_max_context: 14:True 15:True 16:True 17:True 18:True 19:True 20:True 21:True 22:True 23:True 24:True 25:True 26:True 27:True 28:True 29:True 30:True 31:True 32:True 33:True 34:True 35:True 36:True 37:True 38:True 39:True 40:True 41:True 42:True 43:True 44:True 45:True 46:True 47:True 48:True 49:True 50:True 51:True 52:True 53:True 54:True 55:True 56:True 57:True 58:True 59:True 60:True 61:True 62:True 63:True 64:True 65:True 66:True 67:True 68:True 69:True 70:True 71:True 72:True 73:True 74:True 75:True 76:True 77:True 78:True 79:True 80:True 81:True 82:True 83:True 84:True 85:True 86:True 87:True 88:True 89:True 90:True 91:True 92:True 93:True 94:True 95:True 96:True 97:True 98:True 99:True 100:True 101:True 102:True 103:True 104:True 105:True 106:True 107:True 108:True 109:True 110:True 111:True 112:True 113:True 114:True 115:True 116:True 117:True 118:True 119:True 120:True 121:True 122:True 123:True 124:True 125:True 126:True 127:True 128:True 129:True 130:True 131:True 132:True 133:True 134:True 135:True 136:True 137:True 138:True 139:True 140:True 141:True 142:True 143:True 144:True 145:True 146:True 147:True 148:True 149:True 150:True 151:True 152:True 153:True 154:True 155:True 156:True 157:True 158:True\n",
            "I0420 19:06:20.803739 140407150372736 squad_lib.py:368] input_ids: 101 2073 2003 1996 4075 1997 1996 7769 1997 1996 4151 2892 1029 102 1996 2118 2003 1996 2350 2835 1997 1996 7769 1997 4151 2892 1006 12167 2025 2049 2880 4075 1010 2029 2024 1999 4199 1007 1012 2049 2364 8705 1010 2062 4887 8705 1010 2003 2284 2006 1996 3721 2408 2358 1012 3312 2697 2013 1996 2364 2311 1012 2214 2267 1010 1996 4587 2311 2006 3721 1998 2284 2379 1996 5370 1997 2358 1012 2984 2697 1010 3506 8324 18014 7066 1012 3394 8656 1998 3428 13960 1999 27596 2160 1006 1037 2280 7822 2415 1007 1010 4151 2892 2160 1010 2004 2092 2004 8902 25438 2050 2534 2379 1996 24665 23052 1012 1996 2118 2083 1996 2062 4887 8705 2038 7208 2000 17200 5406 20934 15937 3678 1012 2096 2025 3234 1010 20934 15937 3678 2038 5868 4898 2013 10289 8214 1998 2062 4887 8705 2580 1037 20934 15937 3678 3396 2005 17979 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0420 19:06:20.804032 140407150372736 squad_lib.py:369] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0420 19:06:20.804280 140407150372736 squad_lib.py:370] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0420 19:06:20.804369 140407150372736 squad_lib.py:375] start_position: 36\n",
            "I0420 19:06:20.804471 140407150372736 squad_lib.py:376] end_position: 36\n",
            "I0420 19:06:20.804544 140407150372736 squad_lib.py:377] answer: rome\n",
            "I0420 19:06:20.810173 140407150372736 squad_lib.py:353] *** Example ***\n",
            "I0420 19:06:20.810291 140407150372736 squad_lib.py:354] unique_id: 1000000011\n",
            "I0420 19:06:20.810367 140407150372736 squad_lib.py:355] example_index: 11\n",
            "I0420 19:06:20.810428 140407150372736 squad_lib.py:356] doc_span_index: 0\n",
            "I0420 19:06:20.810557 140407150372736 squad_lib.py:358] tokens: [CLS] what is the primary seminary of the congregation of the holy cross ? [SEP] the university is the major seat of the congregation of holy cross ( albeit not its official headquarters , which are in rome ) . its main seminary , more ##au seminary , is located on the campus across st . joseph lake from the main building . old college , the oldest building on campus and located near the shore of st . mary lake , houses undergraduate seminar ##ians . retired priests and brothers reside in fatima house ( a former retreat center ) , holy cross house , as well as col ##umb ##a hall near the gr ##otto . the university through the more ##au seminary has ties to theologian frederick bu ##ech ##ner . while not catholic , bu ##ech ##ner has praised writers from notre dame and more ##au seminary created a bu ##ech ##ner prize for preaching . [SEP]\n",
            "I0420 19:06:20.810699 140407150372736 squad_lib.py:361] token_to_orig_map: 15:0 16:1 17:2 18:3 19:4 20:5 21:6 22:7 23:8 24:9 25:10 26:11 27:12 28:12 29:13 30:14 31:15 32:16 33:16 34:17 35:18 36:19 37:20 38:20 39:20 40:21 41:22 42:23 43:23 44:24 45:24 46:25 47:25 48:26 49:27 50:28 51:29 52:30 53:31 54:32 55:32 56:33 57:34 58:35 59:36 60:37 61:38 62:38 63:39 64:40 65:40 66:41 67:42 68:43 69:44 70:45 71:46 72:47 73:48 74:49 75:50 76:51 77:52 78:52 79:53 80:54 81:54 82:55 83:56 84:57 85:57 86:57 87:58 88:59 89:60 90:61 91:62 92:63 93:64 94:65 95:66 96:66 97:67 98:68 99:69 100:69 101:69 102:70 103:71 104:72 105:72 106:73 107:74 108:75 109:76 110:76 111:76 112:77 113:78 114:79 115:80 116:80 117:80 118:81 119:82 120:83 121:84 122:85 123:85 124:86 125:87 126:88 127:89 128:90 129:91 130:92 131:92 132:92 133:92 134:93 135:94 136:95 137:95 138:96 139:96 140:96 141:97 142:98 143:99 144:100 145:101 146:102 147:103 148:104 149:104 150:105 151:106 152:107 153:108 154:108 155:108 156:109 157:110 158:111 159:111\n",
            "I0420 19:06:20.810822 140407150372736 squad_lib.py:366] token_is_max_context: 15:True 16:True 17:True 18:True 19:True 20:True 21:True 22:True 23:True 24:True 25:True 26:True 27:True 28:True 29:True 30:True 31:True 32:True 33:True 34:True 35:True 36:True 37:True 38:True 39:True 40:True 41:True 42:True 43:True 44:True 45:True 46:True 47:True 48:True 49:True 50:True 51:True 52:True 53:True 54:True 55:True 56:True 57:True 58:True 59:True 60:True 61:True 62:True 63:True 64:True 65:True 66:True 67:True 68:True 69:True 70:True 71:True 72:True 73:True 74:True 75:True 76:True 77:True 78:True 79:True 80:True 81:True 82:True 83:True 84:True 85:True 86:True 87:True 88:True 89:True 90:True 91:True 92:True 93:True 94:True 95:True 96:True 97:True 98:True 99:True 100:True 101:True 102:True 103:True 104:True 105:True 106:True 107:True 108:True 109:True 110:True 111:True 112:True 113:True 114:True 115:True 116:True 117:True 118:True 119:True 120:True 121:True 122:True 123:True 124:True 125:True 126:True 127:True 128:True 129:True 130:True 131:True 132:True 133:True 134:True 135:True 136:True 137:True 138:True 139:True 140:True 141:True 142:True 143:True 144:True 145:True 146:True 147:True 148:True 149:True 150:True 151:True 152:True 153:True 154:True 155:True 156:True 157:True 158:True 159:True\n",
            "I0420 19:06:20.811048 140407150372736 squad_lib.py:368] input_ids: 101 2054 2003 1996 3078 8705 1997 1996 7769 1997 1996 4151 2892 1029 102 1996 2118 2003 1996 2350 2835 1997 1996 7769 1997 4151 2892 1006 12167 2025 2049 2880 4075 1010 2029 2024 1999 4199 1007 1012 2049 2364 8705 1010 2062 4887 8705 1010 2003 2284 2006 1996 3721 2408 2358 1012 3312 2697 2013 1996 2364 2311 1012 2214 2267 1010 1996 4587 2311 2006 3721 1998 2284 2379 1996 5370 1997 2358 1012 2984 2697 1010 3506 8324 18014 7066 1012 3394 8656 1998 3428 13960 1999 27596 2160 1006 1037 2280 7822 2415 1007 1010 4151 2892 2160 1010 2004 2092 2004 8902 25438 2050 2534 2379 1996 24665 23052 1012 1996 2118 2083 1996 2062 4887 8705 2038 7208 2000 17200 5406 20934 15937 3678 1012 2096 2025 3234 1010 20934 15937 3678 2038 5868 4898 2013 10289 8214 1998 2062 4887 8705 2580 1037 20934 15937 3678 3396 2005 17979 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0420 19:06:20.811269 140407150372736 squad_lib.py:369] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0420 19:06:20.811479 140407150372736 squad_lib.py:370] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0420 19:06:20.811547 140407150372736 squad_lib.py:375] start_position: 44\n",
            "I0420 19:06:20.811618 140407150372736 squad_lib.py:376] end_position: 46\n",
            "I0420 19:06:20.811676 140407150372736 squad_lib.py:377] answer: more ##au seminary\n",
            "I0420 19:06:20.817304 140407150372736 squad_lib.py:353] *** Example ***\n",
            "I0420 19:06:20.817411 140407150372736 squad_lib.py:354] unique_id: 1000000012\n",
            "I0420 19:06:20.817479 140407150372736 squad_lib.py:355] example_index: 12\n",
            "I0420 19:06:20.817547 140407150372736 squad_lib.py:356] doc_span_index: 0\n",
            "I0420 19:06:20.817696 140407150372736 squad_lib.py:358] tokens: [CLS] what is the oldest structure at notre dame ? [SEP] the university is the major seat of the congregation of holy cross ( albeit not its official headquarters , which are in rome ) . its main seminary , more ##au seminary , is located on the campus across st . joseph lake from the main building . old college , the oldest building on campus and located near the shore of st . mary lake , houses undergraduate seminar ##ians . retired priests and brothers reside in fatima house ( a former retreat center ) , holy cross house , as well as col ##umb ##a hall near the gr ##otto . the university through the more ##au seminary has ties to theologian frederick bu ##ech ##ner . while not catholic , bu ##ech ##ner has praised writers from notre dame and more ##au seminary created a bu ##ech ##ner prize for preaching . [SEP]\n",
            "I0420 19:06:20.817833 140407150372736 squad_lib.py:361] token_to_orig_map: 11:0 12:1 13:2 14:3 15:4 16:5 17:6 18:7 19:8 20:9 21:10 22:11 23:12 24:12 25:13 26:14 27:15 28:16 29:16 30:17 31:18 32:19 33:20 34:20 35:20 36:21 37:22 38:23 39:23 40:24 41:24 42:25 43:25 44:26 45:27 46:28 47:29 48:30 49:31 50:32 51:32 52:33 53:34 54:35 55:36 56:37 57:38 58:38 59:39 60:40 61:40 62:41 63:42 64:43 65:44 66:45 67:46 68:47 69:48 70:49 71:50 72:51 73:52 74:52 75:53 76:54 77:54 78:55 79:56 80:57 81:57 82:57 83:58 84:59 85:60 86:61 87:62 88:63 89:64 90:65 91:66 92:66 93:67 94:68 95:69 96:69 97:69 98:70 99:71 100:72 101:72 102:73 103:74 104:75 105:76 106:76 107:76 108:77 109:78 110:79 111:80 112:80 113:80 114:81 115:82 116:83 117:84 118:85 119:85 120:86 121:87 122:88 123:89 124:90 125:91 126:92 127:92 128:92 129:92 130:93 131:94 132:95 133:95 134:96 135:96 136:96 137:97 138:98 139:99 140:100 141:101 142:102 143:103 144:104 145:104 146:105 147:106 148:107 149:108 150:108 151:108 152:109 153:110 154:111 155:111\n",
            "I0420 19:06:20.817946 140407150372736 squad_lib.py:366] token_is_max_context: 11:True 12:True 13:True 14:True 15:True 16:True 17:True 18:True 19:True 20:True 21:True 22:True 23:True 24:True 25:True 26:True 27:True 28:True 29:True 30:True 31:True 32:True 33:True 34:True 35:True 36:True 37:True 38:True 39:True 40:True 41:True 42:True 43:True 44:True 45:True 46:True 47:True 48:True 49:True 50:True 51:True 52:True 53:True 54:True 55:True 56:True 57:True 58:True 59:True 60:True 61:True 62:True 63:True 64:True 65:True 66:True 67:True 68:True 69:True 70:True 71:True 72:True 73:True 74:True 75:True 76:True 77:True 78:True 79:True 80:True 81:True 82:True 83:True 84:True 85:True 86:True 87:True 88:True 89:True 90:True 91:True 92:True 93:True 94:True 95:True 96:True 97:True 98:True 99:True 100:True 101:True 102:True 103:True 104:True 105:True 106:True 107:True 108:True 109:True 110:True 111:True 112:True 113:True 114:True 115:True 116:True 117:True 118:True 119:True 120:True 121:True 122:True 123:True 124:True 125:True 126:True 127:True 128:True 129:True 130:True 131:True 132:True 133:True 134:True 135:True 136:True 137:True 138:True 139:True 140:True 141:True 142:True 143:True 144:True 145:True 146:True 147:True 148:True 149:True 150:True 151:True 152:True 153:True 154:True 155:True\n",
            "I0420 19:06:20.818119 140407150372736 squad_lib.py:368] input_ids: 101 2054 2003 1996 4587 3252 2012 10289 8214 1029 102 1996 2118 2003 1996 2350 2835 1997 1996 7769 1997 4151 2892 1006 12167 2025 2049 2880 4075 1010 2029 2024 1999 4199 1007 1012 2049 2364 8705 1010 2062 4887 8705 1010 2003 2284 2006 1996 3721 2408 2358 1012 3312 2697 2013 1996 2364 2311 1012 2214 2267 1010 1996 4587 2311 2006 3721 1998 2284 2379 1996 5370 1997 2358 1012 2984 2697 1010 3506 8324 18014 7066 1012 3394 8656 1998 3428 13960 1999 27596 2160 1006 1037 2280 7822 2415 1007 1010 4151 2892 2160 1010 2004 2092 2004 8902 25438 2050 2534 2379 1996 24665 23052 1012 1996 2118 2083 1996 2062 4887 8705 2038 7208 2000 17200 5406 20934 15937 3678 1012 2096 2025 3234 1010 20934 15937 3678 2038 5868 4898 2013 10289 8214 1998 2062 4887 8705 2580 1037 20934 15937 3678 3396 2005 17979 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0420 19:06:20.903975 140407150372736 squad_lib.py:369] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0420 19:06:20.904623 140407150372736 squad_lib.py:370] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0420 19:06:20.904846 140407150372736 squad_lib.py:375] start_position: 59\n",
            "I0420 19:06:20.905040 140407150372736 squad_lib.py:376] end_position: 60\n",
            "I0420 19:06:20.905229 140407150372736 squad_lib.py:377] answer: old college\n",
            "I0420 19:06:20.909229 140407150372736 squad_lib.py:353] *** Example ***\n",
            "I0420 19:06:20.909329 140407150372736 squad_lib.py:354] unique_id: 1000000013\n",
            "I0420 19:06:20.909418 140407150372736 squad_lib.py:355] example_index: 13\n",
            "I0420 19:06:20.909474 140407150372736 squad_lib.py:356] doc_span_index: 0\n",
            "I0420 19:06:20.909615 140407150372736 squad_lib.py:358] tokens: [CLS] what individuals live at fatima house at notre dame ? [SEP] the university is the major seat of the congregation of holy cross ( albeit not its official headquarters , which are in rome ) . its main seminary , more ##au seminary , is located on the campus across st . joseph lake from the main building . old college , the oldest building on campus and located near the shore of st . mary lake , houses undergraduate seminar ##ians . retired priests and brothers reside in fatima house ( a former retreat center ) , holy cross house , as well as col ##umb ##a hall near the gr ##otto . the university through the more ##au seminary has ties to theologian frederick bu ##ech ##ner . while not catholic , bu ##ech ##ner has praised writers from notre dame and more ##au seminary created a bu ##ech ##ner prize for preaching . [SEP]\n",
            "I0420 19:06:20.909746 140407150372736 squad_lib.py:361] token_to_orig_map: 12:0 13:1 14:2 15:3 16:4 17:5 18:6 19:7 20:8 21:9 22:10 23:11 24:12 25:12 26:13 27:14 28:15 29:16 30:16 31:17 32:18 33:19 34:20 35:20 36:20 37:21 38:22 39:23 40:23 41:24 42:24 43:25 44:25 45:26 46:27 47:28 48:29 49:30 50:31 51:32 52:32 53:33 54:34 55:35 56:36 57:37 58:38 59:38 60:39 61:40 62:40 63:41 64:42 65:43 66:44 67:45 68:46 69:47 70:48 71:49 72:50 73:51 74:52 75:52 76:53 77:54 78:54 79:55 80:56 81:57 82:57 83:57 84:58 85:59 86:60 87:61 88:62 89:63 90:64 91:65 92:66 93:66 94:67 95:68 96:69 97:69 98:69 99:70 100:71 101:72 102:72 103:73 104:74 105:75 106:76 107:76 108:76 109:77 110:78 111:79 112:80 113:80 114:80 115:81 116:82 117:83 118:84 119:85 120:85 121:86 122:87 123:88 124:89 125:90 126:91 127:92 128:92 129:92 130:92 131:93 132:94 133:95 134:95 135:96 136:96 137:96 138:97 139:98 140:99 141:100 142:101 143:102 144:103 145:104 146:104 147:105 148:106 149:107 150:108 151:108 152:108 153:109 154:110 155:111 156:111\n",
            "I0420 19:06:20.909863 140407150372736 squad_lib.py:366] token_is_max_context: 12:True 13:True 14:True 15:True 16:True 17:True 18:True 19:True 20:True 21:True 22:True 23:True 24:True 25:True 26:True 27:True 28:True 29:True 30:True 31:True 32:True 33:True 34:True 35:True 36:True 37:True 38:True 39:True 40:True 41:True 42:True 43:True 44:True 45:True 46:True 47:True 48:True 49:True 50:True 51:True 52:True 53:True 54:True 55:True 56:True 57:True 58:True 59:True 60:True 61:True 62:True 63:True 64:True 65:True 66:True 67:True 68:True 69:True 70:True 71:True 72:True 73:True 74:True 75:True 76:True 77:True 78:True 79:True 80:True 81:True 82:True 83:True 84:True 85:True 86:True 87:True 88:True 89:True 90:True 91:True 92:True 93:True 94:True 95:True 96:True 97:True 98:True 99:True 100:True 101:True 102:True 103:True 104:True 105:True 106:True 107:True 108:True 109:True 110:True 111:True 112:True 113:True 114:True 115:True 116:True 117:True 118:True 119:True 120:True 121:True 122:True 123:True 124:True 125:True 126:True 127:True 128:True 129:True 130:True 131:True 132:True 133:True 134:True 135:True 136:True 137:True 138:True 139:True 140:True 141:True 142:True 143:True 144:True 145:True 146:True 147:True 148:True 149:True 150:True 151:True 152:True 153:True 154:True 155:True 156:True\n",
            "I0420 19:06:20.910031 140407150372736 squad_lib.py:368] input_ids: 101 2054 3633 2444 2012 27596 2160 2012 10289 8214 1029 102 1996 2118 2003 1996 2350 2835 1997 1996 7769 1997 4151 2892 1006 12167 2025 2049 2880 4075 1010 2029 2024 1999 4199 1007 1012 2049 2364 8705 1010 2062 4887 8705 1010 2003 2284 2006 1996 3721 2408 2358 1012 3312 2697 2013 1996 2364 2311 1012 2214 2267 1010 1996 4587 2311 2006 3721 1998 2284 2379 1996 5370 1997 2358 1012 2984 2697 1010 3506 8324 18014 7066 1012 3394 8656 1998 3428 13960 1999 27596 2160 1006 1037 2280 7822 2415 1007 1010 4151 2892 2160 1010 2004 2092 2004 8902 25438 2050 2534 2379 1996 24665 23052 1012 1996 2118 2083 1996 2062 4887 8705 2038 7208 2000 17200 5406 20934 15937 3678 1012 2096 2025 3234 1010 20934 15937 3678 2038 5868 4898 2013 10289 8214 1998 2062 4887 8705 2580 1037 20934 15937 3678 3396 2005 17979 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0420 19:06:20.910192 140407150372736 squad_lib.py:369] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0420 19:06:20.910350 140407150372736 squad_lib.py:370] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0420 19:06:20.910418 140407150372736 squad_lib.py:375] start_position: 84\n",
            "I0420 19:06:20.910489 140407150372736 squad_lib.py:376] end_position: 87\n",
            "I0420 19:06:20.910545 140407150372736 squad_lib.py:377] answer: retired priests and brothers\n",
            "I0420 19:06:20.913652 140407150372736 squad_lib.py:353] *** Example ***\n",
            "I0420 19:06:20.913739 140407150372736 squad_lib.py:354] unique_id: 1000000014\n",
            "I0420 19:06:20.913809 140407150372736 squad_lib.py:355] example_index: 14\n",
            "I0420 19:06:20.913867 140407150372736 squad_lib.py:356] doc_span_index: 0\n",
            "I0420 19:06:20.913984 140407150372736 squad_lib.py:358] tokens: [CLS] which prize did frederick bu ##ech ##ner create ? [SEP] the university is the major seat of the congregation of holy cross ( albeit not its official headquarters , which are in rome ) . its main seminary , more ##au seminary , is located on the campus across st . joseph lake from the main building . old college , the oldest building on campus and located near the shore of st . mary lake , houses undergraduate seminar ##ians . retired priests and brothers reside in fatima house ( a former retreat center ) , holy cross house , as well as col ##umb ##a hall near the gr ##otto . the university through the more ##au seminary has ties to theologian frederick bu ##ech ##ner . while not catholic , bu ##ech ##ner has praised writers from notre dame and more ##au seminary created a bu ##ech ##ner prize for preaching . [SEP]\n",
            "I0420 19:06:20.914101 140407150372736 squad_lib.py:361] token_to_orig_map: 11:0 12:1 13:2 14:3 15:4 16:5 17:6 18:7 19:8 20:9 21:10 22:11 23:12 24:12 25:13 26:14 27:15 28:16 29:16 30:17 31:18 32:19 33:20 34:20 35:20 36:21 37:22 38:23 39:23 40:24 41:24 42:25 43:25 44:26 45:27 46:28 47:29 48:30 49:31 50:32 51:32 52:33 53:34 54:35 55:36 56:37 57:38 58:38 59:39 60:40 61:40 62:41 63:42 64:43 65:44 66:45 67:46 68:47 69:48 70:49 71:50 72:51 73:52 74:52 75:53 76:54 77:54 78:55 79:56 80:57 81:57 82:57 83:58 84:59 85:60 86:61 87:62 88:63 89:64 90:65 91:66 92:66 93:67 94:68 95:69 96:69 97:69 98:70 99:71 100:72 101:72 102:73 103:74 104:75 105:76 106:76 107:76 108:77 109:78 110:79 111:80 112:80 113:80 114:81 115:82 116:83 117:84 118:85 119:85 120:86 121:87 122:88 123:89 124:90 125:91 126:92 127:92 128:92 129:92 130:93 131:94 132:95 133:95 134:96 135:96 136:96 137:97 138:98 139:99 140:100 141:101 142:102 143:103 144:104 145:104 146:105 147:106 148:107 149:108 150:108 151:108 152:109 153:110 154:111 155:111\n",
            "I0420 19:06:20.914213 140407150372736 squad_lib.py:366] token_is_max_context: 11:True 12:True 13:True 14:True 15:True 16:True 17:True 18:True 19:True 20:True 21:True 22:True 23:True 24:True 25:True 26:True 27:True 28:True 29:True 30:True 31:True 32:True 33:True 34:True 35:True 36:True 37:True 38:True 39:True 40:True 41:True 42:True 43:True 44:True 45:True 46:True 47:True 48:True 49:True 50:True 51:True 52:True 53:True 54:True 55:True 56:True 57:True 58:True 59:True 60:True 61:True 62:True 63:True 64:True 65:True 66:True 67:True 68:True 69:True 70:True 71:True 72:True 73:True 74:True 75:True 76:True 77:True 78:True 79:True 80:True 81:True 82:True 83:True 84:True 85:True 86:True 87:True 88:True 89:True 90:True 91:True 92:True 93:True 94:True 95:True 96:True 97:True 98:True 99:True 100:True 101:True 102:True 103:True 104:True 105:True 106:True 107:True 108:True 109:True 110:True 111:True 112:True 113:True 114:True 115:True 116:True 117:True 118:True 119:True 120:True 121:True 122:True 123:True 124:True 125:True 126:True 127:True 128:True 129:True 130:True 131:True 132:True 133:True 134:True 135:True 136:True 137:True 138:True 139:True 140:True 141:True 142:True 143:True 144:True 145:True 146:True 147:True 148:True 149:True 150:True 151:True 152:True 153:True 154:True 155:True\n",
            "I0420 19:06:20.914378 140407150372736 squad_lib.py:368] input_ids: 101 2029 3396 2106 5406 20934 15937 3678 3443 1029 102 1996 2118 2003 1996 2350 2835 1997 1996 7769 1997 4151 2892 1006 12167 2025 2049 2880 4075 1010 2029 2024 1999 4199 1007 1012 2049 2364 8705 1010 2062 4887 8705 1010 2003 2284 2006 1996 3721 2408 2358 1012 3312 2697 2013 1996 2364 2311 1012 2214 2267 1010 1996 4587 2311 2006 3721 1998 2284 2379 1996 5370 1997 2358 1012 2984 2697 1010 3506 8324 18014 7066 1012 3394 8656 1998 3428 13960 1999 27596 2160 1006 1037 2280 7822 2415 1007 1010 4151 2892 2160 1010 2004 2092 2004 8902 25438 2050 2534 2379 1996 24665 23052 1012 1996 2118 2083 1996 2062 4887 8705 2038 7208 2000 17200 5406 20934 15937 3678 1012 2096 2025 3234 1010 20934 15937 3678 2038 5868 4898 2013 10289 8214 1998 2062 4887 8705 2580 1037 20934 15937 3678 3396 2005 17979 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0420 19:06:20.914555 140407150372736 squad_lib.py:369] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0420 19:06:20.914727 140407150372736 squad_lib.py:370] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0420 19:06:20.914784 140407150372736 squad_lib.py:375] start_position: 149\n",
            "I0420 19:06:20.914842 140407150372736 squad_lib.py:376] end_position: 154\n",
            "I0420 19:06:20.914897 140407150372736 squad_lib.py:377] answer: bu ##ech ##ner prize for preaching\n",
            "I0420 19:06:20.918289 140407150372736 squad_lib.py:353] *** Example ***\n",
            "I0420 19:06:20.918378 140407150372736 squad_lib.py:354] unique_id: 1000000015\n",
            "I0420 19:06:20.918452 140407150372736 squad_lib.py:355] example_index: 15\n",
            "I0420 19:06:20.918510 140407150372736 squad_lib.py:356] doc_span_index: 0\n",
            "I0420 19:06:20.918643 140407150372736 squad_lib.py:358] tokens: [CLS] how many bs level degrees are offered in the college of engineering at notre dame ? [SEP] the college of engineering was established in 1920 , however , early courses in civil and mechanical engineering were a part of the college of science since the 1870s . today the college , housed in the fitzpatrick , cu ##shing , and st ##ins ##on - re ##mic ##k halls of engineering , includes five departments of study – aerospace and mechanical engineering , chemical and bio ##mo ##le ##cular engineering , civil engineering and geological sciences , computer science and engineering , and electrical engineering – with eight b . s . degrees offered . additionally , the college offers five - year dual degree programs with the colleges of arts and letters and of business awarding additional b . a . and master of business administration ( mba ) degrees , respectively . [SEP]\n",
            "I0420 19:06:20.918765 140407150372736 squad_lib.py:361] token_to_orig_map: 18:0 19:1 20:2 21:3 22:4 23:5 24:6 25:7 26:7 27:8 28:8 29:9 30:10 31:11 32:12 33:13 34:14 35:15 36:16 37:17 38:18 39:19 40:20 41:21 42:22 43:23 44:24 45:25 46:26 47:26 48:27 49:28 50:29 51:29 52:30 53:31 54:32 55:33 56:33 57:34 58:34 59:34 60:35 61:36 62:36 63:36 64:36 65:36 66:36 67:36 68:37 69:38 70:39 71:39 72:40 73:41 74:42 75:43 76:44 77:45 78:46 79:47 80:48 81:49 82:49 83:50 84:51 85:52 86:52 87:52 88:52 89:53 90:53 91:54 92:55 93:56 94:57 95:58 96:58 97:59 98:60 99:61 100:62 101:62 102:63 103:64 104:65 105:66 106:67 107:68 108:69 109:69 110:69 111:69 112:70 113:71 114:71 115:72 116:72 117:73 118:74 119:75 120:76 121:76 122:76 123:77 124:78 125:79 126:80 127:81 128:82 129:83 130:84 131:85 132:86 133:87 134:88 135:89 136:90 137:91 138:92 139:92 140:92 141:92 142:93 143:94 144:95 145:96 146:97 147:98 148:98 149:98 150:99 151:99 152:100 153:100\n",
            "I0420 19:06:21.007495 140407150372736 squad_lib.py:366] token_is_max_context: 18:True 19:True 20:True 21:True 22:True 23:True 24:True 25:True 26:True 27:True 28:True 29:True 30:True 31:True 32:True 33:True 34:True 35:True 36:True 37:True 38:True 39:True 40:True 41:True 42:True 43:True 44:True 45:True 46:True 47:True 48:True 49:True 50:True 51:True 52:True 53:True 54:True 55:True 56:True 57:True 58:True 59:True 60:True 61:True 62:True 63:True 64:True 65:True 66:True 67:True 68:True 69:True 70:True 71:True 72:True 73:True 74:True 75:True 76:True 77:True 78:True 79:True 80:True 81:True 82:True 83:True 84:True 85:True 86:True 87:True 88:True 89:True 90:True 91:True 92:True 93:True 94:True 95:True 96:True 97:True 98:True 99:True 100:True 101:True 102:True 103:True 104:True 105:True 106:True 107:True 108:True 109:True 110:True 111:True 112:True 113:True 114:True 115:True 116:True 117:True 118:True 119:True 120:True 121:True 122:True 123:True 124:True 125:True 126:True 127:True 128:True 129:True 130:True 131:True 132:True 133:True 134:True 135:True 136:True 137:True 138:True 139:True 140:True 141:True 142:True 143:True 144:True 145:True 146:True 147:True 148:True 149:True 150:True 151:True 152:True 153:True\n",
            "I0420 19:06:21.008007 140407150372736 squad_lib.py:368] input_ids: 101 2129 2116 18667 2504 5445 2024 3253 1999 1996 2267 1997 3330 2012 10289 8214 1029 102 1996 2267 1997 3330 2001 2511 1999 4444 1010 2174 1010 2220 5352 1999 2942 1998 6228 3330 2020 1037 2112 1997 1996 2267 1997 2671 2144 1996 14896 1012 2651 1996 2267 1010 7431 1999 1996 26249 1010 12731 12227 1010 1998 2358 7076 2239 1011 2128 7712 2243 9873 1997 3330 1010 2950 2274 7640 1997 2817 1516 13395 1998 6228 3330 1010 5072 1998 16012 5302 2571 15431 3330 1010 2942 3330 1998 9843 4163 1010 3274 2671 1998 3330 1010 1998 5992 3330 1516 2007 2809 1038 1012 1055 1012 5445 3253 1012 5678 1010 1996 2267 4107 2274 1011 2095 7037 3014 3454 2007 1996 6667 1997 2840 1998 4144 1998 1997 2449 21467 3176 1038 1012 1037 1012 1998 3040 1997 2449 3447 1006 15038 1007 5445 1010 4414 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0420 19:06:21.010589 140407150372736 squad_lib.py:369] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0420 19:06:21.010919 140407150372736 squad_lib.py:370] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0420 19:06:21.011021 140407150372736 squad_lib.py:375] start_position: 107\n",
            "I0420 19:06:21.011099 140407150372736 squad_lib.py:376] end_position: 107\n",
            "I0420 19:06:21.011169 140407150372736 squad_lib.py:377] answer: eight\n",
            "I0420 19:06:21.018838 140407150372736 squad_lib.py:353] *** Example ***\n",
            "I0420 19:06:21.018963 140407150372736 squad_lib.py:354] unique_id: 1000000016\n",
            "I0420 19:06:21.019047 140407150372736 squad_lib.py:355] example_index: 16\n",
            "I0420 19:06:21.019114 140407150372736 squad_lib.py:356] doc_span_index: 0\n",
            "I0420 19:06:21.019251 140407150372736 squad_lib.py:358] tokens: [CLS] in what year was the college of engineering at notre dame formed ? [SEP] the college of engineering was established in 1920 , however , early courses in civil and mechanical engineering were a part of the college of science since the 1870s . today the college , housed in the fitzpatrick , cu ##shing , and st ##ins ##on - re ##mic ##k halls of engineering , includes five departments of study – aerospace and mechanical engineering , chemical and bio ##mo ##le ##cular engineering , civil engineering and geological sciences , computer science and engineering , and electrical engineering – with eight b . s . degrees offered . additionally , the college offers five - year dual degree programs with the colleges of arts and letters and of business awarding additional b . a . and master of business administration ( mba ) degrees , respectively . [SEP]\n",
            "I0420 19:06:21.019384 140407150372736 squad_lib.py:361] token_to_orig_map: 15:0 16:1 17:2 18:3 19:4 20:5 21:6 22:7 23:7 24:8 25:8 26:9 27:10 28:11 29:12 30:13 31:14 32:15 33:16 34:17 35:18 36:19 37:20 38:21 39:22 40:23 41:24 42:25 43:26 44:26 45:27 46:28 47:29 48:29 49:30 50:31 51:32 52:33 53:33 54:34 55:34 56:34 57:35 58:36 59:36 60:36 61:36 62:36 63:36 64:36 65:37 66:38 67:39 68:39 69:40 70:41 71:42 72:43 73:44 74:45 75:46 76:47 77:48 78:49 79:49 80:50 81:51 82:52 83:52 84:52 85:52 86:53 87:53 88:54 89:55 90:56 91:57 92:58 93:58 94:59 95:60 96:61 97:62 98:62 99:63 100:64 101:65 102:66 103:67 104:68 105:69 106:69 107:69 108:69 109:70 110:71 111:71 112:72 113:72 114:73 115:74 116:75 117:76 118:76 119:76 120:77 121:78 122:79 123:80 124:81 125:82 126:83 127:84 128:85 129:86 130:87 131:88 132:89 133:90 134:91 135:92 136:92 137:92 138:92 139:93 140:94 141:95 142:96 143:97 144:98 145:98 146:98 147:99 148:99 149:100 150:100\n",
            "I0420 19:06:21.019509 140407150372736 squad_lib.py:366] token_is_max_context: 15:True 16:True 17:True 18:True 19:True 20:True 21:True 22:True 23:True 24:True 25:True 26:True 27:True 28:True 29:True 30:True 31:True 32:True 33:True 34:True 35:True 36:True 37:True 38:True 39:True 40:True 41:True 42:True 43:True 44:True 45:True 46:True 47:True 48:True 49:True 50:True 51:True 52:True 53:True 54:True 55:True 56:True 57:True 58:True 59:True 60:True 61:True 62:True 63:True 64:True 65:True 66:True 67:True 68:True 69:True 70:True 71:True 72:True 73:True 74:True 75:True 76:True 77:True 78:True 79:True 80:True 81:True 82:True 83:True 84:True 85:True 86:True 87:True 88:True 89:True 90:True 91:True 92:True 93:True 94:True 95:True 96:True 97:True 98:True 99:True 100:True 101:True 102:True 103:True 104:True 105:True 106:True 107:True 108:True 109:True 110:True 111:True 112:True 113:True 114:True 115:True 116:True 117:True 118:True 119:True 120:True 121:True 122:True 123:True 124:True 125:True 126:True 127:True 128:True 129:True 130:True 131:True 132:True 133:True 134:True 135:True 136:True 137:True 138:True 139:True 140:True 141:True 142:True 143:True 144:True 145:True 146:True 147:True 148:True 149:True 150:True\n",
            "I0420 19:06:21.019718 140407150372736 squad_lib.py:368] input_ids: 101 1999 2054 2095 2001 1996 2267 1997 3330 2012 10289 8214 2719 1029 102 1996 2267 1997 3330 2001 2511 1999 4444 1010 2174 1010 2220 5352 1999 2942 1998 6228 3330 2020 1037 2112 1997 1996 2267 1997 2671 2144 1996 14896 1012 2651 1996 2267 1010 7431 1999 1996 26249 1010 12731 12227 1010 1998 2358 7076 2239 1011 2128 7712 2243 9873 1997 3330 1010 2950 2274 7640 1997 2817 1516 13395 1998 6228 3330 1010 5072 1998 16012 5302 2571 15431 3330 1010 2942 3330 1998 9843 4163 1010 3274 2671 1998 3330 1010 1998 5992 3330 1516 2007 2809 1038 1012 1055 1012 5445 3253 1012 5678 1010 1996 2267 4107 2274 1011 2095 7037 3014 3454 2007 1996 6667 1997 2840 1998 4144 1998 1997 2449 21467 3176 1038 1012 1037 1012 1998 3040 1997 2449 3447 1006 15038 1007 5445 1010 4414 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0420 19:06:21.019919 140407150372736 squad_lib.py:369] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0420 19:06:21.020104 140407150372736 squad_lib.py:370] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0420 19:06:21.107621 140407150372736 squad_lib.py:375] start_position: 22\n",
            "I0420 19:06:21.107794 140407150372736 squad_lib.py:376] end_position: 22\n",
            "I0420 19:06:21.107912 140407150372736 squad_lib.py:377] answer: 1920\n",
            "I0420 19:06:21.111590 140407150372736 squad_lib.py:353] *** Example ***\n",
            "I0420 19:06:21.111700 140407150372736 squad_lib.py:354] unique_id: 1000000017\n",
            "I0420 19:06:21.111774 140407150372736 squad_lib.py:355] example_index: 17\n",
            "I0420 19:06:21.111838 140407150372736 squad_lib.py:356] doc_span_index: 0\n",
            "I0420 19:06:21.111976 140407150372736 squad_lib.py:358] tokens: [CLS] before the creation of the college of engineering similar studies were carried out at which notre dame college ? [SEP] the college of engineering was established in 1920 , however , early courses in civil and mechanical engineering were a part of the college of science since the 1870s . today the college , housed in the fitzpatrick , cu ##shing , and st ##ins ##on - re ##mic ##k halls of engineering , includes five departments of study – aerospace and mechanical engineering , chemical and bio ##mo ##le ##cular engineering , civil engineering and geological sciences , computer science and engineering , and electrical engineering – with eight b . s . degrees offered . additionally , the college offers five - year dual degree programs with the colleges of arts and letters and of business awarding additional b . a . and master of business administration ( mba ) degrees , respectively . [SEP]\n",
            "I0420 19:06:21.112098 140407150372736 squad_lib.py:361] token_to_orig_map: 21:0 22:1 23:2 24:3 25:4 26:5 27:6 28:7 29:7 30:8 31:8 32:9 33:10 34:11 35:12 36:13 37:14 38:15 39:16 40:17 41:18 42:19 43:20 44:21 45:22 46:23 47:24 48:25 49:26 50:26 51:27 52:28 53:29 54:29 55:30 56:31 57:32 58:33 59:33 60:34 61:34 62:34 63:35 64:36 65:36 66:36 67:36 68:36 69:36 70:36 71:37 72:38 73:39 74:39 75:40 76:41 77:42 78:43 79:44 80:45 81:46 82:47 83:48 84:49 85:49 86:50 87:51 88:52 89:52 90:52 91:52 92:53 93:53 94:54 95:55 96:56 97:57 98:58 99:58 100:59 101:60 102:61 103:62 104:62 105:63 106:64 107:65 108:66 109:67 110:68 111:69 112:69 113:69 114:69 115:70 116:71 117:71 118:72 119:72 120:73 121:74 122:75 123:76 124:76 125:76 126:77 127:78 128:79 129:80 130:81 131:82 132:83 133:84 134:85 135:86 136:87 137:88 138:89 139:90 140:91 141:92 142:92 143:92 144:92 145:93 146:94 147:95 148:96 149:97 150:98 151:98 152:98 153:99 154:99 155:100 156:100\n",
            "I0420 19:06:21.112208 140407150372736 squad_lib.py:366] token_is_max_context: 21:True 22:True 23:True 24:True 25:True 26:True 27:True 28:True 29:True 30:True 31:True 32:True 33:True 34:True 35:True 36:True 37:True 38:True 39:True 40:True 41:True 42:True 43:True 44:True 45:True 46:True 47:True 48:True 49:True 50:True 51:True 52:True 53:True 54:True 55:True 56:True 57:True 58:True 59:True 60:True 61:True 62:True 63:True 64:True 65:True 66:True 67:True 68:True 69:True 70:True 71:True 72:True 73:True 74:True 75:True 76:True 77:True 78:True 79:True 80:True 81:True 82:True 83:True 84:True 85:True 86:True 87:True 88:True 89:True 90:True 91:True 92:True 93:True 94:True 95:True 96:True 97:True 98:True 99:True 100:True 101:True 102:True 103:True 104:True 105:True 106:True 107:True 108:True 109:True 110:True 111:True 112:True 113:True 114:True 115:True 116:True 117:True 118:True 119:True 120:True 121:True 122:True 123:True 124:True 125:True 126:True 127:True 128:True 129:True 130:True 131:True 132:True 133:True 134:True 135:True 136:True 137:True 138:True 139:True 140:True 141:True 142:True 143:True 144:True 145:True 146:True 147:True 148:True 149:True 150:True 151:True 152:True 153:True 154:True 155:True 156:True\n",
            "I0420 19:06:21.112375 140407150372736 squad_lib.py:368] input_ids: 101 2077 1996 4325 1997 1996 2267 1997 3330 2714 2913 2020 3344 2041 2012 2029 10289 8214 2267 1029 102 1996 2267 1997 3330 2001 2511 1999 4444 1010 2174 1010 2220 5352 1999 2942 1998 6228 3330 2020 1037 2112 1997 1996 2267 1997 2671 2144 1996 14896 1012 2651 1996 2267 1010 7431 1999 1996 26249 1010 12731 12227 1010 1998 2358 7076 2239 1011 2128 7712 2243 9873 1997 3330 1010 2950 2274 7640 1997 2817 1516 13395 1998 6228 3330 1010 5072 1998 16012 5302 2571 15431 3330 1010 2942 3330 1998 9843 4163 1010 3274 2671 1998 3330 1010 1998 5992 3330 1516 2007 2809 1038 1012 1055 1012 5445 3253 1012 5678 1010 1996 2267 4107 2274 1011 2095 7037 3014 3454 2007 1996 6667 1997 2840 1998 4144 1998 1997 2449 21467 3176 1038 1012 1037 1012 1998 3040 1997 2449 3447 1006 15038 1007 5445 1010 4414 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0420 19:06:21.112540 140407150372736 squad_lib.py:369] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0420 19:06:21.112713 140407150372736 squad_lib.py:370] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0420 19:06:21.112771 140407150372736 squad_lib.py:375] start_position: 43\n",
            "I0420 19:06:21.112829 140407150372736 squad_lib.py:376] end_position: 46\n",
            "I0420 19:06:21.112884 140407150372736 squad_lib.py:377] answer: the college of science\n",
            "I0420 19:06:21.116192 140407150372736 squad_lib.py:353] *** Example ***\n",
            "I0420 19:06:21.116288 140407150372736 squad_lib.py:354] unique_id: 1000000018\n",
            "I0420 19:06:21.116358 140407150372736 squad_lib.py:355] example_index: 18\n",
            "I0420 19:06:21.116417 140407150372736 squad_lib.py:356] doc_span_index: 0\n",
            "I0420 19:06:21.116536 140407150372736 squad_lib.py:358] tokens: [CLS] how many departments are within the st ##ins ##on - re ##mic ##k hall of engineering ? [SEP] the college of engineering was established in 1920 , however , early courses in civil and mechanical engineering were a part of the college of science since the 1870s . today the college , housed in the fitzpatrick , cu ##shing , and st ##ins ##on - re ##mic ##k halls of engineering , includes five departments of study – aerospace and mechanical engineering , chemical and bio ##mo ##le ##cular engineering , civil engineering and geological sciences , computer science and engineering , and electrical engineering – with eight b . s . degrees offered . additionally , the college offers five - year dual degree programs with the colleges of arts and letters and of business awarding additional b . a . and master of business administration ( mba ) degrees , respectively . [SEP]\n",
            "I0420 19:06:21.116693 140407150372736 squad_lib.py:361] token_to_orig_map: 19:0 20:1 21:2 22:3 23:4 24:5 25:6 26:7 27:7 28:8 29:8 30:9 31:10 32:11 33:12 34:13 35:14 36:15 37:16 38:17 39:18 40:19 41:20 42:21 43:22 44:23 45:24 46:25 47:26 48:26 49:27 50:28 51:29 52:29 53:30 54:31 55:32 56:33 57:33 58:34 59:34 60:34 61:35 62:36 63:36 64:36 65:36 66:36 67:36 68:36 69:37 70:38 71:39 72:39 73:40 74:41 75:42 76:43 77:44 78:45 79:46 80:47 81:48 82:49 83:49 84:50 85:51 86:52 87:52 88:52 89:52 90:53 91:53 92:54 93:55 94:56 95:57 96:58 97:58 98:59 99:60 100:61 101:62 102:62 103:63 104:64 105:65 106:66 107:67 108:68 109:69 110:69 111:69 112:69 113:70 114:71 115:71 116:72 117:72 118:73 119:74 120:75 121:76 122:76 123:76 124:77 125:78 126:79 127:80 128:81 129:82 130:83 131:84 132:85 133:86 134:87 135:88 136:89 137:90 138:91 139:92 140:92 141:92 142:92 143:93 144:94 145:95 146:96 147:97 148:98 149:98 150:98 151:99 152:99 153:100 154:100\n",
            "I0420 19:06:21.116805 140407150372736 squad_lib.py:366] token_is_max_context: 19:True 20:True 21:True 22:True 23:True 24:True 25:True 26:True 27:True 28:True 29:True 30:True 31:True 32:True 33:True 34:True 35:True 36:True 37:True 38:True 39:True 40:True 41:True 42:True 43:True 44:True 45:True 46:True 47:True 48:True 49:True 50:True 51:True 52:True 53:True 54:True 55:True 56:True 57:True 58:True 59:True 60:True 61:True 62:True 63:True 64:True 65:True 66:True 67:True 68:True 69:True 70:True 71:True 72:True 73:True 74:True 75:True 76:True 77:True 78:True 79:True 80:True 81:True 82:True 83:True 84:True 85:True 86:True 87:True 88:True 89:True 90:True 91:True 92:True 93:True 94:True 95:True 96:True 97:True 98:True 99:True 100:True 101:True 102:True 103:True 104:True 105:True 106:True 107:True 108:True 109:True 110:True 111:True 112:True 113:True 114:True 115:True 116:True 117:True 118:True 119:True 120:True 121:True 122:True 123:True 124:True 125:True 126:True 127:True 128:True 129:True 130:True 131:True 132:True 133:True 134:True 135:True 136:True 137:True 138:True 139:True 140:True 141:True 142:True 143:True 144:True 145:True 146:True 147:True 148:True 149:True 150:True 151:True 152:True 153:True 154:True\n",
            "I0420 19:06:21.117000 140407150372736 squad_lib.py:368] input_ids: 101 2129 2116 7640 2024 2306 1996 2358 7076 2239 1011 2128 7712 2243 2534 1997 3330 1029 102 1996 2267 1997 3330 2001 2511 1999 4444 1010 2174 1010 2220 5352 1999 2942 1998 6228 3330 2020 1037 2112 1997 1996 2267 1997 2671 2144 1996 14896 1012 2651 1996 2267 1010 7431 1999 1996 26249 1010 12731 12227 1010 1998 2358 7076 2239 1011 2128 7712 2243 9873 1997 3330 1010 2950 2274 7640 1997 2817 1516 13395 1998 6228 3330 1010 5072 1998 16012 5302 2571 15431 3330 1010 2942 3330 1998 9843 4163 1010 3274 2671 1998 3330 1010 1998 5992 3330 1516 2007 2809 1038 1012 1055 1012 5445 3253 1012 5678 1010 1996 2267 4107 2274 1011 2095 7037 3014 3454 2007 1996 6667 1997 2840 1998 4144 1998 1997 2449 21467 3176 1038 1012 1037 1012 1998 3040 1997 2449 3447 1006 15038 1007 5445 1010 4414 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0420 19:06:21.117180 140407150372736 squad_lib.py:369] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0420 19:06:21.117340 140407150372736 squad_lib.py:370] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0420 19:06:21.212036 140407150372736 squad_lib.py:375] start_position: 74\n",
            "I0420 19:06:21.212210 140407150372736 squad_lib.py:376] end_position: 74\n",
            "I0420 19:06:21.212285 140407150372736 squad_lib.py:377] answer: five\n",
            "I0420 19:06:21.219128 140407150372736 squad_lib.py:353] *** Example ***\n",
            "I0420 19:06:21.219275 140407150372736 squad_lib.py:354] unique_id: 1000000019\n",
            "I0420 19:06:21.219355 140407150372736 squad_lib.py:355] example_index: 19\n",
            "I0420 19:06:21.219421 140407150372736 squad_lib.py:356] doc_span_index: 0\n",
            "I0420 19:06:21.219547 140407150372736 squad_lib.py:358] tokens: [CLS] the college of science began to offer civil engineering courses beginning at what time at notre dame ? [SEP] the college of engineering was established in 1920 , however , early courses in civil and mechanical engineering were a part of the college of science since the 1870s . today the college , housed in the fitzpatrick , cu ##shing , and st ##ins ##on - re ##mic ##k halls of engineering , includes five departments of study – aerospace and mechanical engineering , chemical and bio ##mo ##le ##cular engineering , civil engineering and geological sciences , computer science and engineering , and electrical engineering – with eight b . s . degrees offered . additionally , the college offers five - year dual degree programs with the colleges of arts and letters and of business awarding additional b . a . and master of business administration ( mba ) degrees , respectively . [SEP]\n",
            "I0420 19:06:21.219692 140407150372736 squad_lib.py:361] token_to_orig_map: 20:0 21:1 22:2 23:3 24:4 25:5 26:6 27:7 28:7 29:8 30:8 31:9 32:10 33:11 34:12 35:13 36:14 37:15 38:16 39:17 40:18 41:19 42:20 43:21 44:22 45:23 46:24 47:25 48:26 49:26 50:27 51:28 52:29 53:29 54:30 55:31 56:32 57:33 58:33 59:34 60:34 61:34 62:35 63:36 64:36 65:36 66:36 67:36 68:36 69:36 70:37 71:38 72:39 73:39 74:40 75:41 76:42 77:43 78:44 79:45 80:46 81:47 82:48 83:49 84:49 85:50 86:51 87:52 88:52 89:52 90:52 91:53 92:53 93:54 94:55 95:56 96:57 97:58 98:58 99:59 100:60 101:61 102:62 103:62 104:63 105:64 106:65 107:66 108:67 109:68 110:69 111:69 112:69 113:69 114:70 115:71 116:71 117:72 118:72 119:73 120:74 121:75 122:76 123:76 124:76 125:77 126:78 127:79 128:80 129:81 130:82 131:83 132:84 133:85 134:86 135:87 136:88 137:89 138:90 139:91 140:92 141:92 142:92 143:92 144:93 145:94 146:95 147:96 148:97 149:98 150:98 151:98 152:99 153:99 154:100 155:100\n",
            "I0420 19:06:21.219821 140407150372736 squad_lib.py:366] token_is_max_context: 20:True 21:True 22:True 23:True 24:True 25:True 26:True 27:True 28:True 29:True 30:True 31:True 32:True 33:True 34:True 35:True 36:True 37:True 38:True 39:True 40:True 41:True 42:True 43:True 44:True 45:True 46:True 47:True 48:True 49:True 50:True 51:True 52:True 53:True 54:True 55:True 56:True 57:True 58:True 59:True 60:True 61:True 62:True 63:True 64:True 65:True 66:True 67:True 68:True 69:True 70:True 71:True 72:True 73:True 74:True 75:True 76:True 77:True 78:True 79:True 80:True 81:True 82:True 83:True 84:True 85:True 86:True 87:True 88:True 89:True 90:True 91:True 92:True 93:True 94:True 95:True 96:True 97:True 98:True 99:True 100:True 101:True 102:True 103:True 104:True 105:True 106:True 107:True 108:True 109:True 110:True 111:True 112:True 113:True 114:True 115:True 116:True 117:True 118:True 119:True 120:True 121:True 122:True 123:True 124:True 125:True 126:True 127:True 128:True 129:True 130:True 131:True 132:True 133:True 134:True 135:True 136:True 137:True 138:True 139:True 140:True 141:True 142:True 143:True 144:True 145:True 146:True 147:True 148:True 149:True 150:True 151:True 152:True 153:True 154:True 155:True\n",
            "I0420 19:06:21.220008 140407150372736 squad_lib.py:368] input_ids: 101 1996 2267 1997 2671 2211 2000 3749 2942 3330 5352 2927 2012 2054 2051 2012 10289 8214 1029 102 1996 2267 1997 3330 2001 2511 1999 4444 1010 2174 1010 2220 5352 1999 2942 1998 6228 3330 2020 1037 2112 1997 1996 2267 1997 2671 2144 1996 14896 1012 2651 1996 2267 1010 7431 1999 1996 26249 1010 12731 12227 1010 1998 2358 7076 2239 1011 2128 7712 2243 9873 1997 3330 1010 2950 2274 7640 1997 2817 1516 13395 1998 6228 3330 1010 5072 1998 16012 5302 2571 15431 3330 1010 2942 3330 1998 9843 4163 1010 3274 2671 1998 3330 1010 1998 5992 3330 1516 2007 2809 1038 1012 1055 1012 5445 3253 1012 5678 1010 1996 2267 4107 2274 1011 2095 7037 3014 3454 2007 1996 6667 1997 2840 1998 4144 1998 1997 2449 21467 3176 1038 1012 1037 1012 1998 3040 1997 2449 3447 1006 15038 1007 5445 1010 4414 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0420 19:06:21.220184 140407150372736 squad_lib.py:369] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0420 19:06:21.220354 140407150372736 squad_lib.py:370] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0420 19:06:21.220415 140407150372736 squad_lib.py:375] start_position: 47\n",
            "I0420 19:06:21.220482 140407150372736 squad_lib.py:376] end_position: 48\n",
            "I0420 19:06:21.220541 140407150372736 squad_lib.py:377] answer: the 1870s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "txLcisEV1UAb"
      },
      "source": [
        "# Start fine-tuning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pPuM_wYA1Z5m",
        "colab_type": "code",
        "outputId": "2b7c64a2-78e6-44b1-b0e6-1b5c5ddf6166",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "#################################################################################################\n",
        "# This script does the actual fine-tuning. However, it does not evaluate the performance of \n",
        "# training. It only writes the predictions after training / fine-tuning the model \n",
        "# to a file 'predictions.json' in the folder ${OUTPUT_DIR}.\n",
        "# Actual evaluation is done by running another script that compares it with the file: ${SQUAD_PRED_FILE}.\n",
        "#################################################################################################\n",
        "print(\"\\nCurrent working directory is: \" + os.getcwd() + \"\\n\")\n",
        "!python ${FINETUNING_TRG_SCRIPT} \\\n",
        "  --input_meta_data_path=${OUTPUT_DIR}/squad_${SQUAD_VERSION}_meta_data \\\n",
        "  --train_data_path=${OUTPUT_DIR}/squad_${SQUAD_VERSION}_train.tf_record \\\n",
        "  --predict_file=${SQUAD_PRED_FILE} \\\n",
        "  --vocab_file=${OUTPUT_DIR}/vocab.txt \\\n",
        "  --bert_config_file=${OUTPUT_DIR}/bert_config.json \\\n",
        "  --hub_module_url=${BERT_MODEL_TO_FINE_TUNE_ON_TF_HUB} \\\n",
        "  --train_batch_size=8 \\\n",
        "  --predict_batch_size=8 \\\n",
        "  --learning_rate=8e-5 \\\n",
        "  --num_train_epochs=2 \\\n",
        "  --mode=train_and_predict \\\n",
        "  --model_dir=${OUTPUT_DIR} \\\n",
        "  --distribution_strategy=mirrored \\\n",
        "  --log_steps=50 \\\n",
        "  --run_eagerly=False \\\n",
        "  --do_lower_case=True"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "I0420 22:02:31.359487 139904526645120 model_training_utils.py:450] Train Step: 17302/21936  / loss = 0.046177130192518234\n",
            "I0420 22:02:31.899845 139904526645120 model_training_utils.py:450] Train Step: 17303/21936  / loss = 0.8529201745986938\n",
            "I0420 22:02:32.437416 139904526645120 model_training_utils.py:450] Train Step: 17304/21936  / loss = 0.8002827763557434\n",
            "I0420 22:02:32.974557 139904526645120 model_training_utils.py:450] Train Step: 17305/21936  / loss = 0.2637821137905121\n",
            "I0420 22:02:33.517811 139904526645120 model_training_utils.py:450] Train Step: 17306/21936  / loss = 0.12173084169626236\n",
            "I0420 22:02:34.055825 139904526645120 model_training_utils.py:450] Train Step: 17307/21936  / loss = 0.3473479151725769\n",
            "I0420 22:02:34.599259 139904526645120 model_training_utils.py:450] Train Step: 17308/21936  / loss = 0.09881244599819183\n",
            "I0420 22:02:35.136355 139904526645120 model_training_utils.py:450] Train Step: 17309/21936  / loss = 0.4933514893054962\n",
            "I0420 22:02:35.673943 139904526645120 model_training_utils.py:450] Train Step: 17310/21936  / loss = 0.6614837646484375\n",
            "I0420 22:02:36.210841 139904526645120 model_training_utils.py:450] Train Step: 17311/21936  / loss = 1.3642094135284424\n",
            "I0420 22:02:36.747475 139904526645120 model_training_utils.py:450] Train Step: 17312/21936  / loss = 0.35059666633605957\n",
            "I0420 22:02:37.285552 139904526645120 model_training_utils.py:450] Train Step: 17313/21936  / loss = 1.9566161632537842\n",
            "I0420 22:02:37.826559 139904526645120 model_training_utils.py:450] Train Step: 17314/21936  / loss = 0.08928196132183075\n",
            "I0420 22:02:38.364073 139904526645120 model_training_utils.py:450] Train Step: 17315/21936  / loss = 0.5189498066902161\n",
            "I0420 22:02:38.908365 139904526645120 model_training_utils.py:450] Train Step: 17316/21936  / loss = 0.5444698929786682\n",
            "I0420 22:02:39.442955 139904526645120 model_training_utils.py:450] Train Step: 17317/21936  / loss = 0.22562485933303833\n",
            "I0420 22:02:39.980779 139904526645120 model_training_utils.py:450] Train Step: 17318/21936  / loss = 0.23186492919921875\n",
            "I0420 22:02:40.519061 139904526645120 keras_utils.py:122] TimeHistory: 26.94 seconds, 14.85 examples/second between steps 28237 and 28287\n",
            "I0420 22:02:40.521763 139904526645120 model_training_utils.py:450] Train Step: 17319/21936  / loss = 0.8118116855621338\n",
            "I0420 22:02:41.062752 139904526645120 model_training_utils.py:450] Train Step: 17320/21936  / loss = 0.46961188316345215\n",
            "I0420 22:02:41.603183 139904526645120 model_training_utils.py:450] Train Step: 17321/21936  / loss = 1.5208635330200195\n",
            "I0420 22:02:42.141940 139904526645120 model_training_utils.py:450] Train Step: 17322/21936  / loss = 0.7245335578918457\n",
            "I0420 22:02:42.680293 139904526645120 model_training_utils.py:450] Train Step: 17323/21936  / loss = 0.74198979139328\n",
            "I0420 22:02:43.218787 139904526645120 model_training_utils.py:450] Train Step: 17324/21936  / loss = 0.5308285355567932\n",
            "I0420 22:02:43.755647 139904526645120 model_training_utils.py:450] Train Step: 17325/21936  / loss = 0.8134938478469849\n",
            "I0420 22:02:44.293711 139904526645120 model_training_utils.py:450] Train Step: 17326/21936  / loss = 0.3644115924835205\n",
            "I0420 22:02:44.829600 139904526645120 model_training_utils.py:450] Train Step: 17327/21936  / loss = 1.584975242614746\n",
            "I0420 22:02:45.366230 139904526645120 model_training_utils.py:450] Train Step: 17328/21936  / loss = 0.15095815062522888\n",
            "I0420 22:02:45.902594 139904526645120 model_training_utils.py:450] Train Step: 17329/21936  / loss = 0.3924758732318878\n",
            "I0420 22:02:46.437782 139904526645120 model_training_utils.py:450] Train Step: 17330/21936  / loss = 0.18262529373168945\n",
            "I0420 22:02:46.977472 139904526645120 model_training_utils.py:450] Train Step: 17331/21936  / loss = 0.3467567563056946\n",
            "I0420 22:02:47.515234 139904526645120 model_training_utils.py:450] Train Step: 17332/21936  / loss = 1.6735376119613647\n",
            "I0420 22:02:48.054370 139904526645120 model_training_utils.py:450] Train Step: 17333/21936  / loss = 0.6429890394210815\n",
            "I0420 22:02:48.593844 139904526645120 model_training_utils.py:450] Train Step: 17334/21936  / loss = 0.631928563117981\n",
            "I0420 22:02:49.133486 139904526645120 model_training_utils.py:450] Train Step: 17335/21936  / loss = 0.5179126262664795\n",
            "I0420 22:02:49.670633 139904526645120 model_training_utils.py:450] Train Step: 17336/21936  / loss = 0.3799578547477722\n",
            "I0420 22:02:50.210371 139904526645120 model_training_utils.py:450] Train Step: 17337/21936  / loss = 0.7767331004142761\n",
            "I0420 22:02:50.747932 139904526645120 model_training_utils.py:450] Train Step: 17338/21936  / loss = 1.3177818059921265\n",
            "I0420 22:02:51.289403 139904526645120 model_training_utils.py:450] Train Step: 17339/21936  / loss = 0.36734408140182495\n",
            "I0420 22:02:51.827963 139904526645120 model_training_utils.py:450] Train Step: 17340/21936  / loss = 0.5577830076217651\n",
            "I0420 22:02:52.363678 139904526645120 model_training_utils.py:450] Train Step: 17341/21936  / loss = 0.31171131134033203\n",
            "I0420 22:02:52.904847 139904526645120 model_training_utils.py:450] Train Step: 17342/21936  / loss = 0.5089237689971924\n",
            "I0420 22:02:53.445850 139904526645120 model_training_utils.py:450] Train Step: 17343/21936  / loss = 1.000410556793213\n",
            "I0420 22:02:53.982273 139904526645120 model_training_utils.py:450] Train Step: 17344/21936  / loss = 1.8720638751983643\n",
            "I0420 22:02:54.521159 139904526645120 model_training_utils.py:450] Train Step: 17345/21936  / loss = 0.19605618715286255\n",
            "I0420 22:02:55.056960 139904526645120 model_training_utils.py:450] Train Step: 17346/21936  / loss = 0.2667647898197174\n",
            "I0420 22:02:55.592619 139904526645120 model_training_utils.py:450] Train Step: 17347/21936  / loss = 0.4596163034439087\n",
            "I0420 22:02:56.129523 139904526645120 model_training_utils.py:450] Train Step: 17348/21936  / loss = 1.6512786149978638\n",
            "I0420 22:02:56.668372 139904526645120 model_training_utils.py:450] Train Step: 17349/21936  / loss = 1.0398640632629395\n",
            "I0420 22:02:57.205120 139904526645120 model_training_utils.py:450] Train Step: 17350/21936  / loss = 0.05195765942335129\n",
            "I0420 22:02:57.743206 139904526645120 model_training_utils.py:450] Train Step: 17351/21936  / loss = 0.31246933341026306\n",
            "I0420 22:02:58.280175 139904526645120 model_training_utils.py:450] Train Step: 17352/21936  / loss = 0.35112473368644714\n",
            "I0420 22:02:58.819637 139904526645120 model_training_utils.py:450] Train Step: 17353/21936  / loss = 0.2709452211856842\n",
            "I0420 22:02:59.357383 139904526645120 model_training_utils.py:450] Train Step: 17354/21936  / loss = 1.6239981651306152\n",
            "I0420 22:02:59.894300 139904526645120 model_training_utils.py:450] Train Step: 17355/21936  / loss = 0.1909676492214203\n",
            "I0420 22:03:00.433156 139904526645120 model_training_utils.py:450] Train Step: 17356/21936  / loss = 0.39628228545188904\n",
            "I0420 22:03:00.968845 139904526645120 model_training_utils.py:450] Train Step: 17357/21936  / loss = 0.4523524045944214\n",
            "I0420 22:03:01.507240 139904526645120 model_training_utils.py:450] Train Step: 17358/21936  / loss = 0.6918933391571045\n",
            "I0420 22:03:02.045463 139904526645120 model_training_utils.py:450] Train Step: 17359/21936  / loss = 0.41683951020240784\n",
            "I0420 22:03:02.583081 139904526645120 model_training_utils.py:450] Train Step: 17360/21936  / loss = 0.06825235486030579\n",
            "I0420 22:03:03.119967 139904526645120 model_training_utils.py:450] Train Step: 17361/21936  / loss = 0.664137601852417\n",
            "I0420 22:03:03.659180 139904526645120 model_training_utils.py:450] Train Step: 17362/21936  / loss = 0.2440194934606552\n",
            "I0420 22:03:04.196374 139904526645120 model_training_utils.py:450] Train Step: 17363/21936  / loss = 0.2616727650165558\n",
            "I0420 22:03:04.731431 139904526645120 model_training_utils.py:450] Train Step: 17364/21936  / loss = 0.12019682675600052\n",
            "I0420 22:03:05.270393 139904526645120 model_training_utils.py:450] Train Step: 17365/21936  / loss = 0.13493570685386658\n",
            "I0420 22:03:05.812029 139904526645120 model_training_utils.py:450] Train Step: 17366/21936  / loss = 0.4106113612651825\n",
            "I0420 22:03:06.355094 139904526645120 model_training_utils.py:450] Train Step: 17367/21936  / loss = 0.5653811693191528\n",
            "I0420 22:03:06.890849 139904526645120 model_training_utils.py:450] Train Step: 17368/21936  / loss = 0.09522680938243866\n",
            "I0420 22:03:07.426209 139904526645120 keras_utils.py:122] TimeHistory: 26.90 seconds, 14.87 examples/second between steps 28287 and 28337\n",
            "I0420 22:03:07.429533 139904526645120 model_training_utils.py:450] Train Step: 17369/21936  / loss = 0.05266664922237396\n",
            "I0420 22:03:07.968074 139904526645120 model_training_utils.py:450] Train Step: 17370/21936  / loss = 0.2921072542667389\n",
            "I0420 22:03:08.505676 139904526645120 model_training_utils.py:450] Train Step: 17371/21936  / loss = 0.331013947725296\n",
            "I0420 22:03:09.042708 139904526645120 model_training_utils.py:450] Train Step: 17372/21936  / loss = 0.0791865885257721\n",
            "I0420 22:03:09.579997 139904526645120 model_training_utils.py:450] Train Step: 17373/21936  / loss = 0.7443199753761292\n",
            "I0420 22:03:10.116957 139904526645120 model_training_utils.py:450] Train Step: 17374/21936  / loss = 0.9687066674232483\n",
            "I0420 22:03:10.653134 139904526645120 model_training_utils.py:450] Train Step: 17375/21936  / loss = 0.059312332421541214\n",
            "I0420 22:03:11.194928 139904526645120 model_training_utils.py:450] Train Step: 17376/21936  / loss = 0.1701710969209671\n",
            "I0420 22:03:11.732050 139904526645120 model_training_utils.py:450] Train Step: 17377/21936  / loss = 0.06198478862643242\n",
            "I0420 22:03:12.271048 139904526645120 model_training_utils.py:450] Train Step: 17378/21936  / loss = 0.3329300284385681\n",
            "I0420 22:03:12.807322 139904526645120 model_training_utils.py:450] Train Step: 17379/21936  / loss = 1.2324488162994385\n",
            "I0420 22:03:13.345083 139904526645120 model_training_utils.py:450] Train Step: 17380/21936  / loss = 1.0861990451812744\n",
            "I0420 22:03:13.882647 139904526645120 model_training_utils.py:450] Train Step: 17381/21936  / loss = 0.6781270503997803\n",
            "I0420 22:03:14.420470 139904526645120 model_training_utils.py:450] Train Step: 17382/21936  / loss = 1.0477218627929688\n",
            "I0420 22:03:14.960284 139904526645120 model_training_utils.py:450] Train Step: 17383/21936  / loss = 1.0100646018981934\n",
            "I0420 22:03:15.499767 139904526645120 model_training_utils.py:450] Train Step: 17384/21936  / loss = 0.6239435076713562\n",
            "I0420 22:03:16.040045 139904526645120 model_training_utils.py:450] Train Step: 17385/21936  / loss = 0.1595098227262497\n",
            "I0420 22:03:16.577949 139904526645120 model_training_utils.py:450] Train Step: 17386/21936  / loss = 0.2665225565433502\n",
            "I0420 22:03:17.117315 139904526645120 model_training_utils.py:450] Train Step: 17387/21936  / loss = 0.8464224338531494\n",
            "I0420 22:03:17.655990 139904526645120 model_training_utils.py:450] Train Step: 17388/21936  / loss = 0.5344541072845459\n",
            "I0420 22:03:18.195744 139904526645120 model_training_utils.py:450] Train Step: 17389/21936  / loss = 0.9321883320808411\n",
            "I0420 22:03:18.732484 139904526645120 model_training_utils.py:450] Train Step: 17390/21936  / loss = 1.4578381776809692\n",
            "I0420 22:03:19.267252 139904526645120 model_training_utils.py:450] Train Step: 17391/21936  / loss = 0.6059685349464417\n",
            "I0420 22:03:19.805321 139904526645120 model_training_utils.py:450] Train Step: 17392/21936  / loss = 0.3296698033809662\n",
            "I0420 22:03:20.344100 139904526645120 model_training_utils.py:450] Train Step: 17393/21936  / loss = 0.6360081434249878\n",
            "I0420 22:03:20.880158 139904526645120 model_training_utils.py:450] Train Step: 17394/21936  / loss = 1.1367309093475342\n",
            "I0420 22:03:21.416234 139904526645120 model_training_utils.py:450] Train Step: 17395/21936  / loss = 0.4766079783439636\n",
            "I0420 22:03:21.953372 139904526645120 model_training_utils.py:450] Train Step: 17396/21936  / loss = 1.8906432390213013\n",
            "I0420 22:03:22.491228 139904526645120 model_training_utils.py:450] Train Step: 17397/21936  / loss = 1.340964674949646\n",
            "I0420 22:03:23.028118 139904526645120 model_training_utils.py:450] Train Step: 17398/21936  / loss = 0.34156280755996704\n",
            "I0420 22:03:23.566376 139904526645120 model_training_utils.py:450] Train Step: 17399/21936  / loss = 1.183337926864624\n",
            "I0420 22:03:24.102743 139904526645120 model_training_utils.py:450] Train Step: 17400/21936  / loss = 0.6558609008789062\n",
            "I0420 22:03:24.638776 139904526645120 model_training_utils.py:450] Train Step: 17401/21936  / loss = 1.3209701776504517\n",
            "I0420 22:03:25.175049 139904526645120 model_training_utils.py:450] Train Step: 17402/21936  / loss = 0.5471218824386597\n",
            "I0420 22:03:25.712065 139904526645120 model_training_utils.py:450] Train Step: 17403/21936  / loss = 0.6403247714042664\n",
            "I0420 22:03:26.251167 139904526645120 model_training_utils.py:450] Train Step: 17404/21936  / loss = 0.9751071929931641\n",
            "I0420 22:03:26.788501 139904526645120 model_training_utils.py:450] Train Step: 17405/21936  / loss = 0.8757001161575317\n",
            "I0420 22:03:27.326046 139904526645120 model_training_utils.py:450] Train Step: 17406/21936  / loss = 0.7727512717247009\n",
            "I0420 22:03:27.871466 139904526645120 model_training_utils.py:450] Train Step: 17407/21936  / loss = 1.00835120677948\n",
            "I0420 22:03:28.412006 139904526645120 model_training_utils.py:450] Train Step: 17408/21936  / loss = 0.6664103865623474\n",
            "I0420 22:03:28.949508 139904526645120 model_training_utils.py:450] Train Step: 17409/21936  / loss = 1.8288651704788208\n",
            "I0420 22:03:29.485956 139904526645120 model_training_utils.py:450] Train Step: 17410/21936  / loss = 1.0934240818023682\n",
            "I0420 22:03:30.022439 139904526645120 model_training_utils.py:450] Train Step: 17411/21936  / loss = 1.3082317113876343\n",
            "I0420 22:03:30.560480 139904526645120 model_training_utils.py:450] Train Step: 17412/21936  / loss = 0.9710825085639954\n",
            "I0420 22:03:31.098615 139904526645120 model_training_utils.py:450] Train Step: 17413/21936  / loss = 0.6072172522544861\n",
            "I0420 22:03:31.634975 139904526645120 model_training_utils.py:450] Train Step: 17414/21936  / loss = 0.3026618957519531\n",
            "I0420 22:03:32.173084 139904526645120 model_training_utils.py:450] Train Step: 17415/21936  / loss = 1.5227299928665161\n",
            "I0420 22:03:32.709325 139904526645120 model_training_utils.py:450] Train Step: 17416/21936  / loss = 0.4659962058067322\n",
            "I0420 22:03:33.246656 139904526645120 model_training_utils.py:450] Train Step: 17417/21936  / loss = 0.47100555896759033\n",
            "I0420 22:03:33.784364 139904526645120 model_training_utils.py:450] Train Step: 17418/21936  / loss = 0.48288339376449585\n",
            "I0420 22:03:34.321218 139904526645120 keras_utils.py:122] TimeHistory: 26.89 seconds, 14.88 examples/second between steps 28337 and 28387\n",
            "I0420 22:03:34.323744 139904526645120 model_training_utils.py:450] Train Step: 17419/21936  / loss = 1.4727983474731445\n",
            "I0420 22:03:34.862944 139904526645120 model_training_utils.py:450] Train Step: 17420/21936  / loss = 0.29944753646850586\n",
            "I0420 22:03:35.399856 139904526645120 model_training_utils.py:450] Train Step: 17421/21936  / loss = 0.9907262921333313\n",
            "I0420 22:03:35.941015 139904526645120 model_training_utils.py:450] Train Step: 17422/21936  / loss = 0.18433837592601776\n",
            "I0420 22:03:36.480169 139904526645120 model_training_utils.py:450] Train Step: 17423/21936  / loss = 0.9252009391784668\n",
            "I0420 22:03:37.018136 139904526645120 model_training_utils.py:450] Train Step: 17424/21936  / loss = 0.44003304839134216\n",
            "I0420 22:03:37.555410 139904526645120 model_training_utils.py:450] Train Step: 17425/21936  / loss = 0.3017125129699707\n",
            "I0420 22:03:38.093444 139904526645120 model_training_utils.py:450] Train Step: 17426/21936  / loss = 0.5985634326934814\n",
            "I0420 22:03:38.634066 139904526645120 model_training_utils.py:450] Train Step: 17427/21936  / loss = 0.6257741451263428\n",
            "I0420 22:03:39.172280 139904526645120 model_training_utils.py:450] Train Step: 17428/21936  / loss = 0.681736409664154\n",
            "I0420 22:03:39.708183 139904526645120 model_training_utils.py:450] Train Step: 17429/21936  / loss = 0.6720672249794006\n",
            "I0420 22:03:40.245829 139904526645120 model_training_utils.py:450] Train Step: 17430/21936  / loss = 0.3263980448246002\n",
            "I0420 22:03:40.786843 139904526645120 model_training_utils.py:450] Train Step: 17431/21936  / loss = 0.14177274703979492\n",
            "I0420 22:03:41.324460 139904526645120 model_training_utils.py:450] Train Step: 17432/21936  / loss = 1.3418960571289062\n",
            "I0420 22:03:41.860107 139904526645120 model_training_utils.py:450] Train Step: 17433/21936  / loss = 1.0247251987457275\n",
            "I0420 22:03:42.396876 139904526645120 model_training_utils.py:450] Train Step: 17434/21936  / loss = 0.06658805161714554\n",
            "I0420 22:03:42.947517 139904526645120 model_training_utils.py:450] Train Step: 17435/21936  / loss = 0.2980159819126129\n",
            "I0420 22:03:43.484322 139904526645120 model_training_utils.py:450] Train Step: 17436/21936  / loss = 0.7070803642272949\n",
            "I0420 22:03:44.023805 139904526645120 model_training_utils.py:450] Train Step: 17437/21936  / loss = 0.5844119787216187\n",
            "I0420 22:03:44.562908 139904526645120 model_training_utils.py:450] Train Step: 17438/21936  / loss = 0.1071992963552475\n",
            "I0420 22:03:45.100622 139904526645120 model_training_utils.py:450] Train Step: 17439/21936  / loss = 0.4502105712890625\n",
            "I0420 22:03:45.637013 139904526645120 model_training_utils.py:450] Train Step: 17440/21936  / loss = 0.38689693808555603\n",
            "I0420 22:03:46.174322 139904526645120 model_training_utils.py:450] Train Step: 17441/21936  / loss = 0.3281650245189667\n",
            "I0420 22:03:46.710505 139904526645120 model_training_utils.py:450] Train Step: 17442/21936  / loss = 0.2737289369106293\n",
            "I0420 22:03:47.248089 139904526645120 model_training_utils.py:450] Train Step: 17443/21936  / loss = 0.08988402038812637\n",
            "I0420 22:03:47.783551 139904526645120 model_training_utils.py:450] Train Step: 17444/21936  / loss = 0.39502495527267456\n",
            "I0420 22:03:48.324584 139904526645120 model_training_utils.py:450] Train Step: 17445/21936  / loss = 0.2109130173921585\n",
            "I0420 22:03:48.862957 139904526645120 model_training_utils.py:450] Train Step: 17446/21936  / loss = 0.20451511442661285\n",
            "I0420 22:03:49.401413 139904526645120 model_training_utils.py:450] Train Step: 17447/21936  / loss = 0.49077653884887695\n",
            "I0420 22:03:49.938384 139904526645120 model_training_utils.py:450] Train Step: 17448/21936  / loss = 0.1866714209318161\n",
            "I0420 22:03:50.475632 139904526645120 model_training_utils.py:450] Train Step: 17449/21936  / loss = 0.3869854807853699\n",
            "I0420 22:03:51.016747 139904526645120 model_training_utils.py:450] Train Step: 17450/21936  / loss = 0.11743315309286118\n",
            "I0420 22:03:51.556675 139904526645120 model_training_utils.py:450] Train Step: 17451/21936  / loss = 0.2591008245944977\n",
            "I0420 22:03:52.094155 139904526645120 model_training_utils.py:450] Train Step: 17452/21936  / loss = 0.38542670011520386\n",
            "I0420 22:03:52.631698 139904526645120 model_training_utils.py:450] Train Step: 17453/21936  / loss = 0.5168759226799011\n",
            "I0420 22:03:53.166657 139904526645120 model_training_utils.py:450] Train Step: 17454/21936  / loss = 0.22649367153644562\n",
            "I0420 22:03:53.704097 139904526645120 model_training_utils.py:450] Train Step: 17455/21936  / loss = 0.3743990957736969\n",
            "I0420 22:03:54.240983 139904526645120 model_training_utils.py:450] Train Step: 17456/21936  / loss = 0.17695532739162445\n",
            "I0420 22:03:54.777831 139904526645120 model_training_utils.py:450] Train Step: 17457/21936  / loss = 0.04427901655435562\n",
            "I0420 22:03:55.315384 139904526645120 model_training_utils.py:450] Train Step: 17458/21936  / loss = 0.28385069966316223\n",
            "I0420 22:03:55.860408 139904526645120 model_training_utils.py:450] Train Step: 17459/21936  / loss = 0.25591275095939636\n",
            "I0420 22:03:56.398431 139904526645120 model_training_utils.py:450] Train Step: 17460/21936  / loss = 0.39126136898994446\n",
            "I0420 22:03:56.938481 139904526645120 model_training_utils.py:450] Train Step: 17461/21936  / loss = 0.3503810167312622\n",
            "I0420 22:03:57.474884 139904526645120 model_training_utils.py:450] Train Step: 17462/21936  / loss = 0.2909407913684845\n",
            "I0420 22:03:58.009084 139904526645120 model_training_utils.py:450] Train Step: 17463/21936  / loss = 0.5041353702545166\n",
            "I0420 22:03:58.548371 139904526645120 model_training_utils.py:450] Train Step: 17464/21936  / loss = 0.16520871222019196\n",
            "I0420 22:03:59.086115 139904526645120 model_training_utils.py:450] Train Step: 17465/21936  / loss = 0.12872397899627686\n",
            "I0420 22:03:59.623377 139904526645120 model_training_utils.py:450] Train Step: 17466/21936  / loss = 0.0817485898733139\n",
            "I0420 22:04:00.160310 139904526645120 model_training_utils.py:450] Train Step: 17467/21936  / loss = 0.055010586977005005\n",
            "I0420 22:04:00.697348 139904526645120 model_training_utils.py:450] Train Step: 17468/21936  / loss = 0.12527625262737274\n",
            "I0420 22:04:01.233877 139904526645120 keras_utils.py:122] TimeHistory: 26.91 seconds, 14.86 examples/second between steps 28387 and 28437\n",
            "I0420 22:04:01.236726 139904526645120 model_training_utils.py:450] Train Step: 17469/21936  / loss = 0.08669869601726532\n",
            "I0420 22:04:01.772907 139904526645120 model_training_utils.py:450] Train Step: 17470/21936  / loss = 0.4083417057991028\n",
            "I0420 22:04:02.309642 139904526645120 model_training_utils.py:450] Train Step: 17471/21936  / loss = 0.1911461502313614\n",
            "I0420 22:04:02.843599 139904526645120 model_training_utils.py:450] Train Step: 17472/21936  / loss = 0.8483500480651855\n",
            "I0420 22:04:03.381423 139904526645120 model_training_utils.py:450] Train Step: 17473/21936  / loss = 0.16416378319263458\n",
            "I0420 22:04:03.919765 139904526645120 model_training_utils.py:450] Train Step: 17474/21936  / loss = 0.3211919069290161\n",
            "I0420 22:04:04.456693 139904526645120 model_training_utils.py:450] Train Step: 17475/21936  / loss = 0.08787432312965393\n",
            "I0420 22:04:04.993150 139904526645120 model_training_utils.py:450] Train Step: 17476/21936  / loss = 0.5229417085647583\n",
            "I0420 22:04:05.529210 139904526645120 model_training_utils.py:450] Train Step: 17477/21936  / loss = 0.024942325428128242\n",
            "I0420 22:04:06.066614 139904526645120 model_training_utils.py:450] Train Step: 17478/21936  / loss = 0.1794968992471695\n",
            "I0420 22:04:06.602670 139904526645120 model_training_utils.py:450] Train Step: 17479/21936  / loss = 0.6875298619270325\n",
            "I0420 22:04:07.141114 139904526645120 model_training_utils.py:450] Train Step: 17480/21936  / loss = 0.3503705561161041\n",
            "I0420 22:04:07.679094 139904526645120 model_training_utils.py:450] Train Step: 17481/21936  / loss = 1.2863835096359253\n",
            "I0420 22:04:08.218217 139904526645120 model_training_utils.py:450] Train Step: 17482/21936  / loss = 0.1056831106543541\n",
            "I0420 22:04:08.753831 139904526645120 model_training_utils.py:450] Train Step: 17483/21936  / loss = 0.2190539389848709\n",
            "I0420 22:04:09.290965 139904526645120 model_training_utils.py:450] Train Step: 17484/21936  / loss = 1.120411992073059\n",
            "I0420 22:04:09.826142 139904526645120 model_training_utils.py:450] Train Step: 17485/21936  / loss = 0.28418058156967163\n",
            "I0420 22:04:10.364686 139904526645120 model_training_utils.py:450] Train Step: 17486/21936  / loss = 0.353160560131073\n",
            "I0420 22:04:10.902999 139904526645120 model_training_utils.py:450] Train Step: 17487/21936  / loss = 1.0814403295516968\n",
            "I0420 22:04:11.436972 139904526645120 model_training_utils.py:450] Train Step: 17488/21936  / loss = 0.23554903268814087\n",
            "I0420 22:04:11.986525 139904526645120 model_training_utils.py:450] Train Step: 17489/21936  / loss = 1.318955898284912\n",
            "I0420 22:04:12.525286 139904526645120 model_training_utils.py:450] Train Step: 17490/21936  / loss = 1.5338751077651978\n",
            "I0420 22:04:13.062922 139904526645120 model_training_utils.py:450] Train Step: 17491/21936  / loss = 0.34082990884780884\n",
            "I0420 22:04:13.599663 139904526645120 model_training_utils.py:450] Train Step: 17492/21936  / loss = 0.39115989208221436\n",
            "I0420 22:04:14.143882 139904526645120 model_training_utils.py:450] Train Step: 17493/21936  / loss = 0.6640077829360962\n",
            "I0420 22:04:14.678900 139904526645120 model_training_utils.py:450] Train Step: 17494/21936  / loss = 0.5849956274032593\n",
            "I0420 22:04:15.217851 139904526645120 model_training_utils.py:450] Train Step: 17495/21936  / loss = 0.49445533752441406\n",
            "I0420 22:04:15.752712 139904526645120 model_training_utils.py:450] Train Step: 17496/21936  / loss = 0.29694128036499023\n",
            "I0420 22:04:16.290121 139904526645120 model_training_utils.py:450] Train Step: 17497/21936  / loss = 0.43141037225723267\n",
            "I0420 22:04:16.826900 139904526645120 model_training_utils.py:450] Train Step: 17498/21936  / loss = 1.1124522686004639\n",
            "I0420 22:04:17.365819 139904526645120 model_training_utils.py:450] Train Step: 17499/21936  / loss = 0.7145460247993469\n",
            "I0420 22:04:17.903331 139904526645120 model_training_utils.py:450] Train Step: 17500/21936  / loss = 0.29673150181770325\n",
            "I0420 22:04:18.439598 139904526645120 model_training_utils.py:450] Train Step: 17501/21936  / loss = 0.948735237121582\n",
            "I0420 22:04:18.980603 139904526645120 model_training_utils.py:450] Train Step: 17502/21936  / loss = 0.23949813842773438\n",
            "I0420 22:04:19.519728 139904526645120 model_training_utils.py:450] Train Step: 17503/21936  / loss = 0.4475865662097931\n",
            "I0420 22:04:20.057157 139904526645120 model_training_utils.py:450] Train Step: 17504/21936  / loss = 0.11916817724704742\n",
            "I0420 22:04:20.603854 139904526645120 model_training_utils.py:450] Train Step: 17505/21936  / loss = 0.0911654606461525\n",
            "I0420 22:04:21.146270 139904526645120 model_training_utils.py:450] Train Step: 17506/21936  / loss = 1.5008337497711182\n",
            "I0420 22:04:21.691039 139904526645120 model_training_utils.py:450] Train Step: 17507/21936  / loss = 0.29333075881004333\n",
            "I0420 22:04:22.228640 139904526645120 model_training_utils.py:450] Train Step: 17508/21936  / loss = 0.2202644944190979\n",
            "I0420 22:04:22.765141 139904526645120 model_training_utils.py:450] Train Step: 17509/21936  / loss = 0.3084818720817566\n",
            "I0420 22:04:23.301873 139904526645120 model_training_utils.py:450] Train Step: 17510/21936  / loss = 0.7806620001792908\n",
            "I0420 22:04:23.838662 139904526645120 model_training_utils.py:450] Train Step: 17511/21936  / loss = 0.6387925148010254\n",
            "I0420 22:04:24.375123 139904526645120 model_training_utils.py:450] Train Step: 17512/21936  / loss = 0.11382931470870972\n",
            "I0420 22:04:24.912090 139904526645120 model_training_utils.py:450] Train Step: 17513/21936  / loss = 0.3242264688014984\n",
            "I0420 22:04:25.450438 139904526645120 model_training_utils.py:450] Train Step: 17514/21936  / loss = 0.6041443943977356\n",
            "I0420 22:04:25.985854 139904526645120 model_training_utils.py:450] Train Step: 17515/21936  / loss = 0.7818841338157654\n",
            "I0420 22:04:26.523204 139904526645120 model_training_utils.py:450] Train Step: 17516/21936  / loss = 1.1240277290344238\n",
            "I0420 22:04:27.061290 139904526645120 model_training_utils.py:450] Train Step: 17517/21936  / loss = 0.289993017911911\n",
            "I0420 22:04:27.596496 139904526645120 model_training_utils.py:450] Train Step: 17518/21936  / loss = 0.3370773494243622\n",
            "I0420 22:04:28.135110 139904526645120 keras_utils.py:122] TimeHistory: 26.90 seconds, 14.87 examples/second between steps 28437 and 28487\n",
            "I0420 22:04:28.137879 139904526645120 model_training_utils.py:450] Train Step: 17519/21936  / loss = 0.21573491394519806\n",
            "I0420 22:04:28.674891 139904526645120 model_training_utils.py:450] Train Step: 17520/21936  / loss = 0.1574527621269226\n",
            "I0420 22:04:29.211633 139904526645120 model_training_utils.py:450] Train Step: 17521/21936  / loss = 0.2571927309036255\n",
            "I0420 22:04:29.748631 139904526645120 model_training_utils.py:450] Train Step: 17522/21936  / loss = 0.25379592180252075\n",
            "I0420 22:04:30.285007 139904526645120 model_training_utils.py:450] Train Step: 17523/21936  / loss = 1.1189866065979004\n",
            "I0420 22:04:30.820970 139904526645120 model_training_utils.py:450] Train Step: 17524/21936  / loss = 1.0333868265151978\n",
            "I0420 22:04:31.360176 139904526645120 model_training_utils.py:450] Train Step: 17525/21936  / loss = 0.7350121736526489\n",
            "I0420 22:04:31.905291 139904526645120 model_training_utils.py:450] Train Step: 17526/21936  / loss = 0.4174996018409729\n",
            "I0420 22:04:32.445267 139904526645120 model_training_utils.py:450] Train Step: 17527/21936  / loss = 0.21555490791797638\n",
            "I0420 22:04:32.983906 139904526645120 model_training_utils.py:450] Train Step: 17528/21936  / loss = 1.1596519947052002\n",
            "I0420 22:04:33.524409 139904526645120 model_training_utils.py:450] Train Step: 17529/21936  / loss = 0.24835415184497833\n",
            "I0420 22:04:34.062424 139904526645120 model_training_utils.py:450] Train Step: 17530/21936  / loss = 0.23862509429454803\n",
            "I0420 22:04:34.602677 139904526645120 model_training_utils.py:450] Train Step: 17531/21936  / loss = 0.4266311526298523\n",
            "I0420 22:04:35.138495 139904526645120 model_training_utils.py:450] Train Step: 17532/21936  / loss = 0.3053573668003082\n",
            "I0420 22:04:35.677164 139904526645120 model_training_utils.py:450] Train Step: 17533/21936  / loss = 0.3198460042476654\n",
            "I0420 22:04:36.216529 139904526645120 model_training_utils.py:450] Train Step: 17534/21936  / loss = 1.2649332284927368\n",
            "I0420 22:04:36.754849 139904526645120 model_training_utils.py:450] Train Step: 17535/21936  / loss = 1.1594942808151245\n",
            "I0420 22:04:37.292774 139904526645120 model_training_utils.py:450] Train Step: 17536/21936  / loss = 0.22050493955612183\n",
            "I0420 22:04:37.828091 139904526645120 model_training_utils.py:450] Train Step: 17537/21936  / loss = 0.6521729230880737\n",
            "I0420 22:04:38.364255 139904526645120 model_training_utils.py:450] Train Step: 17538/21936  / loss = 0.6630837917327881\n",
            "I0420 22:04:38.901890 139904526645120 model_training_utils.py:450] Train Step: 17539/21936  / loss = 2.1917641162872314\n",
            "I0420 22:04:39.445232 139904526645120 model_training_utils.py:450] Train Step: 17540/21936  / loss = 0.3617274761199951\n",
            "I0420 22:04:39.985493 139904526645120 model_training_utils.py:450] Train Step: 17541/21936  / loss = 0.9305698871612549\n",
            "I0420 22:04:40.526625 139904526645120 model_training_utils.py:450] Train Step: 17542/21936  / loss = 0.8210664987564087\n",
            "I0420 22:04:41.064232 139904526645120 model_training_utils.py:450] Train Step: 17543/21936  / loss = 1.0023469924926758\n",
            "I0420 22:04:41.602382 139904526645120 model_training_utils.py:450] Train Step: 17544/21936  / loss = 0.36122551560401917\n",
            "I0420 22:04:42.140221 139904526645120 model_training_utils.py:450] Train Step: 17545/21936  / loss = 0.22980384528636932\n",
            "I0420 22:04:42.689164 139904526645120 model_training_utils.py:450] Train Step: 17546/21936  / loss = 0.5787186026573181\n",
            "I0420 22:04:43.226681 139904526645120 model_training_utils.py:450] Train Step: 17547/21936  / loss = 0.561113178730011\n",
            "I0420 22:04:43.762627 139904526645120 model_training_utils.py:450] Train Step: 17548/21936  / loss = 0.7182315587997437\n",
            "I0420 22:04:44.304879 139904526645120 model_training_utils.py:450] Train Step: 17549/21936  / loss = 0.54050612449646\n",
            "I0420 22:04:44.840778 139904526645120 model_training_utils.py:450] Train Step: 17550/21936  / loss = 0.4262697100639343\n",
            "I0420 22:04:45.377717 139904526645120 model_training_utils.py:450] Train Step: 17551/21936  / loss = 0.07626984268426895\n",
            "I0420 22:04:45.922979 139904526645120 model_training_utils.py:450] Train Step: 17552/21936  / loss = 0.614480197429657\n",
            "I0420 22:04:46.459120 139904526645120 model_training_utils.py:450] Train Step: 17553/21936  / loss = 0.6369142532348633\n",
            "I0420 22:04:46.997253 139904526645120 model_training_utils.py:450] Train Step: 17554/21936  / loss = 0.24280130863189697\n",
            "I0420 22:04:47.535459 139904526645120 model_training_utils.py:450] Train Step: 17555/21936  / loss = 0.5078495144844055\n",
            "I0420 22:04:48.074780 139904526645120 model_training_utils.py:450] Train Step: 17556/21936  / loss = 0.8095263242721558\n",
            "I0420 22:04:48.609932 139904526645120 model_training_utils.py:450] Train Step: 17557/21936  / loss = 0.24791169166564941\n",
            "I0420 22:04:49.143828 139904526645120 model_training_utils.py:450] Train Step: 17558/21936  / loss = 0.903045654296875\n",
            "I0420 22:04:49.681209 139904526645120 model_training_utils.py:450] Train Step: 17559/21936  / loss = 0.1569872945547104\n",
            "I0420 22:04:50.217118 139904526645120 model_training_utils.py:450] Train Step: 17560/21936  / loss = 1.3487120866775513\n",
            "I0420 22:04:50.755348 139904526645120 model_training_utils.py:450] Train Step: 17561/21936  / loss = 0.5947227478027344\n",
            "I0420 22:04:51.292056 139904526645120 model_training_utils.py:450] Train Step: 17562/21936  / loss = 0.48827844858169556\n",
            "I0420 22:04:51.835806 139904526645120 model_training_utils.py:450] Train Step: 17563/21936  / loss = 0.2859198749065399\n",
            "I0420 22:04:52.373923 139904526645120 model_training_utils.py:450] Train Step: 17564/21936  / loss = 1.1692882776260376\n",
            "I0420 22:04:52.912904 139904526645120 model_training_utils.py:450] Train Step: 17565/21936  / loss = 0.7232483625411987\n",
            "I0420 22:04:53.448651 139904526645120 model_training_utils.py:450] Train Step: 17566/21936  / loss = 0.5985763072967529\n",
            "I0420 22:04:53.986645 139904526645120 model_training_utils.py:450] Train Step: 17567/21936  / loss = 1.4367353916168213\n",
            "I0420 22:04:54.524745 139904526645120 model_training_utils.py:450] Train Step: 17568/21936  / loss = 2.004718065261841\n",
            "I0420 22:04:55.070419 139904526645120 keras_utils.py:122] TimeHistory: 26.93 seconds, 14.85 examples/second between steps 28487 and 28537\n",
            "I0420 22:04:55.073079 139904526645120 model_training_utils.py:450] Train Step: 17569/21936  / loss = 1.172450065612793\n",
            "I0420 22:04:55.611731 139904526645120 model_training_utils.py:450] Train Step: 17570/21936  / loss = 0.9060707092285156\n",
            "I0420 22:04:56.147334 139904526645120 model_training_utils.py:450] Train Step: 17571/21936  / loss = 0.46930786967277527\n",
            "I0420 22:04:56.684059 139904526645120 model_training_utils.py:450] Train Step: 17572/21936  / loss = 0.46566542983055115\n",
            "I0420 22:04:57.219823 139904526645120 model_training_utils.py:450] Train Step: 17573/21936  / loss = 0.7764166593551636\n",
            "I0420 22:04:57.758055 139904526645120 model_training_utils.py:450] Train Step: 17574/21936  / loss = 1.352475881576538\n",
            "I0420 22:04:58.295119 139904526645120 model_training_utils.py:450] Train Step: 17575/21936  / loss = 1.4600350856781006\n",
            "I0420 22:04:58.832730 139904526645120 model_training_utils.py:450] Train Step: 17576/21936  / loss = 0.7749453186988831\n",
            "I0420 22:04:59.370252 139904526645120 model_training_utils.py:450] Train Step: 17577/21936  / loss = 0.5517311096191406\n",
            "I0420 22:04:59.907428 139904526645120 model_training_utils.py:450] Train Step: 17578/21936  / loss = 1.0762912034988403\n",
            "I0420 22:05:00.443732 139904526645120 model_training_utils.py:450] Train Step: 17579/21936  / loss = 0.3767835795879364\n",
            "I0420 22:05:00.982150 139904526645120 model_training_utils.py:450] Train Step: 17580/21936  / loss = 0.7398678064346313\n",
            "I0420 22:05:01.520830 139904526645120 model_training_utils.py:450] Train Step: 17581/21936  / loss = 0.2186177372932434\n",
            "I0420 22:05:02.059227 139904526645120 model_training_utils.py:450] Train Step: 17582/21936  / loss = 0.3188228905200958\n",
            "I0420 22:05:02.600064 139904526645120 model_training_utils.py:450] Train Step: 17583/21936  / loss = 0.4373914897441864\n",
            "I0420 22:05:03.135230 139904526645120 model_training_utils.py:450] Train Step: 17584/21936  / loss = 1.2256038188934326\n",
            "I0420 22:05:03.672741 139904526645120 model_training_utils.py:450] Train Step: 17585/21936  / loss = 0.2051558792591095\n",
            "I0420 22:05:04.209089 139904526645120 model_training_utils.py:450] Train Step: 17586/21936  / loss = 0.8258333802223206\n",
            "I0420 22:05:04.745138 139904526645120 model_training_utils.py:450] Train Step: 17587/21936  / loss = 0.5716976523399353\n",
            "I0420 22:05:05.280093 139904526645120 model_training_utils.py:450] Train Step: 17588/21936  / loss = 0.349580317735672\n",
            "I0420 22:05:05.818506 139904526645120 model_training_utils.py:450] Train Step: 17589/21936  / loss = 0.2870939373970032\n",
            "I0420 22:05:06.356590 139904526645120 model_training_utils.py:450] Train Step: 17590/21936  / loss = 0.5179712772369385\n",
            "I0420 22:05:06.896128 139904526645120 model_training_utils.py:450] Train Step: 17591/21936  / loss = 0.1570063829421997\n",
            "I0420 22:05:07.438278 139904526645120 model_training_utils.py:450] Train Step: 17592/21936  / loss = 0.10855555534362793\n",
            "I0420 22:05:07.976383 139904526645120 model_training_utils.py:450] Train Step: 17593/21936  / loss = 0.37211471796035767\n",
            "I0420 22:05:08.514194 139904526645120 model_training_utils.py:450] Train Step: 17594/21936  / loss = 0.1716288924217224\n",
            "I0420 22:05:09.059495 139904526645120 model_training_utils.py:450] Train Step: 17595/21936  / loss = 0.29866844415664673\n",
            "I0420 22:05:09.597718 139904526645120 model_training_utils.py:450] Train Step: 17596/21936  / loss = 0.5750361680984497\n",
            "I0420 22:05:10.134742 139904526645120 model_training_utils.py:450] Train Step: 17597/21936  / loss = 0.7482980489730835\n",
            "I0420 22:05:10.671923 139904526645120 model_training_utils.py:450] Train Step: 17598/21936  / loss = 0.6185568571090698\n",
            "I0420 22:05:11.208291 139904526645120 model_training_utils.py:450] Train Step: 17599/21936  / loss = 0.18881003558635712\n",
            "I0420 22:05:11.743504 139904526645120 model_training_utils.py:450] Train Step: 17600/21936  / loss = 1.3165243864059448\n",
            "I0420 22:05:12.277649 139904526645120 model_training_utils.py:450] Train Step: 17601/21936  / loss = 0.17239272594451904\n",
            "I0420 22:05:12.820391 139904526645120 model_training_utils.py:450] Train Step: 17602/21936  / loss = 0.1704767644405365\n",
            "I0420 22:05:13.355730 139904526645120 model_training_utils.py:450] Train Step: 17603/21936  / loss = 0.2529906630516052\n",
            "I0420 22:05:13.893729 139904526645120 model_training_utils.py:450] Train Step: 17604/21936  / loss = 0.23646794259548187\n",
            "I0420 22:05:14.430989 139904526645120 model_training_utils.py:450] Train Step: 17605/21936  / loss = 0.3163391351699829\n",
            "I0420 22:05:14.968110 139904526645120 model_training_utils.py:450] Train Step: 17606/21936  / loss = 1.1269893646240234\n",
            "I0420 22:05:15.504105 139904526645120 model_training_utils.py:450] Train Step: 17607/21936  / loss = 0.3649028241634369\n",
            "I0420 22:05:16.042479 139904526645120 model_training_utils.py:450] Train Step: 17608/21936  / loss = 0.8511521220207214\n",
            "I0420 22:05:16.579257 139904526645120 model_training_utils.py:450] Train Step: 17609/21936  / loss = 1.121664047241211\n",
            "I0420 22:05:17.117277 139904526645120 model_training_utils.py:450] Train Step: 17610/21936  / loss = 0.6688107252120972\n",
            "I0420 22:05:17.654681 139904526645120 model_training_utils.py:450] Train Step: 17611/21936  / loss = 0.3952571153640747\n",
            "I0420 22:05:18.191908 139904526645120 model_training_utils.py:450] Train Step: 17612/21936  / loss = 1.4051122665405273\n",
            "I0420 22:05:18.726646 139904526645120 model_training_utils.py:450] Train Step: 17613/21936  / loss = 1.09627366065979\n",
            "I0420 22:05:19.265382 139904526645120 model_training_utils.py:450] Train Step: 17614/21936  / loss = 1.085394263267517\n",
            "I0420 22:05:19.801900 139904526645120 model_training_utils.py:450] Train Step: 17615/21936  / loss = 0.6920688152313232\n",
            "I0420 22:05:20.340417 139904526645120 model_training_utils.py:450] Train Step: 17616/21936  / loss = 0.8775711059570312\n",
            "I0420 22:05:20.877146 139904526645120 model_training_utils.py:450] Train Step: 17617/21936  / loss = 1.3008344173431396\n",
            "I0420 22:05:21.415100 139904526645120 model_training_utils.py:450] Train Step: 17618/21936  / loss = 0.6461774110794067\n",
            "I0420 22:05:21.957429 139904526645120 keras_utils.py:122] TimeHistory: 26.88 seconds, 14.88 examples/second between steps 28537 and 28587\n",
            "I0420 22:05:21.960116 139904526645120 model_training_utils.py:450] Train Step: 17619/21936  / loss = 1.0952470302581787\n",
            "I0420 22:05:22.498839 139904526645120 model_training_utils.py:450] Train Step: 17620/21936  / loss = 1.1085155010223389\n",
            "I0420 22:05:23.038011 139904526645120 model_training_utils.py:450] Train Step: 17621/21936  / loss = 0.35603195428848267\n",
            "I0420 22:05:23.577767 139904526645120 model_training_utils.py:450] Train Step: 17622/21936  / loss = 1.454376459121704\n",
            "I0420 22:05:24.114708 139904526645120 model_training_utils.py:450] Train Step: 17623/21936  / loss = 0.5744360685348511\n",
            "I0420 22:05:24.655242 139904526645120 model_training_utils.py:450] Train Step: 17624/21936  / loss = 0.5682417750358582\n",
            "I0420 22:05:25.192439 139904526645120 model_training_utils.py:450] Train Step: 17625/21936  / loss = 0.7078452110290527\n",
            "I0420 22:05:25.728622 139904526645120 model_training_utils.py:450] Train Step: 17626/21936  / loss = 0.336874783039093\n",
            "I0420 22:05:26.264199 139904526645120 model_training_utils.py:450] Train Step: 17627/21936  / loss = 0.9233306050300598\n",
            "I0420 22:05:26.801537 139904526645120 model_training_utils.py:450] Train Step: 17628/21936  / loss = 1.0373716354370117\n",
            "I0420 22:05:27.335713 139904526645120 model_training_utils.py:450] Train Step: 17629/21936  / loss = 0.546984851360321\n",
            "I0420 22:05:27.878221 139904526645120 model_training_utils.py:450] Train Step: 17630/21936  / loss = 0.6020988821983337\n",
            "I0420 22:05:28.416991 139904526645120 model_training_utils.py:450] Train Step: 17631/21936  / loss = 0.6773766279220581\n",
            "I0420 22:05:28.956598 139904526645120 model_training_utils.py:450] Train Step: 17632/21936  / loss = 1.4121181964874268\n",
            "I0420 22:05:29.493643 139904526645120 model_training_utils.py:450] Train Step: 17633/21936  / loss = 0.5912723541259766\n",
            "I0420 22:05:30.034122 139904526645120 model_training_utils.py:450] Train Step: 17634/21936  / loss = 0.5555511116981506\n",
            "I0420 22:05:30.570333 139904526645120 model_training_utils.py:450] Train Step: 17635/21936  / loss = 1.2710657119750977\n",
            "I0420 22:05:31.108964 139904526645120 model_training_utils.py:450] Train Step: 17636/21936  / loss = 0.3497244715690613\n",
            "I0420 22:05:31.647911 139904526645120 model_training_utils.py:450] Train Step: 17637/21936  / loss = 1.0994207859039307\n",
            "I0420 22:05:32.187664 139904526645120 model_training_utils.py:450] Train Step: 17638/21936  / loss = 0.8332857489585876\n",
            "I0420 22:05:32.721802 139904526645120 model_training_utils.py:450] Train Step: 17639/21936  / loss = 0.7640697956085205\n",
            "I0420 22:05:33.258319 139904526645120 model_training_utils.py:450] Train Step: 17640/21936  / loss = 0.6598877310752869\n",
            "I0420 22:05:33.800262 139904526645120 model_training_utils.py:450] Train Step: 17641/21936  / loss = 0.2946358323097229\n",
            "I0420 22:05:34.342816 139904526645120 model_training_utils.py:450] Train Step: 17642/21936  / loss = 0.8945792317390442\n",
            "I0420 22:05:34.881700 139904526645120 model_training_utils.py:450] Train Step: 17643/21936  / loss = 0.8250571489334106\n",
            "I0420 22:05:35.419751 139904526645120 model_training_utils.py:450] Train Step: 17644/21936  / loss = 0.6119277477264404\n",
            "I0420 22:05:35.956481 139904526645120 model_training_utils.py:450] Train Step: 17645/21936  / loss = 0.28005990386009216\n",
            "I0420 22:05:36.494210 139904526645120 model_training_utils.py:450] Train Step: 17646/21936  / loss = 0.6813361644744873\n",
            "I0420 22:05:37.031504 139904526645120 model_training_utils.py:450] Train Step: 17647/21936  / loss = 0.9587199687957764\n",
            "I0420 22:05:37.572273 139904526645120 model_training_utils.py:450] Train Step: 17648/21936  / loss = 0.749178946018219\n",
            "I0420 22:05:38.111510 139904526645120 model_training_utils.py:450] Train Step: 17649/21936  / loss = 0.5133987665176392\n",
            "I0420 22:05:38.648224 139904526645120 model_training_utils.py:450] Train Step: 17650/21936  / loss = 1.628046989440918\n",
            "I0420 22:05:39.186906 139904526645120 model_training_utils.py:450] Train Step: 17651/21936  / loss = 1.6020625829696655\n",
            "I0420 22:05:39.724582 139904526645120 model_training_utils.py:450] Train Step: 17652/21936  / loss = 0.7115843892097473\n",
            "I0420 22:05:40.262253 139904526645120 model_training_utils.py:450] Train Step: 17653/21936  / loss = 0.9649626016616821\n",
            "I0420 22:05:40.800435 139904526645120 model_training_utils.py:450] Train Step: 17654/21936  / loss = 1.0852668285369873\n",
            "I0420 22:05:41.337925 139904526645120 model_training_utils.py:450] Train Step: 17655/21936  / loss = 1.8106062412261963\n",
            "I0420 22:05:41.875539 139904526645120 model_training_utils.py:450] Train Step: 17656/21936  / loss = 0.6996244192123413\n",
            "I0420 22:05:42.417509 139904526645120 model_training_utils.py:450] Train Step: 17657/21936  / loss = 1.4456167221069336\n",
            "I0420 22:05:42.960047 139904526645120 model_training_utils.py:450] Train Step: 17658/21936  / loss = 0.8907046318054199\n",
            "I0420 22:05:43.501289 139904526645120 model_training_utils.py:450] Train Step: 17659/21936  / loss = 0.6356297731399536\n",
            "I0420 22:05:44.041306 139904526645120 model_training_utils.py:450] Train Step: 17660/21936  / loss = 1.205798625946045\n",
            "I0420 22:05:44.581226 139904526645120 model_training_utils.py:450] Train Step: 17661/21936  / loss = 0.5475432872772217\n",
            "I0420 22:05:45.119539 139904526645120 model_training_utils.py:450] Train Step: 17662/21936  / loss = 1.3795216083526611\n",
            "I0420 22:05:45.656436 139904526645120 model_training_utils.py:450] Train Step: 17663/21936  / loss = 0.5201520323753357\n",
            "I0420 22:05:46.203587 139904526645120 model_training_utils.py:450] Train Step: 17664/21936  / loss = 1.1988763809204102\n",
            "I0420 22:05:46.739936 139904526645120 model_training_utils.py:450] Train Step: 17665/21936  / loss = 1.6187278032302856\n",
            "I0420 22:05:47.275685 139904526645120 model_training_utils.py:450] Train Step: 17666/21936  / loss = 0.9756423234939575\n",
            "I0420 22:05:47.815232 139904526645120 model_training_utils.py:450] Train Step: 17667/21936  / loss = 0.48997169733047485\n",
            "I0420 22:05:48.353219 139904526645120 model_training_utils.py:450] Train Step: 17668/21936  / loss = 0.9698898196220398\n",
            "I0420 22:05:48.890759 139904526645120 keras_utils.py:122] TimeHistory: 26.93 seconds, 14.85 examples/second between steps 28587 and 28637\n",
            "I0420 22:05:48.893357 139904526645120 model_training_utils.py:450] Train Step: 17669/21936  / loss = 0.2898736000061035\n",
            "I0420 22:05:49.429144 139904526645120 model_training_utils.py:450] Train Step: 17670/21936  / loss = 1.1235347986221313\n",
            "I0420 22:05:49.968886 139904526645120 model_training_utils.py:450] Train Step: 17671/21936  / loss = 1.1745567321777344\n",
            "I0420 22:05:50.506230 139904526645120 model_training_utils.py:450] Train Step: 17672/21936  / loss = 0.870132327079773\n",
            "I0420 22:05:51.046496 139904526645120 model_training_utils.py:450] Train Step: 17673/21936  / loss = 1.1917154788970947\n",
            "I0420 22:05:51.585715 139904526645120 model_training_utils.py:450] Train Step: 17674/21936  / loss = 0.8084067702293396\n",
            "I0420 22:05:52.125524 139904526645120 model_training_utils.py:450] Train Step: 17675/21936  / loss = 1.653823971748352\n",
            "I0420 22:05:52.663017 139904526645120 model_training_utils.py:450] Train Step: 17676/21936  / loss = 0.7500753402709961\n",
            "I0420 22:05:53.202480 139904526645120 model_training_utils.py:450] Train Step: 17677/21936  / loss = 0.8755161166191101\n",
            "I0420 22:05:53.740194 139904526645120 model_training_utils.py:450] Train Step: 17678/21936  / loss = 0.382956862449646\n",
            "I0420 22:05:54.282705 139904526645120 model_training_utils.py:450] Train Step: 17679/21936  / loss = 0.22520118951797485\n",
            "I0420 22:05:54.829307 139904526645120 model_training_utils.py:450] Train Step: 17680/21936  / loss = 1.0992319583892822\n",
            "I0420 22:05:55.366429 139904526645120 model_training_utils.py:450] Train Step: 17681/21936  / loss = 0.5290187001228333\n",
            "I0420 22:05:55.903666 139904526645120 model_training_utils.py:450] Train Step: 17682/21936  / loss = 0.9281011819839478\n",
            "I0420 22:05:56.442297 139904526645120 model_training_utils.py:450] Train Step: 17683/21936  / loss = 0.8324236869812012\n",
            "I0420 22:05:56.979928 139904526645120 model_training_utils.py:450] Train Step: 17684/21936  / loss = 1.6099936962127686\n",
            "I0420 22:05:57.521404 139904526645120 model_training_utils.py:450] Train Step: 17685/21936  / loss = 1.7003018856048584\n",
            "I0420 22:05:58.059650 139904526645120 model_training_utils.py:450] Train Step: 17686/21936  / loss = 0.774215817451477\n",
            "I0420 22:05:58.599008 139904526645120 model_training_utils.py:450] Train Step: 17687/21936  / loss = 0.7452221512794495\n",
            "I0420 22:05:59.136269 139904526645120 model_training_utils.py:450] Train Step: 17688/21936  / loss = 0.6870441436767578\n",
            "I0420 22:05:59.672916 139904526645120 model_training_utils.py:450] Train Step: 17689/21936  / loss = 1.9330871105194092\n",
            "I0420 22:06:00.209222 139904526645120 model_training_utils.py:450] Train Step: 17690/21936  / loss = 0.7531173825263977\n",
            "I0420 22:06:00.746178 139904526645120 model_training_utils.py:450] Train Step: 17691/21936  / loss = 0.42075151205062866\n",
            "I0420 22:06:01.284548 139904526645120 model_training_utils.py:450] Train Step: 17692/21936  / loss = 0.7422378659248352\n",
            "I0420 22:06:01.823930 139904526645120 model_training_utils.py:450] Train Step: 17693/21936  / loss = 0.9581674337387085\n",
            "I0420 22:06:02.366840 139904526645120 model_training_utils.py:450] Train Step: 17694/21936  / loss = 0.9964570999145508\n",
            "I0420 22:06:02.904300 139904526645120 model_training_utils.py:450] Train Step: 17695/21936  / loss = 0.9207176566123962\n",
            "I0420 22:06:03.441451 139904526645120 model_training_utils.py:450] Train Step: 17696/21936  / loss = 0.7613236904144287\n",
            "I0420 22:06:03.978258 139904526645120 model_training_utils.py:450] Train Step: 17697/21936  / loss = 0.786971390247345\n",
            "I0420 22:06:04.519391 139904526645120 model_training_utils.py:450] Train Step: 17698/21936  / loss = 0.7045415639877319\n",
            "I0420 22:06:05.054759 139904526645120 model_training_utils.py:450] Train Step: 17699/21936  / loss = 0.8312684297561646\n",
            "I0420 22:06:05.592492 139904526645120 model_training_utils.py:450] Train Step: 17700/21936  / loss = 0.3864993453025818\n",
            "I0420 22:06:06.131258 139904526645120 model_training_utils.py:450] Train Step: 17701/21936  / loss = 0.5677810907363892\n",
            "I0420 22:06:06.670207 139904526645120 model_training_utils.py:450] Train Step: 17702/21936  / loss = 0.5090749263763428\n",
            "I0420 22:06:07.206592 139904526645120 model_training_utils.py:450] Train Step: 17703/21936  / loss = 0.3565586507320404\n",
            "I0420 22:06:07.745802 139904526645120 model_training_utils.py:450] Train Step: 17704/21936  / loss = 1.1462687253952026\n",
            "I0420 22:06:08.286120 139904526645120 model_training_utils.py:450] Train Step: 17705/21936  / loss = 0.9857876300811768\n",
            "I0420 22:06:08.822527 139904526645120 model_training_utils.py:450] Train Step: 17706/21936  / loss = 0.43123722076416016\n",
            "I0420 22:06:09.360937 139904526645120 model_training_utils.py:450] Train Step: 17707/21936  / loss = 0.35582566261291504\n",
            "I0420 22:06:09.896219 139904526645120 model_training_utils.py:450] Train Step: 17708/21936  / loss = 0.23155584931373596\n",
            "I0420 22:06:10.441000 139904526645120 model_training_utils.py:450] Train Step: 17709/21936  / loss = 1.461719036102295\n",
            "I0420 22:06:10.980038 139904526645120 model_training_utils.py:450] Train Step: 17710/21936  / loss = 0.5005211234092712\n",
            "I0420 22:06:11.517782 139904526645120 model_training_utils.py:450] Train Step: 17711/21936  / loss = 0.2883305549621582\n",
            "I0420 22:06:12.054187 139904526645120 model_training_utils.py:450] Train Step: 17712/21936  / loss = 0.30204594135284424\n",
            "I0420 22:06:12.591318 139904526645120 model_training_utils.py:450] Train Step: 17713/21936  / loss = 0.30491745471954346\n",
            "I0420 22:06:13.128932 139904526645120 model_training_utils.py:450] Train Step: 17714/21936  / loss = 0.43242332339286804\n",
            "I0420 22:06:13.667492 139904526645120 model_training_utils.py:450] Train Step: 17715/21936  / loss = 0.4696504473686218\n",
            "I0420 22:06:14.203484 139904526645120 model_training_utils.py:450] Train Step: 17716/21936  / loss = 0.2678944170475006\n",
            "I0420 22:06:14.739864 139904526645120 model_training_utils.py:450] Train Step: 17717/21936  / loss = 0.8153053522109985\n",
            "I0420 22:06:15.281083 139904526645120 model_training_utils.py:450] Train Step: 17718/21936  / loss = 0.5251438617706299\n",
            "I0420 22:06:15.820153 139904526645120 keras_utils.py:122] TimeHistory: 26.93 seconds, 14.86 examples/second between steps 28637 and 28687\n",
            "I0420 22:06:15.822778 139904526645120 model_training_utils.py:450] Train Step: 17719/21936  / loss = 0.3448428511619568\n",
            "I0420 22:06:16.359298 139904526645120 model_training_utils.py:450] Train Step: 17720/21936  / loss = 0.4705285429954529\n",
            "I0420 22:06:16.897788 139904526645120 model_training_utils.py:450] Train Step: 17721/21936  / loss = 0.38187482953071594\n",
            "I0420 22:06:17.437685 139904526645120 model_training_utils.py:450] Train Step: 17722/21936  / loss = 0.7522027492523193\n",
            "I0420 22:06:17.974873 139904526645120 model_training_utils.py:450] Train Step: 17723/21936  / loss = 0.40201956033706665\n",
            "I0420 22:06:18.512818 139904526645120 model_training_utils.py:450] Train Step: 17724/21936  / loss = 0.29607635736465454\n",
            "I0420 22:06:19.051657 139904526645120 model_training_utils.py:450] Train Step: 17725/21936  / loss = 0.5519464612007141\n",
            "I0420 22:06:19.589897 139904526645120 model_training_utils.py:450] Train Step: 17726/21936  / loss = 0.48223617672920227\n",
            "I0420 22:06:20.128314 139904526645120 model_training_utils.py:450] Train Step: 17727/21936  / loss = 0.2831856906414032\n",
            "I0420 22:06:20.667226 139904526645120 model_training_utils.py:450] Train Step: 17728/21936  / loss = 0.33942273259162903\n",
            "I0420 22:06:21.204979 139904526645120 model_training_utils.py:450] Train Step: 17729/21936  / loss = 0.19604745507240295\n",
            "I0420 22:06:21.743096 139904526645120 model_training_utils.py:450] Train Step: 17730/21936  / loss = 0.3338488042354584\n",
            "I0420 22:06:22.278007 139904526645120 model_training_utils.py:450] Train Step: 17731/21936  / loss = 1.3100712299346924\n",
            "I0420 22:06:22.817611 139904526645120 model_training_utils.py:450] Train Step: 17732/21936  / loss = 0.3124140501022339\n",
            "I0420 22:06:23.363506 139904526645120 model_training_utils.py:450] Train Step: 17733/21936  / loss = 0.36294102668762207\n",
            "I0420 22:06:23.899786 139904526645120 model_training_utils.py:450] Train Step: 17734/21936  / loss = 0.40454649925231934\n",
            "I0420 22:06:24.442608 139904526645120 model_training_utils.py:450] Train Step: 17735/21936  / loss = 0.3326961398124695\n",
            "I0420 22:06:24.983661 139904526645120 model_training_utils.py:450] Train Step: 17736/21936  / loss = 0.43972447514533997\n",
            "I0420 22:06:25.523224 139904526645120 model_training_utils.py:450] Train Step: 17737/21936  / loss = 0.23783616721630096\n",
            "I0420 22:06:26.060921 139904526645120 model_training_utils.py:450] Train Step: 17738/21936  / loss = 0.23190748691558838\n",
            "I0420 22:06:26.600377 139904526645120 model_training_utils.py:450] Train Step: 17739/21936  / loss = 0.4359787106513977\n",
            "I0420 22:06:27.139478 139904526645120 model_training_utils.py:450] Train Step: 17740/21936  / loss = 0.24866175651550293\n",
            "I0420 22:06:27.676196 139904526645120 model_training_utils.py:450] Train Step: 17741/21936  / loss = 1.4871751070022583\n",
            "I0420 22:06:28.214629 139904526645120 model_training_utils.py:450] Train Step: 17742/21936  / loss = 0.20410719513893127\n",
            "I0420 22:06:28.752652 139904526645120 model_training_utils.py:450] Train Step: 17743/21936  / loss = 0.43058061599731445\n",
            "I0420 22:06:29.292832 139904526645120 model_training_utils.py:450] Train Step: 17744/21936  / loss = 0.5492926836013794\n",
            "I0420 22:06:29.831011 139904526645120 model_training_utils.py:450] Train Step: 17745/21936  / loss = 0.16126763820648193\n",
            "I0420 22:06:30.368811 139904526645120 model_training_utils.py:450] Train Step: 17746/21936  / loss = 0.44501638412475586\n",
            "I0420 22:06:30.906535 139904526645120 model_training_utils.py:450] Train Step: 17747/21936  / loss = 0.13325123488903046\n",
            "I0420 22:06:31.443341 139904526645120 model_training_utils.py:450] Train Step: 17748/21936  / loss = 0.29076048731803894\n",
            "I0420 22:06:31.985332 139904526645120 model_training_utils.py:450] Train Step: 17749/21936  / loss = 0.12702676653862\n",
            "I0420 22:06:32.524349 139904526645120 model_training_utils.py:450] Train Step: 17750/21936  / loss = 0.04958244040608406\n",
            "I0420 22:06:33.063775 139904526645120 model_training_utils.py:450] Train Step: 17751/21936  / loss = 0.5918077230453491\n",
            "I0420 22:06:33.602207 139904526645120 model_training_utils.py:450] Train Step: 17752/21936  / loss = 0.9614982604980469\n",
            "I0420 22:06:34.143076 139904526645120 model_training_utils.py:450] Train Step: 17753/21936  / loss = 0.25980210304260254\n",
            "I0420 22:06:34.679996 139904526645120 model_training_utils.py:450] Train Step: 17754/21936  / loss = 0.5716724395751953\n",
            "I0420 22:06:35.225358 139904526645120 model_training_utils.py:450] Train Step: 17755/21936  / loss = 0.5980494022369385\n",
            "I0420 22:06:35.761300 139904526645120 model_training_utils.py:450] Train Step: 17756/21936  / loss = 0.2071956992149353\n",
            "I0420 22:06:36.310311 139904526645120 model_training_utils.py:450] Train Step: 17757/21936  / loss = 0.04876413941383362\n",
            "I0420 22:06:36.850320 139904526645120 model_training_utils.py:450] Train Step: 17758/21936  / loss = 0.48321032524108887\n",
            "I0420 22:06:37.390630 139904526645120 model_training_utils.py:450] Train Step: 17759/21936  / loss = 0.6212817430496216\n",
            "I0420 22:06:37.935909 139904526645120 model_training_utils.py:450] Train Step: 17760/21936  / loss = 0.4627090394496918\n",
            "I0420 22:06:38.474174 139904526645120 model_training_utils.py:450] Train Step: 17761/21936  / loss = 0.5359797477722168\n",
            "I0420 22:06:39.014402 139904526645120 model_training_utils.py:450] Train Step: 17762/21936  / loss = 1.2020792961120605\n",
            "I0420 22:06:39.555471 139904526645120 model_training_utils.py:450] Train Step: 17763/21936  / loss = 0.22011762857437134\n",
            "I0420 22:06:40.092605 139904526645120 model_training_utils.py:450] Train Step: 17764/21936  / loss = 0.40065014362335205\n",
            "I0420 22:06:40.633245 139904526645120 model_training_utils.py:450] Train Step: 17765/21936  / loss = 0.3799363970756531\n",
            "I0420 22:06:41.169910 139904526645120 model_training_utils.py:450] Train Step: 17766/21936  / loss = 0.4726097285747528\n",
            "I0420 22:06:41.715796 139904526645120 model_training_utils.py:450] Train Step: 17767/21936  / loss = 0.18053004145622253\n",
            "I0420 22:06:42.259367 139904526645120 model_training_utils.py:450] Train Step: 17768/21936  / loss = 0.24633590877056122\n",
            "I0420 22:06:42.799521 139904526645120 keras_utils.py:122] TimeHistory: 26.98 seconds, 14.83 examples/second between steps 28687 and 28737\n",
            "I0420 22:06:42.802057 139904526645120 model_training_utils.py:450] Train Step: 17769/21936  / loss = 0.5470994710922241\n",
            "I0420 22:06:43.338020 139904526645120 model_training_utils.py:450] Train Step: 17770/21936  / loss = 0.4431535005569458\n",
            "I0420 22:06:43.874771 139904526645120 model_training_utils.py:450] Train Step: 17771/21936  / loss = 0.18974173069000244\n",
            "I0420 22:06:44.412382 139904526645120 model_training_utils.py:450] Train Step: 17772/21936  / loss = 0.27470242977142334\n",
            "I0420 22:06:44.949753 139904526645120 model_training_utils.py:450] Train Step: 17773/21936  / loss = 0.49451687932014465\n",
            "I0420 22:06:45.485589 139904526645120 model_training_utils.py:450] Train Step: 17774/21936  / loss = 0.7502663135528564\n",
            "I0420 22:06:46.024964 139904526645120 model_training_utils.py:450] Train Step: 17775/21936  / loss = 0.1797458380460739\n",
            "I0420 22:06:46.563287 139904526645120 model_training_utils.py:450] Train Step: 17776/21936  / loss = 0.9511845111846924\n",
            "I0420 22:06:47.100999 139904526645120 model_training_utils.py:450] Train Step: 17777/21936  / loss = 0.31078287959098816\n",
            "I0420 22:06:47.638849 139904526645120 model_training_utils.py:450] Train Step: 17778/21936  / loss = 0.3867510259151459\n",
            "I0420 22:06:48.177206 139904526645120 model_training_utils.py:450] Train Step: 17779/21936  / loss = 1.6542799472808838\n",
            "I0420 22:06:48.713942 139904526645120 model_training_utils.py:450] Train Step: 17780/21936  / loss = 0.27547532320022583\n",
            "I0420 22:06:49.252542 139904526645120 model_training_utils.py:450] Train Step: 17781/21936  / loss = 0.13788235187530518\n",
            "I0420 22:06:49.788508 139904526645120 model_training_utils.py:450] Train Step: 17782/21936  / loss = 0.19925227761268616\n",
            "I0420 22:06:50.332872 139904526645120 model_training_utils.py:450] Train Step: 17783/21936  / loss = 1.3608026504516602\n",
            "I0420 22:06:50.871207 139904526645120 model_training_utils.py:450] Train Step: 17784/21936  / loss = 0.6132528185844421\n",
            "I0420 22:06:51.413030 139904526645120 model_training_utils.py:450] Train Step: 17785/21936  / loss = 0.56112140417099\n",
            "I0420 22:06:51.963160 139904526645120 model_training_utils.py:450] Train Step: 17786/21936  / loss = 1.0104137659072876\n",
            "I0420 22:06:52.503556 139904526645120 model_training_utils.py:450] Train Step: 17787/21936  / loss = 1.2952725887298584\n",
            "I0420 22:06:53.040708 139904526645120 model_training_utils.py:450] Train Step: 17788/21936  / loss = 0.1547815352678299\n",
            "I0420 22:06:53.579550 139904526645120 model_training_utils.py:450] Train Step: 17789/21936  / loss = 0.21491418778896332\n",
            "I0420 22:06:54.128404 139904526645120 model_training_utils.py:450] Train Step: 17790/21936  / loss = 0.6234415769577026\n",
            "I0420 22:06:54.668771 139904526645120 model_training_utils.py:450] Train Step: 17791/21936  / loss = 0.5719677209854126\n",
            "I0420 22:06:55.208237 139904526645120 model_training_utils.py:450] Train Step: 17792/21936  / loss = 0.592240035533905\n",
            "I0420 22:06:55.747020 139904526645120 model_training_utils.py:450] Train Step: 17793/21936  / loss = 0.5299505591392517\n",
            "I0420 22:06:56.285883 139904526645120 model_training_utils.py:450] Train Step: 17794/21936  / loss = 0.5268054008483887\n",
            "I0420 22:06:56.824500 139904526645120 model_training_utils.py:450] Train Step: 17795/21936  / loss = 0.8553137183189392\n",
            "I0420 22:06:57.363045 139904526645120 model_training_utils.py:450] Train Step: 17796/21936  / loss = 0.5858238935470581\n",
            "I0420 22:06:57.906337 139904526645120 model_training_utils.py:450] Train Step: 17797/21936  / loss = 0.4629408121109009\n",
            "I0420 22:06:58.446717 139904526645120 model_training_utils.py:450] Train Step: 17798/21936  / loss = 1.2386400699615479\n",
            "I0420 22:06:58.985509 139904526645120 model_training_utils.py:450] Train Step: 17799/21936  / loss = 0.7132855653762817\n",
            "I0420 22:06:59.525638 139904526645120 model_training_utils.py:450] Train Step: 17800/21936  / loss = 0.6139674186706543\n",
            "I0420 22:07:00.063336 139904526645120 model_training_utils.py:450] Train Step: 17801/21936  / loss = 1.0304160118103027\n",
            "I0420 22:07:00.604124 139904526645120 model_training_utils.py:450] Train Step: 17802/21936  / loss = 0.310237854719162\n",
            "I0420 22:07:01.142555 139904526645120 model_training_utils.py:450] Train Step: 17803/21936  / loss = 0.5644620060920715\n",
            "I0420 22:07:01.681113 139904526645120 model_training_utils.py:450] Train Step: 17804/21936  / loss = 0.1486319601535797\n",
            "I0420 22:07:02.225036 139904526645120 model_training_utils.py:450] Train Step: 17805/21936  / loss = 0.9184212684631348\n",
            "I0420 22:07:02.763362 139904526645120 model_training_utils.py:450] Train Step: 17806/21936  / loss = 0.224911630153656\n",
            "I0420 22:07:03.304027 139904526645120 model_training_utils.py:450] Train Step: 17807/21936  / loss = 1.1780182123184204\n",
            "I0420 22:07:03.847703 139904526645120 model_training_utils.py:450] Train Step: 17808/21936  / loss = 0.17880986630916595\n",
            "I0420 22:07:04.386283 139904526645120 model_training_utils.py:450] Train Step: 17809/21936  / loss = 0.5026429295539856\n",
            "I0420 22:07:04.926681 139904526645120 model_training_utils.py:450] Train Step: 17810/21936  / loss = 1.0217479467391968\n",
            "I0420 22:07:05.466631 139904526645120 model_training_utils.py:450] Train Step: 17811/21936  / loss = 1.1976118087768555\n",
            "I0420 22:07:06.006502 139904526645120 model_training_utils.py:450] Train Step: 17812/21936  / loss = 0.40076056122779846\n",
            "I0420 22:07:06.543060 139904526645120 model_training_utils.py:450] Train Step: 17813/21936  / loss = 0.30877619981765747\n",
            "I0420 22:07:07.080627 139904526645120 model_training_utils.py:450] Train Step: 17814/21936  / loss = 0.2682843804359436\n",
            "I0420 22:07:07.618203 139904526645120 model_training_utils.py:450] Train Step: 17815/21936  / loss = 0.6268429756164551\n",
            "I0420 22:07:08.156978 139904526645120 model_training_utils.py:450] Train Step: 17816/21936  / loss = 0.5316012501716614\n",
            "I0420 22:07:08.695518 139904526645120 model_training_utils.py:450] Train Step: 17817/21936  / loss = 0.11740916222333908\n",
            "I0420 22:07:09.233909 139904526645120 model_training_utils.py:450] Train Step: 17818/21936  / loss = 1.1294755935668945\n",
            "I0420 22:07:09.770075 139904526645120 keras_utils.py:122] TimeHistory: 26.97 seconds, 14.83 examples/second between steps 28737 and 28787\n",
            "I0420 22:07:09.772695 139904526645120 model_training_utils.py:450] Train Step: 17819/21936  / loss = 0.48885172605514526\n",
            "I0420 22:07:10.319143 139904526645120 model_training_utils.py:450] Train Step: 17820/21936  / loss = 0.3683321475982666\n",
            "I0420 22:07:10.858286 139904526645120 model_training_utils.py:450] Train Step: 17821/21936  / loss = 0.38201314210891724\n",
            "I0420 22:07:11.397463 139904526645120 model_training_utils.py:450] Train Step: 17822/21936  / loss = 0.27899688482284546\n",
            "I0420 22:07:11.936843 139904526645120 model_training_utils.py:450] Train Step: 17823/21936  / loss = 0.07804518193006516\n",
            "I0420 22:07:12.475859 139904526645120 model_training_utils.py:450] Train Step: 17824/21936  / loss = 1.1378642320632935\n",
            "I0420 22:07:13.013649 139904526645120 model_training_utils.py:450] Train Step: 17825/21936  / loss = 0.14925400912761688\n",
            "I0420 22:07:13.548934 139904526645120 model_training_utils.py:450] Train Step: 17826/21936  / loss = 0.43772122263908386\n",
            "I0420 22:07:14.086688 139904526645120 model_training_utils.py:450] Train Step: 17827/21936  / loss = 0.40637534856796265\n",
            "I0420 22:07:14.628929 139904526645120 model_training_utils.py:450] Train Step: 17828/21936  / loss = 0.37050002813339233\n",
            "I0420 22:07:15.171328 139904526645120 model_training_utils.py:450] Train Step: 17829/21936  / loss = 0.35372424125671387\n",
            "I0420 22:07:15.709246 139904526645120 model_training_utils.py:450] Train Step: 17830/21936  / loss = 0.10606712102890015\n",
            "I0420 22:07:16.249510 139904526645120 model_training_utils.py:450] Train Step: 17831/21936  / loss = 0.08130253106355667\n",
            "I0420 22:07:16.793449 139904526645120 model_training_utils.py:450] Train Step: 17832/21936  / loss = 0.3478567898273468\n",
            "I0420 22:07:17.333827 139904526645120 model_training_utils.py:450] Train Step: 17833/21936  / loss = 0.5471937656402588\n",
            "I0420 22:07:17.873203 139904526645120 model_training_utils.py:450] Train Step: 17834/21936  / loss = 0.2657310366630554\n",
            "I0420 22:07:18.411485 139904526645120 model_training_utils.py:450] Train Step: 17835/21936  / loss = 0.3727796673774719\n",
            "I0420 22:07:18.951692 139904526645120 model_training_utils.py:450] Train Step: 17836/21936  / loss = 0.13828575611114502\n",
            "I0420 22:07:19.490804 139904526645120 model_training_utils.py:450] Train Step: 17837/21936  / loss = 0.5245046615600586\n",
            "I0420 22:07:20.027251 139904526645120 model_training_utils.py:450] Train Step: 17838/21936  / loss = 0.17674443125724792\n",
            "I0420 22:07:20.566257 139904526645120 model_training_utils.py:450] Train Step: 17839/21936  / loss = 0.5664964914321899\n",
            "I0420 22:07:21.103507 139904526645120 model_training_utils.py:450] Train Step: 17840/21936  / loss = 0.12042798101902008\n",
            "I0420 22:07:21.648819 139904526645120 model_training_utils.py:450] Train Step: 17841/21936  / loss = 0.2898879945278168\n",
            "I0420 22:07:22.186727 139904526645120 model_training_utils.py:450] Train Step: 17842/21936  / loss = 0.48069316148757935\n",
            "I0420 22:07:22.724042 139904526645120 model_training_utils.py:450] Train Step: 17843/21936  / loss = 0.11089721322059631\n",
            "I0420 22:07:23.260781 139904526645120 model_training_utils.py:450] Train Step: 17844/21936  / loss = 0.13426807522773743\n",
            "I0420 22:07:23.798538 139904526645120 model_training_utils.py:450] Train Step: 17845/21936  / loss = 0.10878990590572357\n",
            "I0420 22:07:24.334435 139904526645120 model_training_utils.py:450] Train Step: 17846/21936  / loss = 0.1515214741230011\n",
            "I0420 22:07:24.874045 139904526645120 model_training_utils.py:450] Train Step: 17847/21936  / loss = 0.1525467336177826\n",
            "I0420 22:07:25.415434 139904526645120 model_training_utils.py:450] Train Step: 17848/21936  / loss = 1.0221682786941528\n",
            "I0420 22:07:25.954122 139904526645120 model_training_utils.py:450] Train Step: 17849/21936  / loss = 0.520871102809906\n",
            "I0420 22:07:26.489678 139904526645120 model_training_utils.py:450] Train Step: 17850/21936  / loss = 0.1889851838350296\n",
            "I0420 22:07:27.028700 139904526645120 model_training_utils.py:450] Train Step: 17851/21936  / loss = 0.6607491374015808\n",
            "I0420 22:07:27.566485 139904526645120 model_training_utils.py:450] Train Step: 17852/21936  / loss = 0.1630631685256958\n",
            "I0420 22:07:28.103724 139904526645120 model_training_utils.py:450] Train Step: 17853/21936  / loss = 0.3709895610809326\n",
            "I0420 22:07:28.642471 139904526645120 model_training_utils.py:450] Train Step: 17854/21936  / loss = 0.2754548192024231\n",
            "I0420 22:07:29.180995 139904526645120 model_training_utils.py:450] Train Step: 17855/21936  / loss = 0.2344876229763031\n",
            "I0420 22:07:29.720670 139904526645120 model_training_utils.py:450] Train Step: 17856/21936  / loss = 0.41157668828964233\n",
            "I0420 22:07:30.258658 139904526645120 model_training_utils.py:450] Train Step: 17857/21936  / loss = 0.7870566844940186\n",
            "I0420 22:07:30.798265 139904526645120 model_training_utils.py:450] Train Step: 17858/21936  / loss = 0.0646662712097168\n",
            "I0420 22:07:31.335092 139904526645120 model_training_utils.py:450] Train Step: 17859/21936  / loss = 0.21091096103191376\n",
            "I0420 22:07:31.872923 139904526645120 model_training_utils.py:450] Train Step: 17860/21936  / loss = 0.13613265752792358\n",
            "I0420 22:07:32.410143 139904526645120 model_training_utils.py:450] Train Step: 17861/21936  / loss = 0.13242392241954803\n",
            "I0420 22:07:32.949757 139904526645120 model_training_utils.py:450] Train Step: 17862/21936  / loss = 1.2872414588928223\n",
            "I0420 22:07:33.491462 139904526645120 model_training_utils.py:450] Train Step: 17863/21936  / loss = 0.5825865864753723\n",
            "I0420 22:07:34.029944 139904526645120 model_training_utils.py:450] Train Step: 17864/21936  / loss = 1.2985652685165405\n",
            "I0420 22:07:34.570240 139904526645120 model_training_utils.py:450] Train Step: 17865/21936  / loss = 1.718759536743164\n",
            "I0420 22:07:35.108630 139904526645120 model_training_utils.py:450] Train Step: 17866/21936  / loss = 0.5294259190559387\n",
            "I0420 22:07:35.655912 139904526645120 model_training_utils.py:450] Train Step: 17867/21936  / loss = 1.3860946893692017\n",
            "I0420 22:07:36.196998 139904526645120 model_training_utils.py:450] Train Step: 17868/21936  / loss = 1.1677411794662476\n",
            "I0420 22:07:36.735736 139904526645120 keras_utils.py:122] TimeHistory: 26.96 seconds, 14.84 examples/second between steps 28787 and 28837\n",
            "I0420 22:07:36.738650 139904526645120 model_training_utils.py:450] Train Step: 17869/21936  / loss = 0.30466240644454956\n",
            "I0420 22:07:37.283119 139904526645120 model_training_utils.py:450] Train Step: 17870/21936  / loss = 2.451979160308838\n",
            "I0420 22:07:37.834650 139904526645120 model_training_utils.py:450] Train Step: 17871/21936  / loss = 0.8719986081123352\n",
            "I0420 22:07:38.375613 139904526645120 model_training_utils.py:450] Train Step: 17872/21936  / loss = 2.7717809677124023\n",
            "I0420 22:07:38.913763 139904526645120 model_training_utils.py:450] Train Step: 17873/21936  / loss = 1.9875882863998413\n",
            "I0420 22:07:39.454848 139904526645120 model_training_utils.py:450] Train Step: 17874/21936  / loss = 1.9052720069885254\n",
            "I0420 22:07:39.994350 139904526645120 model_training_utils.py:450] Train Step: 17875/21936  / loss = 1.110827922821045\n",
            "I0420 22:07:40.539654 139904526645120 model_training_utils.py:450] Train Step: 17876/21936  / loss = 1.8061898946762085\n",
            "I0420 22:07:41.077413 139904526645120 model_training_utils.py:450] Train Step: 17877/21936  / loss = 1.1756470203399658\n",
            "I0420 22:07:41.624205 139904526645120 model_training_utils.py:450] Train Step: 17878/21936  / loss = 1.6247787475585938\n",
            "I0420 22:07:42.163616 139904526645120 model_training_utils.py:450] Train Step: 17879/21936  / loss = 3.1574277877807617\n",
            "I0420 22:07:42.703884 139904526645120 model_training_utils.py:450] Train Step: 17880/21936  / loss = 1.7031035423278809\n",
            "I0420 22:07:43.243087 139904526645120 model_training_utils.py:450] Train Step: 17881/21936  / loss = 1.240431785583496\n",
            "I0420 22:07:43.781088 139904526645120 model_training_utils.py:450] Train Step: 17882/21936  / loss = 1.1565616130828857\n",
            "I0420 22:07:44.324481 139904526645120 model_training_utils.py:450] Train Step: 17883/21936  / loss = 2.251117467880249\n",
            "I0420 22:07:44.866890 139904526645120 model_training_utils.py:450] Train Step: 17884/21936  / loss = 0.38532814383506775\n",
            "I0420 22:07:45.404367 139904526645120 model_training_utils.py:450] Train Step: 17885/21936  / loss = 0.9340105652809143\n",
            "I0420 22:07:45.946100 139904526645120 model_training_utils.py:450] Train Step: 17886/21936  / loss = 2.233975410461426\n",
            "I0420 22:07:46.483624 139904526645120 model_training_utils.py:450] Train Step: 17887/21936  / loss = 0.6975018978118896\n",
            "I0420 22:07:47.024931 139904526645120 model_training_utils.py:450] Train Step: 17888/21936  / loss = 1.2003482580184937\n",
            "I0420 22:07:47.563337 139904526645120 model_training_utils.py:450] Train Step: 17889/21936  / loss = 0.6918637752532959\n",
            "I0420 22:07:48.102425 139904526645120 model_training_utils.py:450] Train Step: 17890/21936  / loss = 0.6612184047698975\n",
            "I0420 22:07:48.640958 139904526645120 model_training_utils.py:450] Train Step: 17891/21936  / loss = 1.1420533657073975\n",
            "I0420 22:07:49.185008 139904526645120 model_training_utils.py:450] Train Step: 17892/21936  / loss = 0.38208311796188354\n",
            "I0420 22:07:49.725311 139904526645120 model_training_utils.py:450] Train Step: 17893/21936  / loss = 0.27567940950393677\n",
            "I0420 22:07:50.263514 139904526645120 model_training_utils.py:450] Train Step: 17894/21936  / loss = 0.8453001976013184\n",
            "I0420 22:07:50.806431 139904526645120 model_training_utils.py:450] Train Step: 17895/21936  / loss = 0.6957339644432068\n",
            "I0420 22:07:51.346060 139904526645120 model_training_utils.py:450] Train Step: 17896/21936  / loss = 0.5014333724975586\n",
            "I0420 22:07:51.886175 139904526645120 model_training_utils.py:450] Train Step: 17897/21936  / loss = 0.3601214289665222\n",
            "I0420 22:07:52.428181 139904526645120 model_training_utils.py:450] Train Step: 17898/21936  / loss = 0.5772910714149475\n",
            "I0420 22:07:52.967537 139904526645120 model_training_utils.py:450] Train Step: 17899/21936  / loss = 0.49582797288894653\n",
            "I0420 22:07:53.503703 139904526645120 model_training_utils.py:450] Train Step: 17900/21936  / loss = 0.4714996814727783\n",
            "I0420 22:07:54.044955 139904526645120 model_training_utils.py:450] Train Step: 17901/21936  / loss = 0.6033655405044556\n",
            "I0420 22:07:54.585162 139904526645120 model_training_utils.py:450] Train Step: 17902/21936  / loss = 1.2306126356124878\n",
            "I0420 22:07:55.122212 139904526645120 model_training_utils.py:450] Train Step: 17903/21936  / loss = 0.8614900708198547\n",
            "I0420 22:07:55.664777 139904526645120 model_training_utils.py:450] Train Step: 17904/21936  / loss = 0.6842929124832153\n",
            "I0420 22:07:56.205239 139904526645120 model_training_utils.py:450] Train Step: 17905/21936  / loss = 0.6474507451057434\n",
            "I0420 22:07:56.748169 139904526645120 model_training_utils.py:450] Train Step: 17906/21936  / loss = 1.2029485702514648\n",
            "I0420 22:07:57.287623 139904526645120 model_training_utils.py:450] Train Step: 17907/21936  / loss = 0.4474760890007019\n",
            "I0420 22:07:57.825160 139904526645120 model_training_utils.py:450] Train Step: 17908/21936  / loss = 0.17658284306526184\n",
            "I0420 22:07:58.374855 139904526645120 model_training_utils.py:450] Train Step: 17909/21936  / loss = 0.6956460475921631\n",
            "I0420 22:07:58.914161 139904526645120 model_training_utils.py:450] Train Step: 17910/21936  / loss = 0.5371116995811462\n",
            "I0420 22:07:59.456902 139904526645120 model_training_utils.py:450] Train Step: 17911/21936  / loss = 0.708854079246521\n",
            "I0420 22:07:59.996132 139904526645120 model_training_utils.py:450] Train Step: 17912/21936  / loss = 0.14370304346084595\n",
            "I0420 22:08:00.537006 139904526645120 model_training_utils.py:450] Train Step: 17913/21936  / loss = 0.2813360095024109\n",
            "I0420 22:08:01.077042 139904526645120 model_training_utils.py:450] Train Step: 17914/21936  / loss = 1.0437602996826172\n",
            "I0420 22:08:01.625786 139904526645120 model_training_utils.py:450] Train Step: 17915/21936  / loss = 0.8811816573143005\n",
            "I0420 22:08:02.163196 139904526645120 model_training_utils.py:450] Train Step: 17916/21936  / loss = 0.9746593236923218\n",
            "I0420 22:08:02.702384 139904526645120 model_training_utils.py:450] Train Step: 17917/21936  / loss = 0.7678948640823364\n",
            "I0420 22:08:03.239832 139904526645120 model_training_utils.py:450] Train Step: 17918/21936  / loss = 0.5953827500343323\n",
            "I0420 22:08:03.783783 139904526645120 keras_utils.py:122] TimeHistory: 27.04 seconds, 14.79 examples/second between steps 28837 and 28887\n",
            "I0420 22:08:03.786815 139904526645120 model_training_utils.py:450] Train Step: 17919/21936  / loss = 0.7340918183326721\n",
            "I0420 22:08:04.334280 139904526645120 model_training_utils.py:450] Train Step: 17920/21936  / loss = 0.36320269107818604\n",
            "I0420 22:08:04.871861 139904526645120 model_training_utils.py:450] Train Step: 17921/21936  / loss = 0.4802820682525635\n",
            "I0420 22:08:05.410692 139904526645120 model_training_utils.py:450] Train Step: 17922/21936  / loss = 0.531252384185791\n",
            "I0420 22:08:05.950142 139904526645120 model_training_utils.py:450] Train Step: 17923/21936  / loss = 0.5914875864982605\n",
            "I0420 22:08:06.491338 139904526645120 model_training_utils.py:450] Train Step: 17924/21936  / loss = 0.3801443576812744\n",
            "I0420 22:08:07.030180 139904526645120 model_training_utils.py:450] Train Step: 17925/21936  / loss = 0.6158315539360046\n",
            "I0420 22:08:07.569893 139904526645120 model_training_utils.py:450] Train Step: 17926/21936  / loss = 1.5264027118682861\n",
            "I0420 22:08:08.107974 139904526645120 model_training_utils.py:450] Train Step: 17927/21936  / loss = 0.4546184837818146\n",
            "I0420 22:08:08.646072 139904526645120 model_training_utils.py:450] Train Step: 17928/21936  / loss = 1.8283394575119019\n",
            "I0420 22:08:09.185458 139904526645120 model_training_utils.py:450] Train Step: 17929/21936  / loss = 0.595363974571228\n",
            "I0420 22:08:09.726778 139904526645120 model_training_utils.py:450] Train Step: 17930/21936  / loss = 0.447376549243927\n",
            "I0420 22:08:10.265376 139904526645120 model_training_utils.py:450] Train Step: 17931/21936  / loss = 1.2080063819885254\n",
            "I0420 22:08:10.802640 139904526645120 model_training_utils.py:450] Train Step: 17932/21936  / loss = 0.5338876247406006\n",
            "I0420 22:08:11.341817 139904526645120 model_training_utils.py:450] Train Step: 17933/21936  / loss = 0.9801852107048035\n",
            "I0420 22:08:11.885002 139904526645120 model_training_utils.py:450] Train Step: 17934/21936  / loss = 0.5084898471832275\n",
            "I0420 22:08:12.424633 139904526645120 model_training_utils.py:450] Train Step: 17935/21936  / loss = 1.734976887702942\n",
            "I0420 22:08:12.963632 139904526645120 model_training_utils.py:450] Train Step: 17936/21936  / loss = 0.5966677665710449\n",
            "I0420 22:08:13.503894 139904526645120 model_training_utils.py:450] Train Step: 17937/21936  / loss = 0.6407685279846191\n",
            "I0420 22:08:14.046149 139904526645120 model_training_utils.py:450] Train Step: 17938/21936  / loss = 0.7423494458198547\n",
            "I0420 22:08:14.588208 139904526645120 model_training_utils.py:450] Train Step: 17939/21936  / loss = 1.0447826385498047\n",
            "I0420 22:08:15.126170 139904526645120 model_training_utils.py:450] Train Step: 17940/21936  / loss = 0.7732833623886108\n",
            "I0420 22:08:15.662805 139904526645120 model_training_utils.py:450] Train Step: 17941/21936  / loss = 0.2988039553165436\n",
            "I0420 22:08:16.204628 139904526645120 model_training_utils.py:450] Train Step: 17942/21936  / loss = 0.6155027747154236\n",
            "I0420 22:08:16.746160 139904526645120 model_training_utils.py:450] Train Step: 17943/21936  / loss = 0.5097103118896484\n",
            "I0420 22:08:17.286650 139904526645120 model_training_utils.py:450] Train Step: 17944/21936  / loss = 1.1703994274139404\n",
            "I0420 22:08:17.827814 139904526645120 model_training_utils.py:450] Train Step: 17945/21936  / loss = 0.38176602125167847\n",
            "I0420 22:08:18.365731 139904526645120 model_training_utils.py:450] Train Step: 17946/21936  / loss = 0.18149617314338684\n",
            "I0420 22:08:18.906679 139904526645120 model_training_utils.py:450] Train Step: 17947/21936  / loss = 0.28603124618530273\n",
            "I0420 22:08:19.448741 139904526645120 model_training_utils.py:450] Train Step: 17948/21936  / loss = 0.32375338673591614\n",
            "I0420 22:08:19.988271 139904526645120 model_training_utils.py:450] Train Step: 17949/21936  / loss = 1.143241286277771\n",
            "I0420 22:08:20.529105 139904526645120 model_training_utils.py:450] Train Step: 17950/21936  / loss = 0.35066744685173035\n",
            "I0420 22:08:21.068035 139904526645120 model_training_utils.py:450] Train Step: 17951/21936  / loss = 0.4496561884880066\n",
            "I0420 22:08:21.607458 139904526645120 model_training_utils.py:450] Train Step: 17952/21936  / loss = 0.5617535710334778\n",
            "I0420 22:08:22.147109 139904526645120 model_training_utils.py:450] Train Step: 17953/21936  / loss = 0.3858685791492462\n",
            "I0420 22:08:22.688706 139904526645120 model_training_utils.py:450] Train Step: 17954/21936  / loss = 0.12670345604419708\n",
            "I0420 22:08:23.226402 139904526645120 model_training_utils.py:450] Train Step: 17955/21936  / loss = 0.28695860505104065\n",
            "I0420 22:08:23.769298 139904526645120 model_training_utils.py:450] Train Step: 17956/21936  / loss = 0.5556090474128723\n",
            "I0420 22:08:24.309149 139904526645120 model_training_utils.py:450] Train Step: 17957/21936  / loss = 0.5949017405509949\n",
            "I0420 22:08:24.854516 139904526645120 model_training_utils.py:450] Train Step: 17958/21936  / loss = 0.2782452404499054\n",
            "I0420 22:08:25.395043 139904526645120 model_training_utils.py:450] Train Step: 17959/21936  / loss = 0.5852973461151123\n",
            "I0420 22:08:25.934279 139904526645120 model_training_utils.py:450] Train Step: 17960/21936  / loss = 0.5826888680458069\n",
            "I0420 22:08:26.474147 139904526645120 model_training_utils.py:450] Train Step: 17961/21936  / loss = 0.2740420699119568\n",
            "I0420 22:08:27.012889 139904526645120 model_training_utils.py:450] Train Step: 17962/21936  / loss = 0.4907727539539337\n",
            "I0420 22:08:27.554349 139904526645120 model_training_utils.py:450] Train Step: 17963/21936  / loss = 0.2773635983467102\n",
            "I0420 22:08:28.093625 139904526645120 model_training_utils.py:450] Train Step: 17964/21936  / loss = 0.20212450623512268\n",
            "I0420 22:08:28.634613 139904526645120 model_training_utils.py:450] Train Step: 17965/21936  / loss = 0.19036266207695007\n",
            "I0420 22:08:29.173739 139904526645120 model_training_utils.py:450] Train Step: 17966/21936  / loss = 0.41227537393569946\n",
            "I0420 22:08:29.713560 139904526645120 model_training_utils.py:450] Train Step: 17967/21936  / loss = 0.11310799419879913\n",
            "I0420 22:08:30.258425 139904526645120 model_training_utils.py:450] Train Step: 17968/21936  / loss = 0.6608648300170898\n",
            "I0420 22:08:30.796291 139904526645120 keras_utils.py:122] TimeHistory: 27.01 seconds, 14.81 examples/second between steps 28887 and 28937\n",
            "I0420 22:08:30.799145 139904526645120 model_training_utils.py:450] Train Step: 17969/21936  / loss = 0.3070765733718872\n",
            "I0420 22:08:31.336328 139904526645120 model_training_utils.py:450] Train Step: 17970/21936  / loss = 0.06078464165329933\n",
            "I0420 22:08:31.875780 139904526645120 model_training_utils.py:450] Train Step: 17971/21936  / loss = 0.4721250534057617\n",
            "I0420 22:08:32.416373 139904526645120 model_training_utils.py:450] Train Step: 17972/21936  / loss = 0.11556403338909149\n",
            "I0420 22:08:32.954095 139904526645120 model_training_utils.py:450] Train Step: 17973/21936  / loss = 0.07302450388669968\n",
            "I0420 22:08:33.492236 139904526645120 model_training_utils.py:450] Train Step: 17974/21936  / loss = 0.18467740714550018\n",
            "I0420 22:08:34.031347 139904526645120 model_training_utils.py:450] Train Step: 17975/21936  / loss = 0.3592742085456848\n",
            "I0420 22:08:34.570320 139904526645120 model_training_utils.py:450] Train Step: 17976/21936  / loss = 0.10475939512252808\n",
            "I0420 22:08:35.108605 139904526645120 model_training_utils.py:450] Train Step: 17977/21936  / loss = 0.11541230976581573\n",
            "I0420 22:08:35.646104 139904526645120 model_training_utils.py:450] Train Step: 17978/21936  / loss = 0.30107128620147705\n",
            "I0420 22:08:36.185931 139904526645120 model_training_utils.py:450] Train Step: 17979/21936  / loss = 0.4129979610443115\n",
            "I0420 22:08:36.725863 139904526645120 model_training_utils.py:450] Train Step: 17980/21936  / loss = 0.06725657731294632\n",
            "I0420 22:08:37.262871 139904526645120 model_training_utils.py:450] Train Step: 17981/21936  / loss = 0.22108745574951172\n",
            "I0420 22:08:37.802120 139904526645120 model_training_utils.py:450] Train Step: 17982/21936  / loss = 0.262664794921875\n",
            "I0420 22:08:38.341528 139904526645120 model_training_utils.py:450] Train Step: 17983/21936  / loss = 0.2950522005558014\n",
            "I0420 22:08:38.881079 139904526645120 model_training_utils.py:450] Train Step: 17984/21936  / loss = 0.15844041109085083\n",
            "I0420 22:08:39.419219 139904526645120 model_training_utils.py:450] Train Step: 17985/21936  / loss = 0.31515946984291077\n",
            "I0420 22:08:39.957677 139904526645120 model_training_utils.py:450] Train Step: 17986/21936  / loss = 0.07572892308235168\n",
            "I0420 22:08:40.496892 139904526645120 model_training_utils.py:450] Train Step: 17987/21936  / loss = 0.1625400334596634\n",
            "I0420 22:08:41.034461 139904526645120 model_training_utils.py:450] Train Step: 17988/21936  / loss = 1.0289485454559326\n",
            "I0420 22:08:41.574320 139904526645120 model_training_utils.py:450] Train Step: 17989/21936  / loss = 0.25415316224098206\n",
            "I0420 22:08:42.113153 139904526645120 model_training_utils.py:450] Train Step: 17990/21936  / loss = 0.24306564033031464\n",
            "I0420 22:08:42.652517 139904526645120 model_training_utils.py:450] Train Step: 17991/21936  / loss = 0.770990252494812\n",
            "I0420 22:08:43.197306 139904526645120 model_training_utils.py:450] Train Step: 17992/21936  / loss = 0.10113981366157532\n",
            "I0420 22:08:43.736371 139904526645120 model_training_utils.py:450] Train Step: 17993/21936  / loss = 0.43918535113334656\n",
            "I0420 22:08:44.283801 139904526645120 model_training_utils.py:450] Train Step: 17994/21936  / loss = 0.4420567750930786\n",
            "I0420 22:08:44.826152 139904526645120 model_training_utils.py:450] Train Step: 17995/21936  / loss = 0.4831320643424988\n",
            "I0420 22:08:45.367399 139904526645120 model_training_utils.py:450] Train Step: 17996/21936  / loss = 0.2099599391222\n",
            "I0420 22:08:45.906170 139904526645120 model_training_utils.py:450] Train Step: 17997/21936  / loss = 0.8537859916687012\n",
            "I0420 22:08:46.443359 139904526645120 model_training_utils.py:450] Train Step: 17998/21936  / loss = 0.534332811832428\n",
            "I0420 22:08:46.981319 139904526645120 model_training_utils.py:450] Train Step: 17999/21936  / loss = 0.31597742438316345\n",
            "I0420 22:08:47.520165 139904526645120 model_training_utils.py:450] Train Step: 18000/21936  / loss = 0.8196835517883301\n",
            "I0420 22:08:48.056634 139904526645120 model_training_utils.py:450] Train Step: 18001/21936  / loss = 0.3608028292655945\n",
            "I0420 22:08:48.595688 139904526645120 model_training_utils.py:450] Train Step: 18002/21936  / loss = 0.3023951053619385\n",
            "I0420 22:08:49.133050 139904526645120 model_training_utils.py:450] Train Step: 18003/21936  / loss = 0.8956266641616821\n",
            "I0420 22:08:49.672340 139904526645120 model_training_utils.py:450] Train Step: 18004/21936  / loss = 0.9313337206840515\n",
            "I0420 22:08:50.212278 139904526645120 model_training_utils.py:450] Train Step: 18005/21936  / loss = 0.25897330045700073\n",
            "I0420 22:08:50.751066 139904526645120 model_training_utils.py:450] Train Step: 18006/21936  / loss = 0.9299740195274353\n",
            "I0420 22:08:51.288845 139904526645120 model_training_utils.py:450] Train Step: 18007/21936  / loss = 0.585565984249115\n",
            "I0420 22:08:51.832970 139904526645120 model_training_utils.py:450] Train Step: 18008/21936  / loss = 1.3700706958770752\n",
            "I0420 22:08:52.376290 139904526645120 model_training_utils.py:450] Train Step: 18009/21936  / loss = 0.3197845220565796\n",
            "I0420 22:08:52.915928 139904526645120 model_training_utils.py:450] Train Step: 18010/21936  / loss = 0.876288115978241\n",
            "I0420 22:08:53.454499 139904526645120 model_training_utils.py:450] Train Step: 18011/21936  / loss = 0.6025996208190918\n",
            "I0420 22:08:53.994239 139904526645120 model_training_utils.py:450] Train Step: 18012/21936  / loss = 0.9002150297164917\n",
            "I0420 22:08:54.534228 139904526645120 model_training_utils.py:450] Train Step: 18013/21936  / loss = 1.3789349794387817\n",
            "I0420 22:08:55.073362 139904526645120 model_training_utils.py:450] Train Step: 18014/21936  / loss = 0.5332866311073303\n",
            "I0420 22:08:55.611381 139904526645120 model_training_utils.py:450] Train Step: 18015/21936  / loss = 2.6510164737701416\n",
            "I0420 22:08:56.148947 139904526645120 model_training_utils.py:450] Train Step: 18016/21936  / loss = 1.2175477743148804\n",
            "I0420 22:08:56.685488 139904526645120 model_training_utils.py:450] Train Step: 18017/21936  / loss = 0.9913918375968933\n",
            "I0420 22:08:57.225312 139904526645120 model_training_utils.py:450] Train Step: 18018/21936  / loss = 1.044805645942688\n",
            "I0420 22:08:57.760986 139904526645120 keras_utils.py:122] TimeHistory: 26.96 seconds, 14.84 examples/second between steps 28937 and 28987\n",
            "I0420 22:08:57.764137 139904526645120 model_training_utils.py:450] Train Step: 18019/21936  / loss = 0.5541125535964966\n",
            "I0420 22:08:58.301555 139904526645120 model_training_utils.py:450] Train Step: 18020/21936  / loss = 0.6632764339447021\n",
            "I0420 22:08:58.839928 139904526645120 model_training_utils.py:450] Train Step: 18021/21936  / loss = 0.8544908761978149\n",
            "I0420 22:08:59.377327 139904526645120 model_training_utils.py:450] Train Step: 18022/21936  / loss = 0.4450497031211853\n",
            "I0420 22:08:59.915124 139904526645120 model_training_utils.py:450] Train Step: 18023/21936  / loss = 0.5330264568328857\n",
            "I0420 22:09:00.455796 139904526645120 model_training_utils.py:450] Train Step: 18024/21936  / loss = 1.0880203247070312\n",
            "I0420 22:09:00.995195 139904526645120 model_training_utils.py:450] Train Step: 18025/21936  / loss = 0.9634013772010803\n",
            "I0420 22:09:01.532909 139904526645120 model_training_utils.py:450] Train Step: 18026/21936  / loss = 0.3953719437122345\n",
            "I0420 22:09:02.072189 139904526645120 model_training_utils.py:450] Train Step: 18027/21936  / loss = 0.49221959710121155\n",
            "I0420 22:09:02.610224 139904526645120 model_training_utils.py:450] Train Step: 18028/21936  / loss = 1.1994524002075195\n",
            "I0420 22:09:03.157066 139904526645120 model_training_utils.py:450] Train Step: 18029/21936  / loss = 0.7025931477546692\n",
            "I0420 22:09:03.697046 139904526645120 model_training_utils.py:450] Train Step: 18030/21936  / loss = 0.2371232956647873\n",
            "I0420 22:09:04.236785 139904526645120 model_training_utils.py:450] Train Step: 18031/21936  / loss = 0.2291596531867981\n",
            "I0420 22:09:04.778798 139904526645120 model_training_utils.py:450] Train Step: 18032/21936  / loss = 0.49529343843460083\n",
            "I0420 22:09:05.316064 139904526645120 model_training_utils.py:450] Train Step: 18033/21936  / loss = 0.612349808216095\n",
            "I0420 22:09:05.853684 139904526645120 model_training_utils.py:450] Train Step: 18034/21936  / loss = 0.26643043756484985\n",
            "I0420 22:09:06.391970 139904526645120 model_training_utils.py:450] Train Step: 18035/21936  / loss = 0.22455792129039764\n",
            "I0420 22:09:06.930209 139904526645120 model_training_utils.py:450] Train Step: 18036/21936  / loss = 0.3004969358444214\n",
            "I0420 22:09:07.466235 139904526645120 model_training_utils.py:450] Train Step: 18037/21936  / loss = 0.3709082305431366\n",
            "I0420 22:09:08.004597 139904526645120 model_training_utils.py:450] Train Step: 18038/21936  / loss = 0.054101794958114624\n",
            "I0420 22:09:08.544407 139904526645120 model_training_utils.py:450] Train Step: 18039/21936  / loss = 0.7772161960601807\n",
            "I0420 22:09:09.084332 139904526645120 model_training_utils.py:450] Train Step: 18040/21936  / loss = 1.3006982803344727\n",
            "I0420 22:09:09.622714 139904526645120 model_training_utils.py:450] Train Step: 18041/21936  / loss = 0.7769653797149658\n",
            "I0420 22:09:10.159477 139904526645120 model_training_utils.py:450] Train Step: 18042/21936  / loss = 0.9896788597106934\n",
            "I0420 22:09:10.699128 139904526645120 model_training_utils.py:450] Train Step: 18043/21936  / loss = 0.4025034010410309\n",
            "I0420 22:09:11.237663 139904526645120 model_training_utils.py:450] Train Step: 18044/21936  / loss = 0.6881216764450073\n",
            "I0420 22:09:11.776002 139904526645120 model_training_utils.py:450] Train Step: 18045/21936  / loss = 0.17429062724113464\n",
            "I0420 22:09:12.314193 139904526645120 model_training_utils.py:450] Train Step: 18046/21936  / loss = 0.2975541949272156\n",
            "I0420 22:09:12.850625 139904526645120 model_training_utils.py:450] Train Step: 18047/21936  / loss = 0.2224900722503662\n",
            "I0420 22:09:13.389191 139904526645120 model_training_utils.py:450] Train Step: 18048/21936  / loss = 0.3268345296382904\n",
            "I0420 22:09:13.927424 139904526645120 model_training_utils.py:450] Train Step: 18049/21936  / loss = 0.13423335552215576\n",
            "I0420 22:09:14.465302 139904526645120 model_training_utils.py:450] Train Step: 18050/21936  / loss = 0.4118364751338959\n",
            "I0420 22:09:15.002711 139904526645120 model_training_utils.py:450] Train Step: 18051/21936  / loss = 0.3125762939453125\n",
            "I0420 22:09:15.540545 139904526645120 model_training_utils.py:450] Train Step: 18052/21936  / loss = 0.29808473587036133\n",
            "I0420 22:09:16.077882 139904526645120 model_training_utils.py:450] Train Step: 18053/21936  / loss = 0.6752617359161377\n",
            "I0420 22:09:16.614446 139904526645120 model_training_utils.py:450] Train Step: 18054/21936  / loss = 0.8456453084945679\n",
            "I0420 22:09:17.152759 139904526645120 model_training_utils.py:450] Train Step: 18055/21936  / loss = 0.42749834060668945\n",
            "I0420 22:09:17.691821 139904526645120 model_training_utils.py:450] Train Step: 18056/21936  / loss = 0.732630729675293\n",
            "I0420 22:09:18.228512 139904526645120 model_training_utils.py:450] Train Step: 18057/21936  / loss = 0.677415132522583\n",
            "I0420 22:09:18.763777 139904526645120 model_training_utils.py:450] Train Step: 18058/21936  / loss = 0.16489969193935394\n",
            "I0420 22:09:19.301994 139904526645120 model_training_utils.py:450] Train Step: 18059/21936  / loss = 0.40104052424430847\n",
            "I0420 22:09:19.837917 139904526645120 model_training_utils.py:450] Train Step: 18060/21936  / loss = 0.30738669633865356\n",
            "I0420 22:09:20.382698 139904526645120 model_training_utils.py:450] Train Step: 18061/21936  / loss = 1.3276584148406982\n",
            "I0420 22:09:20.919890 139904526645120 model_training_utils.py:450] Train Step: 18062/21936  / loss = 0.8107336759567261\n",
            "I0420 22:09:21.458296 139904526645120 model_training_utils.py:450] Train Step: 18063/21936  / loss = 1.406836748123169\n",
            "I0420 22:09:21.996392 139904526645120 model_training_utils.py:450] Train Step: 18064/21936  / loss = 1.2728538513183594\n",
            "I0420 22:09:22.537138 139904526645120 model_training_utils.py:450] Train Step: 18065/21936  / loss = 0.6655158996582031\n",
            "I0420 22:09:23.076793 139904526645120 model_training_utils.py:450] Train Step: 18066/21936  / loss = 0.793168842792511\n",
            "I0420 22:09:23.612649 139904526645120 model_training_utils.py:450] Train Step: 18067/21936  / loss = 1.8183839321136475\n",
            "I0420 22:09:24.151082 139904526645120 model_training_utils.py:450] Train Step: 18068/21936  / loss = 0.7817933559417725\n",
            "I0420 22:09:24.688057 139904526645120 keras_utils.py:122] TimeHistory: 26.92 seconds, 14.86 examples/second between steps 28987 and 29037\n",
            "I0420 22:09:24.690798 139904526645120 model_training_utils.py:450] Train Step: 18069/21936  / loss = 1.4925323724746704\n",
            "I0420 22:09:25.230847 139904526645120 model_training_utils.py:450] Train Step: 18070/21936  / loss = 1.116913080215454\n",
            "I0420 22:09:25.769659 139904526645120 model_training_utils.py:450] Train Step: 18071/21936  / loss = 1.1194770336151123\n",
            "I0420 22:09:26.307134 139904526645120 model_training_utils.py:450] Train Step: 18072/21936  / loss = 0.6758770942687988\n",
            "I0420 22:09:26.846461 139904526645120 model_training_utils.py:450] Train Step: 18073/21936  / loss = 0.3577115535736084\n",
            "I0420 22:09:27.384138 139904526645120 model_training_utils.py:450] Train Step: 18074/21936  / loss = 1.0610675811767578\n",
            "I0420 22:09:27.922251 139904526645120 model_training_utils.py:450] Train Step: 18075/21936  / loss = 0.3199947476387024\n",
            "I0420 22:09:28.459242 139904526645120 model_training_utils.py:450] Train Step: 18076/21936  / loss = 0.24503156542778015\n",
            "I0420 22:09:28.995774 139904526645120 model_training_utils.py:450] Train Step: 18077/21936  / loss = 0.3963084816932678\n",
            "I0420 22:09:29.534435 139904526645120 model_training_utils.py:450] Train Step: 18078/21936  / loss = 0.6385818123817444\n",
            "I0420 22:09:30.082816 139904526645120 model_training_utils.py:450] Train Step: 18079/21936  / loss = 0.6640950441360474\n",
            "I0420 22:09:30.621001 139904526645120 model_training_utils.py:450] Train Step: 18080/21936  / loss = 1.0693092346191406\n",
            "I0420 22:09:31.164456 139904526645120 model_training_utils.py:450] Train Step: 18081/21936  / loss = 0.4620797634124756\n",
            "I0420 22:09:31.702590 139904526645120 model_training_utils.py:450] Train Step: 18082/21936  / loss = 0.4572523236274719\n",
            "I0420 22:09:32.239476 139904526645120 model_training_utils.py:450] Train Step: 18083/21936  / loss = 0.3902812600135803\n",
            "I0420 22:09:32.776791 139904526645120 model_training_utils.py:450] Train Step: 18084/21936  / loss = 0.4052659571170807\n",
            "I0420 22:09:33.316005 139904526645120 model_training_utils.py:450] Train Step: 18085/21936  / loss = 0.6677688360214233\n",
            "I0420 22:09:33.859179 139904526645120 model_training_utils.py:450] Train Step: 18086/21936  / loss = 0.40710926055908203\n",
            "I0420 22:09:34.401509 139904526645120 model_training_utils.py:450] Train Step: 18087/21936  / loss = 0.5045921206474304\n",
            "I0420 22:09:34.938919 139904526645120 model_training_utils.py:450] Train Step: 18088/21936  / loss = 0.8343086242675781\n",
            "I0420 22:09:35.475607 139904526645120 model_training_utils.py:450] Train Step: 18089/21936  / loss = 0.2294606864452362\n",
            "I0420 22:09:36.012691 139904526645120 model_training_utils.py:450] Train Step: 18090/21936  / loss = 0.24499118328094482\n",
            "I0420 22:09:36.554107 139904526645120 model_training_utils.py:450] Train Step: 18091/21936  / loss = 0.0764954537153244\n",
            "I0420 22:09:37.094242 139904526645120 model_training_utils.py:450] Train Step: 18092/21936  / loss = 0.7372930645942688\n",
            "I0420 22:09:37.631905 139904526645120 model_training_utils.py:450] Train Step: 18093/21936  / loss = 0.4184555411338806\n",
            "I0420 22:09:38.172137 139904526645120 model_training_utils.py:450] Train Step: 18094/21936  / loss = 0.34879395365715027\n",
            "I0420 22:09:38.711629 139904526645120 model_training_utils.py:450] Train Step: 18095/21936  / loss = 0.27516087889671326\n",
            "I0420 22:09:39.252184 139904526645120 model_training_utils.py:450] Train Step: 18096/21936  / loss = 0.4034983515739441\n",
            "I0420 22:09:39.788983 139904526645120 model_training_utils.py:450] Train Step: 18097/21936  / loss = 1.006408452987671\n",
            "I0420 22:09:40.328278 139904526645120 model_training_utils.py:450] Train Step: 18098/21936  / loss = 0.5131019949913025\n",
            "I0420 22:09:40.867405 139904526645120 model_training_utils.py:450] Train Step: 18099/21936  / loss = 0.42023998498916626\n",
            "I0420 22:09:41.405364 139904526645120 model_training_utils.py:450] Train Step: 18100/21936  / loss = 0.6204568147659302\n",
            "I0420 22:09:41.943967 139904526645120 model_training_utils.py:450] Train Step: 18101/21936  / loss = 0.19254028797149658\n",
            "I0420 22:09:42.482133 139904526645120 model_training_utils.py:450] Train Step: 18102/21936  / loss = 0.09692385792732239\n",
            "I0420 22:09:43.019969 139904526645120 model_training_utils.py:450] Train Step: 18103/21936  / loss = 0.33123189210891724\n",
            "I0420 22:09:43.558650 139904526645120 model_training_utils.py:450] Train Step: 18104/21936  / loss = 0.3826417326927185\n",
            "I0420 22:09:44.096411 139904526645120 model_training_utils.py:450] Train Step: 18105/21936  / loss = 0.32125699520111084\n",
            "I0420 22:09:44.646362 139904526645120 model_training_utils.py:450] Train Step: 18106/21936  / loss = 0.6704221367835999\n",
            "I0420 22:09:45.184193 139904526645120 model_training_utils.py:450] Train Step: 18107/21936  / loss = 0.7083771228790283\n",
            "I0420 22:09:45.721424 139904526645120 model_training_utils.py:450] Train Step: 18108/21936  / loss = 1.457700490951538\n",
            "I0420 22:09:46.258150 139904526645120 model_training_utils.py:450] Train Step: 18109/21936  / loss = 0.5794100761413574\n",
            "I0420 22:09:46.798322 139904526645120 model_training_utils.py:450] Train Step: 18110/21936  / loss = 0.317546546459198\n",
            "I0420 22:09:47.339281 139904526645120 model_training_utils.py:450] Train Step: 18111/21936  / loss = 0.8523402214050293\n",
            "I0420 22:09:47.884681 139904526645120 model_training_utils.py:450] Train Step: 18112/21936  / loss = 0.579553484916687\n",
            "I0420 22:09:48.424701 139904526645120 model_training_utils.py:450] Train Step: 18113/21936  / loss = 0.7659685611724854\n",
            "I0420 22:09:48.964358 139904526645120 model_training_utils.py:450] Train Step: 18114/21936  / loss = 0.43880999088287354\n",
            "I0420 22:09:49.505162 139904526645120 model_training_utils.py:450] Train Step: 18115/21936  / loss = 0.5765544176101685\n",
            "I0420 22:09:50.041999 139904526645120 model_training_utils.py:450] Train Step: 18116/21936  / loss = 0.28914082050323486\n",
            "I0420 22:09:50.580945 139904526645120 model_training_utils.py:450] Train Step: 18117/21936  / loss = 1.000317096710205\n",
            "I0420 22:09:51.123001 139904526645120 model_training_utils.py:450] Train Step: 18118/21936  / loss = 0.5791465044021606\n",
            "I0420 22:09:51.661914 139904526645120 keras_utils.py:122] TimeHistory: 26.97 seconds, 14.83 examples/second between steps 29037 and 29087\n",
            "I0420 22:09:51.664629 139904526645120 model_training_utils.py:450] Train Step: 18119/21936  / loss = 0.8077296614646912\n",
            "I0420 22:09:52.204423 139904526645120 model_training_utils.py:450] Train Step: 18120/21936  / loss = 0.5064356923103333\n",
            "I0420 22:09:52.744185 139904526645120 model_training_utils.py:450] Train Step: 18121/21936  / loss = 1.0607171058654785\n",
            "I0420 22:09:53.280600 139904526645120 model_training_utils.py:450] Train Step: 18122/21936  / loss = 0.8717982172966003\n",
            "I0420 22:09:53.819380 139904526645120 model_training_utils.py:450] Train Step: 18123/21936  / loss = 0.4101502001285553\n",
            "I0420 22:09:54.362010 139904526645120 model_training_utils.py:450] Train Step: 18124/21936  / loss = 1.6687304973602295\n",
            "I0420 22:09:54.900684 139904526645120 model_training_utils.py:450] Train Step: 18125/21936  / loss = 1.1277092695236206\n",
            "I0420 22:09:55.437145 139904526645120 model_training_utils.py:450] Train Step: 18126/21936  / loss = 1.0111911296844482\n",
            "I0420 22:09:55.975659 139904526645120 model_training_utils.py:450] Train Step: 18127/21936  / loss = 1.830667495727539\n",
            "I0420 22:09:56.515899 139904526645120 model_training_utils.py:450] Train Step: 18128/21936  / loss = 2.347526788711548\n",
            "I0420 22:09:57.054784 139904526645120 model_training_utils.py:450] Train Step: 18129/21936  / loss = 1.8377139568328857\n",
            "I0420 22:09:57.592810 139904526645120 model_training_utils.py:450] Train Step: 18130/21936  / loss = 1.6998023986816406\n",
            "I0420 22:09:58.133175 139904526645120 model_training_utils.py:450] Train Step: 18131/21936  / loss = 1.0601565837860107\n",
            "I0420 22:09:58.672395 139904526645120 model_training_utils.py:450] Train Step: 18132/21936  / loss = 1.5368520021438599\n",
            "I0420 22:09:59.215116 139904526645120 model_training_utils.py:450] Train Step: 18133/21936  / loss = 1.6276869773864746\n",
            "I0420 22:09:59.753281 139904526645120 model_training_utils.py:450] Train Step: 18134/21936  / loss = 0.3069654703140259\n",
            "I0420 22:10:00.291093 139904526645120 model_training_utils.py:450] Train Step: 18135/21936  / loss = 1.698065996170044\n",
            "I0420 22:10:00.827050 139904526645120 model_training_utils.py:450] Train Step: 18136/21936  / loss = 0.970952033996582\n",
            "I0420 22:10:01.372583 139904526645120 model_training_utils.py:450] Train Step: 18137/21936  / loss = 1.3983759880065918\n",
            "I0420 22:10:01.910180 139904526645120 model_training_utils.py:450] Train Step: 18138/21936  / loss = 0.2076353281736374\n",
            "I0420 22:10:02.446714 139904526645120 model_training_utils.py:450] Train Step: 18139/21936  / loss = 0.7756439447402954\n",
            "I0420 22:10:02.983273 139904526645120 model_training_utils.py:450] Train Step: 18140/21936  / loss = 0.9179286956787109\n",
            "I0420 22:10:03.521047 139904526645120 model_training_utils.py:450] Train Step: 18141/21936  / loss = 0.9528729915618896\n",
            "I0420 22:10:04.061673 139904526645120 model_training_utils.py:450] Train Step: 18142/21936  / loss = 1.3684031963348389\n",
            "I0420 22:10:04.601983 139904526645120 model_training_utils.py:450] Train Step: 18143/21936  / loss = 0.7013075351715088\n",
            "I0420 22:10:05.140373 139904526645120 model_training_utils.py:450] Train Step: 18144/21936  / loss = 0.8558903336524963\n",
            "I0420 22:10:05.680779 139904526645120 model_training_utils.py:450] Train Step: 18145/21936  / loss = 0.4205783009529114\n",
            "I0420 22:10:06.220326 139904526645120 model_training_utils.py:450] Train Step: 18146/21936  / loss = 1.2306113243103027\n",
            "I0420 22:10:06.760543 139904526645120 model_training_utils.py:450] Train Step: 18147/21936  / loss = 1.593377947807312\n",
            "I0420 22:10:07.299489 139904526645120 model_training_utils.py:450] Train Step: 18148/21936  / loss = 1.2871254682540894\n",
            "I0420 22:10:07.839787 139904526645120 model_training_utils.py:450] Train Step: 18149/21936  / loss = 1.2610011100769043\n",
            "I0420 22:10:08.379277 139904526645120 model_training_utils.py:450] Train Step: 18150/21936  / loss = 0.8861260414123535\n",
            "I0420 22:10:08.918337 139904526645120 model_training_utils.py:450] Train Step: 18151/21936  / loss = 1.3412559032440186\n",
            "I0420 22:10:09.456324 139904526645120 model_training_utils.py:450] Train Step: 18152/21936  / loss = 0.8517365455627441\n",
            "I0420 22:10:09.996270 139904526645120 model_training_utils.py:450] Train Step: 18153/21936  / loss = 0.3035777807235718\n",
            "I0420 22:10:10.535457 139904526645120 model_training_utils.py:450] Train Step: 18154/21936  / loss = 0.10499687492847443\n",
            "I0420 22:10:11.072667 139904526645120 model_training_utils.py:450] Train Step: 18155/21936  / loss = 0.5677416920661926\n",
            "I0420 22:10:11.611377 139904526645120 model_training_utils.py:450] Train Step: 18156/21936  / loss = 0.6990893483161926\n",
            "I0420 22:10:12.159036 139904526645120 model_training_utils.py:450] Train Step: 18157/21936  / loss = 1.6436084508895874\n",
            "I0420 22:10:12.698082 139904526645120 model_training_utils.py:450] Train Step: 18158/21936  / loss = 1.0097742080688477\n",
            "I0420 22:10:13.239190 139904526645120 model_training_utils.py:450] Train Step: 18159/21936  / loss = 1.0312151908874512\n",
            "I0420 22:10:13.780447 139904526645120 model_training_utils.py:450] Train Step: 18160/21936  / loss = 0.4811539053916931\n",
            "I0420 22:10:14.323138 139904526645120 model_training_utils.py:450] Train Step: 18161/21936  / loss = 1.344914197921753\n",
            "I0420 22:10:14.860066 139904526645120 model_training_utils.py:450] Train Step: 18162/21936  / loss = 1.095630407333374\n",
            "I0420 22:10:15.396751 139904526645120 model_training_utils.py:450] Train Step: 18163/21936  / loss = 0.6005730628967285\n",
            "I0420 22:10:15.944061 139904526645120 model_training_utils.py:450] Train Step: 18164/21936  / loss = 1.0023956298828125\n",
            "I0420 22:10:16.482420 139904526645120 model_training_utils.py:450] Train Step: 18165/21936  / loss = 0.18687325716018677\n",
            "I0420 22:10:17.018407 139904526645120 model_training_utils.py:450] Train Step: 18166/21936  / loss = 1.622153401374817\n",
            "I0420 22:10:17.555015 139904526645120 model_training_utils.py:450] Train Step: 18167/21936  / loss = 1.3023865222930908\n",
            "I0420 22:10:18.093423 139904526645120 model_training_utils.py:450] Train Step: 18168/21936  / loss = 1.5467181205749512\n",
            "I0420 22:10:18.629707 139904526645120 keras_utils.py:122] TimeHistory: 26.96 seconds, 14.83 examples/second between steps 29087 and 29137\n",
            "I0420 22:10:18.632445 139904526645120 model_training_utils.py:450] Train Step: 18169/21936  / loss = 1.8493540287017822\n",
            "I0420 22:10:19.173013 139904526645120 model_training_utils.py:450] Train Step: 18170/21936  / loss = 1.348930835723877\n",
            "I0420 22:10:19.708665 139904526645120 model_training_utils.py:450] Train Step: 18171/21936  / loss = 0.444327175617218\n",
            "I0420 22:10:20.250896 139904526645120 model_training_utils.py:450] Train Step: 18172/21936  / loss = 0.7873850464820862\n",
            "I0420 22:10:20.788911 139904526645120 model_training_utils.py:450] Train Step: 18173/21936  / loss = 0.9661048054695129\n",
            "I0420 22:10:21.326330 139904526645120 model_training_utils.py:450] Train Step: 18174/21936  / loss = 1.4697790145874023\n",
            "I0420 22:10:21.867854 139904526645120 model_training_utils.py:450] Train Step: 18175/21936  / loss = 1.3601787090301514\n",
            "I0420 22:10:22.407146 139904526645120 model_training_utils.py:450] Train Step: 18176/21936  / loss = 1.2846462726593018\n",
            "I0420 22:10:22.944993 139904526645120 model_training_utils.py:450] Train Step: 18177/21936  / loss = 1.3093416690826416\n",
            "I0420 22:10:23.483600 139904526645120 model_training_utils.py:450] Train Step: 18178/21936  / loss = 1.413272500038147\n",
            "I0420 22:10:24.021286 139904526645120 model_training_utils.py:450] Train Step: 18179/21936  / loss = 0.8396819829940796\n",
            "I0420 22:10:24.562054 139904526645120 model_training_utils.py:450] Train Step: 18180/21936  / loss = 1.0745205879211426\n",
            "I0420 22:10:25.101501 139904526645120 model_training_utils.py:450] Train Step: 18181/21936  / loss = 0.886061429977417\n",
            "I0420 22:10:25.639672 139904526645120 model_training_utils.py:450] Train Step: 18182/21936  / loss = 1.0146269798278809\n",
            "I0420 22:10:26.176993 139904526645120 model_training_utils.py:450] Train Step: 18183/21936  / loss = 0.9763783812522888\n",
            "I0420 22:10:26.719601 139904526645120 model_training_utils.py:450] Train Step: 18184/21936  / loss = 0.7293047904968262\n",
            "I0420 22:10:27.259139 139904526645120 model_training_utils.py:450] Train Step: 18185/21936  / loss = 0.7621737122535706\n",
            "I0420 22:10:27.799235 139904526645120 model_training_utils.py:450] Train Step: 18186/21936  / loss = 0.3939211368560791\n",
            "I0420 22:10:28.337476 139904526645120 model_training_utils.py:450] Train Step: 18187/21936  / loss = 0.5500550270080566\n",
            "I0420 22:10:28.875764 139904526645120 model_training_utils.py:450] Train Step: 18188/21936  / loss = 0.36190956830978394\n",
            "I0420 22:10:29.415089 139904526645120 model_training_utils.py:450] Train Step: 18189/21936  / loss = 0.6678014993667603\n",
            "I0420 22:10:29.960996 139904526645120 model_training_utils.py:450] Train Step: 18190/21936  / loss = 0.995479941368103\n",
            "I0420 22:10:30.505472 139904526645120 model_training_utils.py:450] Train Step: 18191/21936  / loss = 0.18185551464557648\n",
            "I0420 22:10:31.043905 139904526645120 model_training_utils.py:450] Train Step: 18192/21936  / loss = 0.7524601817131042\n",
            "I0420 22:10:31.582662 139904526645120 model_training_utils.py:450] Train Step: 18193/21936  / loss = 0.8252238035202026\n",
            "I0420 22:10:32.119890 139904526645120 model_training_utils.py:450] Train Step: 18194/21936  / loss = 0.7598698139190674\n",
            "I0420 22:10:32.658716 139904526645120 model_training_utils.py:450] Train Step: 18195/21936  / loss = 0.6161953210830688\n",
            "I0420 22:10:33.196366 139904526645120 model_training_utils.py:450] Train Step: 18196/21936  / loss = 1.0626566410064697\n",
            "I0420 22:10:33.734620 139904526645120 model_training_utils.py:450] Train Step: 18197/21936  / loss = 0.10461829602718353\n",
            "I0420 22:10:34.274099 139904526645120 model_training_utils.py:450] Train Step: 18198/21936  / loss = 0.9571672081947327\n",
            "I0420 22:10:34.818451 139904526645120 model_training_utils.py:450] Train Step: 18199/21936  / loss = 0.2677646279335022\n",
            "I0420 22:10:35.357551 139904526645120 model_training_utils.py:450] Train Step: 18200/21936  / loss = 0.14942540228366852\n",
            "I0420 22:10:35.896515 139904526645120 model_training_utils.py:450] Train Step: 18201/21936  / loss = 0.4733934998512268\n",
            "I0420 22:10:36.435499 139904526645120 model_training_utils.py:450] Train Step: 18202/21936  / loss = 0.5149142146110535\n",
            "I0420 22:10:36.974036 139904526645120 model_training_utils.py:450] Train Step: 18203/21936  / loss = 1.131622076034546\n",
            "I0420 22:10:37.513228 139904526645120 model_training_utils.py:450] Train Step: 18204/21936  / loss = 0.4821856617927551\n",
            "I0420 22:10:38.054366 139904526645120 model_training_utils.py:450] Train Step: 18205/21936  / loss = 0.2247498631477356\n",
            "I0420 22:10:38.597285 139904526645120 model_training_utils.py:450] Train Step: 18206/21936  / loss = 0.1069597378373146\n",
            "I0420 22:10:39.145342 139904526645120 model_training_utils.py:450] Train Step: 18207/21936  / loss = 0.8365987539291382\n",
            "I0420 22:10:39.685943 139904526645120 model_training_utils.py:450] Train Step: 18208/21936  / loss = 1.0911794900894165\n",
            "I0420 22:10:40.225252 139904526645120 model_training_utils.py:450] Train Step: 18209/21936  / loss = 0.7028646469116211\n",
            "I0420 22:10:40.763041 139904526645120 model_training_utils.py:450] Train Step: 18210/21936  / loss = 0.26659131050109863\n",
            "I0420 22:10:41.299729 139904526645120 model_training_utils.py:450] Train Step: 18211/21936  / loss = 0.12163347005844116\n",
            "I0420 22:10:41.838586 139904526645120 model_training_utils.py:450] Train Step: 18212/21936  / loss = 0.13922813534736633\n",
            "I0420 22:10:42.377624 139904526645120 model_training_utils.py:450] Train Step: 18213/21936  / loss = 0.4968056082725525\n",
            "I0420 22:10:42.914759 139904526645120 model_training_utils.py:450] Train Step: 18214/21936  / loss = 0.625245213508606\n",
            "I0420 22:10:43.453111 139904526645120 model_training_utils.py:450] Train Step: 18215/21936  / loss = 0.38555359840393066\n",
            "I0420 22:10:43.988128 139904526645120 model_training_utils.py:450] Train Step: 18216/21936  / loss = 0.40488627552986145\n",
            "I0420 22:10:44.526463 139904526645120 model_training_utils.py:450] Train Step: 18217/21936  / loss = 0.7218896150588989\n",
            "I0420 22:10:45.074354 139904526645120 model_training_utils.py:450] Train Step: 18218/21936  / loss = 0.08286204934120178\n",
            "I0420 22:10:45.612352 139904526645120 keras_utils.py:122] TimeHistory: 26.98 seconds, 14.83 examples/second between steps 29137 and 29187\n",
            "I0420 22:10:45.615336 139904526645120 model_training_utils.py:450] Train Step: 18219/21936  / loss = 1.372306227684021\n",
            "I0420 22:10:46.156289 139904526645120 model_training_utils.py:450] Train Step: 18220/21936  / loss = 0.6264849305152893\n",
            "I0420 22:10:46.694029 139904526645120 model_training_utils.py:450] Train Step: 18221/21936  / loss = 0.5948097705841064\n",
            "I0420 22:10:47.232485 139904526645120 model_training_utils.py:450] Train Step: 18222/21936  / loss = 0.7912569046020508\n",
            "I0420 22:10:47.770693 139904526645120 model_training_utils.py:450] Train Step: 18223/21936  / loss = 0.8247246742248535\n",
            "I0420 22:10:48.312390 139904526645120 model_training_utils.py:450] Train Step: 18224/21936  / loss = 0.46727535128593445\n",
            "I0420 22:10:48.856857 139904526645120 model_training_utils.py:450] Train Step: 18225/21936  / loss = 0.40133166313171387\n",
            "I0420 22:10:49.399550 139904526645120 model_training_utils.py:450] Train Step: 18226/21936  / loss = 0.2530503273010254\n",
            "I0420 22:10:49.938026 139904526645120 model_training_utils.py:450] Train Step: 18227/21936  / loss = 1.262697458267212\n",
            "I0420 22:10:50.475941 139904526645120 model_training_utils.py:450] Train Step: 18228/21936  / loss = 0.18084385991096497\n",
            "I0420 22:10:51.013547 139904526645120 model_training_utils.py:450] Train Step: 18229/21936  / loss = 0.3596630096435547\n",
            "I0420 22:10:51.551477 139904526645120 model_training_utils.py:450] Train Step: 18230/21936  / loss = 0.06772029399871826\n",
            "I0420 22:10:52.090346 139904526645120 model_training_utils.py:450] Train Step: 18231/21936  / loss = 0.27652478218078613\n",
            "I0420 22:10:52.629271 139904526645120 model_training_utils.py:450] Train Step: 18232/21936  / loss = 0.4566173553466797\n",
            "I0420 22:10:53.167142 139904526645120 model_training_utils.py:450] Train Step: 18233/21936  / loss = 1.2910724878311157\n",
            "I0420 22:10:53.705100 139904526645120 model_training_utils.py:450] Train Step: 18234/21936  / loss = 0.2844809889793396\n",
            "I0420 22:10:54.245426 139904526645120 model_training_utils.py:450] Train Step: 18235/21936  / loss = 0.41599270701408386\n",
            "I0420 22:10:54.784038 139904526645120 model_training_utils.py:450] Train Step: 18236/21936  / loss = 0.3118916451931\n",
            "I0420 22:10:55.327444 139904526645120 model_training_utils.py:450] Train Step: 18237/21936  / loss = 0.8653932213783264\n",
            "I0420 22:10:55.863684 139904526645120 model_training_utils.py:450] Train Step: 18238/21936  / loss = 0.4241260290145874\n",
            "I0420 22:10:56.401541 139904526645120 model_training_utils.py:450] Train Step: 18239/21936  / loss = 0.5666807889938354\n",
            "I0420 22:10:56.941301 139904526645120 model_training_utils.py:450] Train Step: 18240/21936  / loss = 0.7256867289543152\n",
            "I0420 22:10:57.484166 139904526645120 model_training_utils.py:450] Train Step: 18241/21936  / loss = 0.8617113828659058\n",
            "I0420 22:10:58.022280 139904526645120 model_training_utils.py:450] Train Step: 18242/21936  / loss = 0.1511010080575943\n",
            "I0420 22:10:58.560683 139904526645120 model_training_utils.py:450] Train Step: 18243/21936  / loss = 0.31974148750305176\n",
            "I0420 22:10:59.096536 139904526645120 model_training_utils.py:450] Train Step: 18244/21936  / loss = 0.35496097803115845\n",
            "I0420 22:10:59.633821 139904526645120 model_training_utils.py:450] Train Step: 18245/21936  / loss = 0.4577549993991852\n",
            "I0420 22:11:00.172758 139904526645120 model_training_utils.py:450] Train Step: 18246/21936  / loss = 0.19982397556304932\n",
            "I0420 22:11:00.708813 139904526645120 model_training_utils.py:450] Train Step: 18247/21936  / loss = 0.19537392258644104\n",
            "I0420 22:11:01.247250 139904526645120 model_training_utils.py:450] Train Step: 18248/21936  / loss = 0.5660879611968994\n",
            "I0420 22:11:01.782858 139904526645120 model_training_utils.py:450] Train Step: 18249/21936  / loss = 1.199237585067749\n",
            "I0420 22:11:02.320960 139904526645120 model_training_utils.py:450] Train Step: 18250/21936  / loss = 0.13015611469745636\n",
            "I0420 22:11:02.860332 139904526645120 model_training_utils.py:450] Train Step: 18251/21936  / loss = 1.186047077178955\n",
            "I0420 22:11:03.398850 139904526645120 model_training_utils.py:450] Train Step: 18252/21936  / loss = 0.2685590386390686\n",
            "I0420 22:11:03.934678 139904526645120 model_training_utils.py:450] Train Step: 18253/21936  / loss = 0.07146666198968887\n",
            "I0420 22:11:04.481535 139904526645120 model_training_utils.py:450] Train Step: 18254/21936  / loss = 0.5278688073158264\n",
            "I0420 22:11:05.019075 139904526645120 model_training_utils.py:450] Train Step: 18255/21936  / loss = 0.38471463322639465\n",
            "I0420 22:11:05.555951 139904526645120 model_training_utils.py:450] Train Step: 18256/21936  / loss = 0.12253664433956146\n",
            "I0420 22:11:06.095227 139904526645120 model_training_utils.py:450] Train Step: 18257/21936  / loss = 0.8526883125305176\n",
            "I0420 22:11:06.635189 139904526645120 model_training_utils.py:450] Train Step: 18258/21936  / loss = 0.5542516112327576\n",
            "I0420 22:11:07.173925 139904526645120 model_training_utils.py:450] Train Step: 18259/21936  / loss = 0.9418518543243408\n",
            "I0420 22:11:07.711206 139904526645120 model_training_utils.py:450] Train Step: 18260/21936  / loss = 0.6202620267868042\n",
            "I0420 22:11:08.247875 139904526645120 model_training_utils.py:450] Train Step: 18261/21936  / loss = 0.24649403989315033\n",
            "I0420 22:11:08.787320 139904526645120 model_training_utils.py:450] Train Step: 18262/21936  / loss = 0.12991677224636078\n",
            "I0420 22:11:09.325143 139904526645120 model_training_utils.py:450] Train Step: 18263/21936  / loss = 0.26733267307281494\n",
            "I0420 22:11:09.865013 139904526645120 model_training_utils.py:450] Train Step: 18264/21936  / loss = 0.7067519426345825\n",
            "I0420 22:11:10.404454 139904526645120 model_training_utils.py:450] Train Step: 18265/21936  / loss = 0.4715982675552368\n",
            "I0420 22:11:10.943974 139904526645120 model_training_utils.py:450] Train Step: 18266/21936  / loss = 0.3070531487464905\n",
            "I0420 22:11:11.489879 139904526645120 model_training_utils.py:450] Train Step: 18267/21936  / loss = 0.6382325887680054\n",
            "I0420 22:11:12.028409 139904526645120 model_training_utils.py:450] Train Step: 18268/21936  / loss = 0.22756865620613098\n",
            "I0420 22:11:12.568161 139904526645120 keras_utils.py:122] TimeHistory: 26.95 seconds, 14.84 examples/second between steps 29187 and 29237\n",
            "I0420 22:11:12.570985 139904526645120 model_training_utils.py:450] Train Step: 18269/21936  / loss = 0.2881194055080414\n",
            "I0420 22:11:13.110306 139904526645120 model_training_utils.py:450] Train Step: 18270/21936  / loss = 0.26179665327072144\n",
            "I0420 22:11:13.649186 139904526645120 model_training_utils.py:450] Train Step: 18271/21936  / loss = 0.44271600246429443\n",
            "I0420 22:11:14.189409 139904526645120 model_training_utils.py:450] Train Step: 18272/21936  / loss = 0.8866965174674988\n",
            "I0420 22:11:14.728230 139904526645120 model_training_utils.py:450] Train Step: 18273/21936  / loss = 0.4136837124824524\n",
            "I0420 22:11:15.271120 139904526645120 model_training_utils.py:450] Train Step: 18274/21936  / loss = 0.41121411323547363\n",
            "I0420 22:11:15.813728 139904526645120 model_training_utils.py:450] Train Step: 18275/21936  / loss = 0.5914382934570312\n",
            "I0420 22:11:16.353357 139904526645120 model_training_utils.py:450] Train Step: 18276/21936  / loss = 0.568688154220581\n",
            "I0420 22:11:16.892651 139904526645120 model_training_utils.py:450] Train Step: 18277/21936  / loss = 0.6296581625938416\n",
            "I0420 22:11:17.430589 139904526645120 model_training_utils.py:450] Train Step: 18278/21936  / loss = 0.5490500926971436\n",
            "I0420 22:11:17.970207 139904526645120 model_training_utils.py:450] Train Step: 18279/21936  / loss = 0.4734687805175781\n",
            "I0420 22:11:18.508119 139904526645120 model_training_utils.py:450] Train Step: 18280/21936  / loss = 0.7864577770233154\n",
            "I0420 22:11:19.046285 139904526645120 model_training_utils.py:450] Train Step: 18281/21936  / loss = 1.0604989528656006\n",
            "I0420 22:11:19.585695 139904526645120 model_training_utils.py:450] Train Step: 18282/21936  / loss = 0.1312163919210434\n",
            "I0420 22:11:20.123823 139904526645120 model_training_utils.py:450] Train Step: 18283/21936  / loss = 0.10099971294403076\n",
            "I0420 22:11:20.663662 139904526645120 model_training_utils.py:450] Train Step: 18284/21936  / loss = 0.30603915452957153\n",
            "I0420 22:11:21.212416 139904526645120 model_training_utils.py:450] Train Step: 18285/21936  / loss = 0.1900196671485901\n",
            "I0420 22:11:21.749061 139904526645120 model_training_utils.py:450] Train Step: 18286/21936  / loss = 0.37532544136047363\n",
            "I0420 22:11:22.288182 139904526645120 model_training_utils.py:450] Train Step: 18287/21936  / loss = 0.41647040843963623\n",
            "I0420 22:11:22.827335 139904526645120 model_training_utils.py:450] Train Step: 18288/21936  / loss = 0.5599687099456787\n",
            "I0420 22:11:23.369964 139904526645120 model_training_utils.py:450] Train Step: 18289/21936  / loss = 0.357055127620697\n",
            "I0420 22:11:23.907405 139904526645120 model_training_utils.py:450] Train Step: 18290/21936  / loss = 1.4118843078613281\n",
            "I0420 22:11:24.445459 139904526645120 model_training_utils.py:450] Train Step: 18291/21936  / loss = 0.4796069264411926\n",
            "I0420 22:11:24.982934 139904526645120 model_training_utils.py:450] Train Step: 18292/21936  / loss = 0.35147643089294434\n",
            "I0420 22:11:25.524457 139904526645120 model_training_utils.py:450] Train Step: 18293/21936  / loss = 0.23535631597042084\n",
            "I0420 22:11:26.064515 139904526645120 model_training_utils.py:450] Train Step: 18294/21936  / loss = 0.977741003036499\n",
            "I0420 22:11:26.603976 139904526645120 model_training_utils.py:450] Train Step: 18295/21936  / loss = 0.7828871011734009\n",
            "I0420 22:11:27.141167 139904526645120 model_training_utils.py:450] Train Step: 18296/21936  / loss = 0.6306090354919434\n",
            "I0420 22:11:27.681161 139904526645120 model_training_utils.py:450] Train Step: 18297/21936  / loss = 0.7947062253952026\n",
            "I0420 22:11:28.222275 139904526645120 model_training_utils.py:450] Train Step: 18298/21936  / loss = 0.4437219798564911\n",
            "I0420 22:11:28.758795 139904526645120 model_training_utils.py:450] Train Step: 18299/21936  / loss = 0.6414225697517395\n",
            "I0420 22:11:29.297698 139904526645120 model_training_utils.py:450] Train Step: 18300/21936  / loss = 0.29673969745635986\n",
            "I0420 22:11:29.835741 139904526645120 model_training_utils.py:450] Train Step: 18301/21936  / loss = 0.3273119330406189\n",
            "I0420 22:11:30.374969 139904526645120 model_training_utils.py:450] Train Step: 18302/21936  / loss = 0.252782940864563\n",
            "I0420 22:11:30.915425 139904526645120 model_training_utils.py:450] Train Step: 18303/21936  / loss = 0.4922105371952057\n",
            "I0420 22:11:31.461676 139904526645120 model_training_utils.py:450] Train Step: 18304/21936  / loss = 0.7613430023193359\n",
            "I0420 22:11:32.001732 139904526645120 model_training_utils.py:450] Train Step: 18305/21936  / loss = 0.8389161825180054\n",
            "I0420 22:11:32.539282 139904526645120 model_training_utils.py:450] Train Step: 18306/21936  / loss = 1.9180340766906738\n",
            "I0420 22:11:33.079710 139904526645120 model_training_utils.py:450] Train Step: 18307/21936  / loss = 0.3271939158439636\n",
            "I0420 22:11:33.617190 139904526645120 model_training_utils.py:450] Train Step: 18308/21936  / loss = 1.5798970460891724\n",
            "I0420 22:11:34.158474 139904526645120 model_training_utils.py:450] Train Step: 18309/21936  / loss = 1.0515341758728027\n",
            "I0420 22:11:34.701242 139904526645120 model_training_utils.py:450] Train Step: 18310/21936  / loss = 0.44712740182876587\n",
            "I0420 22:11:35.246963 139904526645120 model_training_utils.py:450] Train Step: 18311/21936  / loss = 0.19099265336990356\n",
            "I0420 22:11:35.784534 139904526645120 model_training_utils.py:450] Train Step: 18312/21936  / loss = 1.0102226734161377\n",
            "I0420 22:11:36.323284 139904526645120 model_training_utils.py:450] Train Step: 18313/21936  / loss = 0.24533796310424805\n",
            "I0420 22:11:36.861632 139904526645120 model_training_utils.py:450] Train Step: 18314/21936  / loss = 0.486320436000824\n",
            "I0420 22:11:37.397409 139904526645120 model_training_utils.py:450] Train Step: 18315/21936  / loss = 0.5017367005348206\n",
            "I0420 22:11:37.936039 139904526645120 model_training_utils.py:450] Train Step: 18316/21936  / loss = 1.5060489177703857\n",
            "I0420 22:11:38.472373 139904526645120 model_training_utils.py:450] Train Step: 18317/21936  / loss = 0.8911539316177368\n",
            "I0420 22:11:39.010229 139904526645120 model_training_utils.py:450] Train Step: 18318/21936  / loss = 0.3945061266422272\n",
            "I0420 22:11:39.547361 139904526645120 keras_utils.py:122] TimeHistory: 26.98 seconds, 14.83 examples/second between steps 29237 and 29287\n",
            "I0420 22:11:39.550410 139904526645120 model_training_utils.py:450] Train Step: 18319/21936  / loss = 0.49664926528930664\n",
            "I0420 22:11:40.090930 139904526645120 model_training_utils.py:450] Train Step: 18320/21936  / loss = 0.8224202990531921\n",
            "I0420 22:11:40.630549 139904526645120 model_training_utils.py:450] Train Step: 18321/21936  / loss = 1.929651141166687\n",
            "I0420 22:11:41.169340 139904526645120 model_training_utils.py:450] Train Step: 18322/21936  / loss = 0.8616021871566772\n",
            "I0420 22:11:41.710222 139904526645120 model_training_utils.py:450] Train Step: 18323/21936  / loss = 0.7154622077941895\n",
            "I0420 22:11:42.246677 139904526645120 model_training_utils.py:450] Train Step: 18324/21936  / loss = 0.9116232991218567\n",
            "I0420 22:11:42.786762 139904526645120 model_training_utils.py:450] Train Step: 18325/21936  / loss = 0.9030473232269287\n",
            "I0420 22:11:43.324531 139904526645120 model_training_utils.py:450] Train Step: 18326/21936  / loss = 0.7395430207252502\n",
            "I0420 22:11:43.862895 139904526645120 model_training_utils.py:450] Train Step: 18327/21936  / loss = 0.5985612869262695\n",
            "I0420 22:11:44.400962 139904526645120 model_training_utils.py:450] Train Step: 18328/21936  / loss = 1.4959475994110107\n",
            "I0420 22:11:44.939693 139904526645120 model_training_utils.py:450] Train Step: 18329/21936  / loss = 0.9277987480163574\n",
            "I0420 22:11:45.475703 139904526645120 model_training_utils.py:450] Train Step: 18330/21936  / loss = 0.4055086672306061\n",
            "I0420 22:11:46.014168 139904526645120 model_training_utils.py:450] Train Step: 18331/21936  / loss = 1.1868001222610474\n",
            "I0420 22:11:46.554166 139904526645120 model_training_utils.py:450] Train Step: 18332/21936  / loss = 0.3859262466430664\n",
            "I0420 22:11:47.095420 139904526645120 model_training_utils.py:450] Train Step: 18333/21936  / loss = 1.084380030632019\n",
            "I0420 22:11:47.640835 139904526645120 model_training_utils.py:450] Train Step: 18334/21936  / loss = 0.7131261229515076\n",
            "I0420 22:11:48.180729 139904526645120 model_training_utils.py:450] Train Step: 18335/21936  / loss = 1.2832893133163452\n",
            "I0420 22:11:48.717470 139904526645120 model_training_utils.py:450] Train Step: 18336/21936  / loss = 0.9826796054840088\n",
            "I0420 22:11:49.257129 139904526645120 model_training_utils.py:450] Train Step: 18337/21936  / loss = 0.2545151710510254\n",
            "I0420 22:11:49.794499 139904526645120 model_training_utils.py:450] Train Step: 18338/21936  / loss = 0.6274127960205078\n",
            "I0420 22:11:50.333126 139904526645120 model_training_utils.py:450] Train Step: 18339/21936  / loss = 0.6028361916542053\n",
            "I0420 22:11:50.872300 139904526645120 model_training_utils.py:450] Train Step: 18340/21936  / loss = 0.6959047317504883\n",
            "I0420 22:11:51.410222 139904526645120 model_training_utils.py:450] Train Step: 18341/21936  / loss = 0.5736391544342041\n",
            "I0420 22:11:51.949102 139904526645120 model_training_utils.py:450] Train Step: 18342/21936  / loss = 0.3334499001502991\n",
            "I0420 22:11:52.486076 139904526645120 model_training_utils.py:450] Train Step: 18343/21936  / loss = 0.6588207483291626\n",
            "I0420 22:11:53.025701 139904526645120 model_training_utils.py:450] Train Step: 18344/21936  / loss = 0.33812105655670166\n",
            "I0420 22:11:53.564419 139904526645120 model_training_utils.py:450] Train Step: 18345/21936  / loss = 0.8543245792388916\n",
            "I0420 22:11:54.105600 139904526645120 model_training_utils.py:450] Train Step: 18346/21936  / loss = 1.0085444450378418\n",
            "I0420 22:11:54.648240 139904526645120 model_training_utils.py:450] Train Step: 18347/21936  / loss = 0.16561031341552734\n",
            "I0420 22:11:55.184700 139904526645120 model_training_utils.py:450] Train Step: 18348/21936  / loss = 0.11131871491670609\n",
            "I0420 22:11:55.721884 139904526645120 model_training_utils.py:450] Train Step: 18349/21936  / loss = 0.7816007137298584\n",
            "I0420 22:11:56.258297 139904526645120 model_training_utils.py:450] Train Step: 18350/21936  / loss = 0.2404032200574875\n",
            "I0420 22:11:56.795637 139904526645120 model_training_utils.py:450] Train Step: 18351/21936  / loss = 0.35162872076034546\n",
            "I0420 22:11:57.335488 139904526645120 model_training_utils.py:450] Train Step: 18352/21936  / loss = 0.552219808101654\n",
            "I0420 22:11:57.875536 139904526645120 model_training_utils.py:450] Train Step: 18353/21936  / loss = 0.6325163841247559\n",
            "I0420 22:11:58.415615 139904526645120 model_training_utils.py:450] Train Step: 18354/21936  / loss = 0.6927952766418457\n",
            "I0420 22:11:58.954021 139904526645120 model_training_utils.py:450] Train Step: 18355/21936  / loss = 0.7728754878044128\n",
            "I0420 22:11:59.492338 139904526645120 model_training_utils.py:450] Train Step: 18356/21936  / loss = 0.6044399738311768\n",
            "I0420 22:12:00.031365 139904526645120 model_training_utils.py:450] Train Step: 18357/21936  / loss = 0.4987049102783203\n",
            "I0420 22:12:00.569729 139904526645120 model_training_utils.py:450] Train Step: 18358/21936  / loss = 0.053151197731494904\n",
            "I0420 22:12:01.109208 139904526645120 model_training_utils.py:450] Train Step: 18359/21936  / loss = 0.4584631025791168\n",
            "I0420 22:12:01.647649 139904526645120 model_training_utils.py:450] Train Step: 18360/21936  / loss = 0.5980432629585266\n",
            "I0420 22:12:02.187741 139904526645120 model_training_utils.py:450] Train Step: 18361/21936  / loss = 0.07607334107160568\n",
            "I0420 22:12:02.726289 139904526645120 model_training_utils.py:450] Train Step: 18362/21936  / loss = 0.2880632281303406\n",
            "I0420 22:12:03.265480 139904526645120 model_training_utils.py:450] Train Step: 18363/21936  / loss = 0.14226382970809937\n",
            "I0420 22:12:03.804997 139904526645120 model_training_utils.py:450] Train Step: 18364/21936  / loss = 0.7736818790435791\n",
            "I0420 22:12:04.344825 139904526645120 model_training_utils.py:450] Train Step: 18365/21936  / loss = 0.20623822510242462\n",
            "I0420 22:12:04.883360 139904526645120 model_training_utils.py:450] Train Step: 18366/21936  / loss = 0.30427995324134827\n",
            "I0420 22:12:05.422456 139904526645120 model_training_utils.py:450] Train Step: 18367/21936  / loss = 0.42703109979629517\n",
            "I0420 22:12:05.964621 139904526645120 model_training_utils.py:450] Train Step: 18368/21936  / loss = 0.8291394710540771\n",
            "I0420 22:12:06.508068 139904526645120 keras_utils.py:122] TimeHistory: 26.96 seconds, 14.84 examples/second between steps 29287 and 29337\n",
            "I0420 22:12:06.510730 139904526645120 model_training_utils.py:450] Train Step: 18369/21936  / loss = 0.3430238962173462\n",
            "I0420 22:12:07.050112 139904526645120 model_training_utils.py:450] Train Step: 18370/21936  / loss = 0.6831817626953125\n",
            "I0420 22:12:07.592104 139904526645120 model_training_utils.py:450] Train Step: 18371/21936  / loss = 0.7852338552474976\n",
            "I0420 22:12:08.130871 139904526645120 model_training_utils.py:450] Train Step: 18372/21936  / loss = 0.17008623480796814\n",
            "I0420 22:12:08.670176 139904526645120 model_training_utils.py:450] Train Step: 18373/21936  / loss = 0.5367199182510376\n",
            "I0420 22:12:09.208954 139904526645120 model_training_utils.py:450] Train Step: 18374/21936  / loss = 0.40405479073524475\n",
            "I0420 22:12:09.748298 139904526645120 model_training_utils.py:450] Train Step: 18375/21936  / loss = 0.4734140634536743\n",
            "I0420 22:12:10.287025 139904526645120 model_training_utils.py:450] Train Step: 18376/21936  / loss = 0.5099692344665527\n",
            "I0420 22:12:10.826851 139904526645120 model_training_utils.py:450] Train Step: 18377/21936  / loss = 0.9312557578086853\n",
            "I0420 22:12:11.366080 139904526645120 model_training_utils.py:450] Train Step: 18378/21936  / loss = 0.5473714470863342\n",
            "I0420 22:12:11.904383 139904526645120 model_training_utils.py:450] Train Step: 18379/21936  / loss = 0.25571519136428833\n",
            "I0420 22:12:12.443314 139904526645120 model_training_utils.py:450] Train Step: 18380/21936  / loss = 0.2017626017332077\n",
            "I0420 22:12:12.982730 139904526645120 model_training_utils.py:450] Train Step: 18381/21936  / loss = 0.11697143316268921\n",
            "I0420 22:12:13.522315 139904526645120 model_training_utils.py:450] Train Step: 18382/21936  / loss = 0.6954976320266724\n",
            "I0420 22:12:14.065975 139904526645120 model_training_utils.py:450] Train Step: 18383/21936  / loss = 0.37346965074539185\n",
            "I0420 22:12:14.612095 139904526645120 model_training_utils.py:450] Train Step: 18384/21936  / loss = 0.5113916993141174\n",
            "I0420 22:12:15.153887 139904526645120 model_training_utils.py:450] Train Step: 18385/21936  / loss = 0.6204105615615845\n",
            "I0420 22:12:15.696750 139904526645120 model_training_utils.py:450] Train Step: 18386/21936  / loss = 0.5922619700431824\n",
            "I0420 22:12:16.236025 139904526645120 model_training_utils.py:450] Train Step: 18387/21936  / loss = 0.264221727848053\n",
            "I0420 22:12:16.774604 139904526645120 model_training_utils.py:450] Train Step: 18388/21936  / loss = 0.5983545780181885\n",
            "I0420 22:12:17.312263 139904526645120 model_training_utils.py:450] Train Step: 18389/21936  / loss = 0.6572855710983276\n",
            "I0420 22:12:17.850189 139904526645120 model_training_utils.py:450] Train Step: 18390/21936  / loss = 0.31975871324539185\n",
            "I0420 22:12:18.389184 139904526645120 model_training_utils.py:450] Train Step: 18391/21936  / loss = 0.6320509910583496\n",
            "I0420 22:12:18.926827 139904526645120 model_training_utils.py:450] Train Step: 18392/21936  / loss = 0.6034934520721436\n",
            "I0420 22:12:19.467342 139904526645120 model_training_utils.py:450] Train Step: 18393/21936  / loss = 0.7219604849815369\n",
            "I0420 22:12:20.004457 139904526645120 model_training_utils.py:450] Train Step: 18394/21936  / loss = 0.3245716691017151\n",
            "I0420 22:12:20.546771 139904526645120 model_training_utils.py:450] Train Step: 18395/21936  / loss = 0.9881322383880615\n",
            "I0420 22:12:21.087109 139904526645120 model_training_utils.py:450] Train Step: 18396/21936  / loss = 0.40403005480766296\n",
            "I0420 22:12:21.629341 139904526645120 model_training_utils.py:450] Train Step: 18397/21936  / loss = 0.4332255721092224\n",
            "I0420 22:12:22.166860 139904526645120 model_training_utils.py:450] Train Step: 18398/21936  / loss = 0.4493774175643921\n",
            "I0420 22:12:22.705940 139904526645120 model_training_utils.py:450] Train Step: 18399/21936  / loss = 0.7428131699562073\n",
            "I0420 22:12:23.243713 139904526645120 model_training_utils.py:450] Train Step: 18400/21936  / loss = 0.7995353937149048\n",
            "I0420 22:12:23.781753 139904526645120 model_training_utils.py:450] Train Step: 18401/21936  / loss = 0.2747780382633209\n",
            "I0420 22:12:24.320017 139904526645120 model_training_utils.py:450] Train Step: 18402/21936  / loss = 0.0769566148519516\n",
            "I0420 22:12:24.864903 139904526645120 model_training_utils.py:450] Train Step: 18403/21936  / loss = 0.23462851345539093\n",
            "I0420 22:12:25.402539 139904526645120 model_training_utils.py:450] Train Step: 18404/21936  / loss = 0.6879094839096069\n",
            "I0420 22:12:25.940861 139904526645120 model_training_utils.py:450] Train Step: 18405/21936  / loss = 0.9812586307525635\n",
            "I0420 22:12:26.479605 139904526645120 model_training_utils.py:450] Train Step: 18406/21936  / loss = 0.5181560516357422\n",
            "I0420 22:12:27.016703 139904526645120 model_training_utils.py:450] Train Step: 18407/21936  / loss = 0.9250633120536804\n",
            "I0420 22:12:27.555552 139904526645120 model_training_utils.py:450] Train Step: 18408/21936  / loss = 1.117098093032837\n",
            "I0420 22:12:28.092841 139904526645120 model_training_utils.py:450] Train Step: 18409/21936  / loss = 0.35331714153289795\n",
            "I0420 22:12:28.628696 139904526645120 model_training_utils.py:450] Train Step: 18410/21936  / loss = 0.5749232172966003\n",
            "I0420 22:12:29.169352 139904526645120 model_training_utils.py:450] Train Step: 18411/21936  / loss = 0.27931076288223267\n",
            "I0420 22:12:29.716220 139904526645120 model_training_utils.py:450] Train Step: 18412/21936  / loss = 0.3503308892250061\n",
            "I0420 22:12:30.254963 139904526645120 model_training_utils.py:450] Train Step: 18413/21936  / loss = 0.22880513966083527\n",
            "I0420 22:12:30.794131 139904526645120 model_training_utils.py:450] Train Step: 18414/21936  / loss = 0.19708234071731567\n",
            "I0420 22:12:31.329657 139904526645120 model_training_utils.py:450] Train Step: 18415/21936  / loss = 0.5189335346221924\n",
            "I0420 22:12:31.867413 139904526645120 model_training_utils.py:450] Train Step: 18416/21936  / loss = 0.8070353865623474\n",
            "I0420 22:12:32.405234 139904526645120 model_training_utils.py:450] Train Step: 18417/21936  / loss = 0.2830701470375061\n",
            "I0420 22:12:32.941950 139904526645120 model_training_utils.py:450] Train Step: 18418/21936  / loss = 0.9541877508163452\n",
            "I0420 22:12:33.479743 139904526645120 keras_utils.py:122] TimeHistory: 26.97 seconds, 14.83 examples/second between steps 29337 and 29387\n",
            "I0420 22:12:33.482558 139904526645120 model_training_utils.py:450] Train Step: 18419/21936  / loss = 0.2672843337059021\n",
            "I0420 22:12:34.018926 139904526645120 model_training_utils.py:450] Train Step: 18420/21936  / loss = 1.0023815631866455\n",
            "I0420 22:12:34.554279 139904526645120 model_training_utils.py:450] Train Step: 18421/21936  / loss = 0.7054581642150879\n",
            "I0420 22:12:35.091057 139904526645120 model_training_utils.py:450] Train Step: 18422/21936  / loss = 0.16974914073944092\n",
            "I0420 22:12:35.629169 139904526645120 model_training_utils.py:450] Train Step: 18423/21936  / loss = 0.6912168264389038\n",
            "I0420 22:12:36.166493 139904526645120 model_training_utils.py:450] Train Step: 18424/21936  / loss = 0.20403774082660675\n",
            "I0420 22:12:36.704893 139904526645120 model_training_utils.py:450] Train Step: 18425/21936  / loss = 0.34295719861984253\n",
            "I0420 22:12:37.241497 139904526645120 model_training_utils.py:450] Train Step: 18426/21936  / loss = 0.5543614625930786\n",
            "I0420 22:12:37.781485 139904526645120 model_training_utils.py:450] Train Step: 18427/21936  / loss = 0.3220106363296509\n",
            "I0420 22:12:38.323359 139904526645120 model_training_utils.py:450] Train Step: 18428/21936  / loss = 0.48939722776412964\n",
            "I0420 22:12:38.862515 139904526645120 model_training_utils.py:450] Train Step: 18429/21936  / loss = 0.3916260600090027\n",
            "I0420 22:12:39.401945 139904526645120 model_training_utils.py:450] Train Step: 18430/21936  / loss = 0.16636060178279877\n",
            "I0420 22:12:39.940503 139904526645120 model_training_utils.py:450] Train Step: 18431/21936  / loss = 0.8906934261322021\n",
            "I0420 22:12:40.478888 139904526645120 model_training_utils.py:450] Train Step: 18432/21936  / loss = 0.06837163865566254\n",
            "I0420 22:12:41.019165 139904526645120 model_training_utils.py:450] Train Step: 18433/21936  / loss = 0.5816683769226074\n",
            "I0420 22:12:41.559220 139904526645120 model_training_utils.py:450] Train Step: 18434/21936  / loss = 0.5788778066635132\n",
            "I0420 22:12:42.098809 139904526645120 model_training_utils.py:450] Train Step: 18435/21936  / loss = 0.3492004871368408\n",
            "I0420 22:12:42.637252 139904526645120 model_training_utils.py:450] Train Step: 18436/21936  / loss = 0.18055926263332367\n",
            "I0420 22:12:43.175412 139904526645120 model_training_utils.py:450] Train Step: 18437/21936  / loss = 0.5841837525367737\n",
            "I0420 22:12:43.711738 139904526645120 model_training_utils.py:450] Train Step: 18438/21936  / loss = 0.24163000285625458\n",
            "I0420 22:12:44.250374 139904526645120 model_training_utils.py:450] Train Step: 18439/21936  / loss = 0.02195069193840027\n",
            "I0420 22:12:44.792285 139904526645120 model_training_utils.py:450] Train Step: 18440/21936  / loss = 0.2938399016857147\n",
            "I0420 22:12:45.330130 139904526645120 model_training_utils.py:450] Train Step: 18441/21936  / loss = 0.3043791651725769\n",
            "I0420 22:12:45.870253 139904526645120 model_training_utils.py:450] Train Step: 18442/21936  / loss = 0.24831926822662354\n",
            "I0420 22:12:46.410839 139904526645120 model_training_utils.py:450] Train Step: 18443/21936  / loss = 0.06573782861232758\n",
            "I0420 22:12:46.954258 139904526645120 model_training_utils.py:450] Train Step: 18444/21936  / loss = 0.4362275302410126\n",
            "I0420 22:12:47.492539 139904526645120 model_training_utils.py:450] Train Step: 18445/21936  / loss = 0.23772113025188446\n",
            "I0420 22:12:48.035587 139904526645120 model_training_utils.py:450] Train Step: 18446/21936  / loss = 0.04583995044231415\n",
            "I0420 22:12:48.575448 139904526645120 model_training_utils.py:450] Train Step: 18447/21936  / loss = 0.16314086318016052\n",
            "I0420 22:12:49.117416 139904526645120 model_training_utils.py:450] Train Step: 18448/21936  / loss = 0.22684481739997864\n",
            "I0420 22:12:49.656336 139904526645120 model_training_utils.py:450] Train Step: 18449/21936  / loss = 0.3844946026802063\n",
            "I0420 22:12:50.196046 139904526645120 model_training_utils.py:450] Train Step: 18450/21936  / loss = 0.36322012543678284\n",
            "I0420 22:12:50.732643 139904526645120 model_training_utils.py:450] Train Step: 18451/21936  / loss = 0.2975468337535858\n",
            "I0420 22:12:51.273998 139904526645120 model_training_utils.py:450] Train Step: 18452/21936  / loss = 0.023451469838619232\n",
            "I0420 22:12:51.813209 139904526645120 model_training_utils.py:450] Train Step: 18453/21936  / loss = 0.2484530508518219\n",
            "I0420 22:12:52.350723 139904526645120 model_training_utils.py:450] Train Step: 18454/21936  / loss = 0.17684492468833923\n",
            "I0420 22:12:52.888215 139904526645120 model_training_utils.py:450] Train Step: 18455/21936  / loss = 0.09562183916568756\n",
            "I0420 22:12:53.423537 139904526645120 model_training_utils.py:450] Train Step: 18456/21936  / loss = 0.09283962845802307\n",
            "I0420 22:12:53.958993 139904526645120 model_training_utils.py:450] Train Step: 18457/21936  / loss = 0.44738689064979553\n",
            "I0420 22:12:54.495756 139904526645120 model_training_utils.py:450] Train Step: 18458/21936  / loss = 1.0107446908950806\n",
            "I0420 22:12:55.034453 139904526645120 model_training_utils.py:450] Train Step: 18459/21936  / loss = 0.2750287353992462\n",
            "I0420 22:12:55.571052 139904526645120 model_training_utils.py:450] Train Step: 18460/21936  / loss = 0.6454479098320007\n",
            "I0420 22:12:56.109496 139904526645120 model_training_utils.py:450] Train Step: 18461/21936  / loss = 0.271338552236557\n",
            "I0420 22:12:56.645537 139904526645120 model_training_utils.py:450] Train Step: 18462/21936  / loss = 0.13919158279895782\n",
            "I0420 22:12:57.180691 139904526645120 model_training_utils.py:450] Train Step: 18463/21936  / loss = 0.0959068089723587\n",
            "I0420 22:12:57.719085 139904526645120 model_training_utils.py:450] Train Step: 18464/21936  / loss = 0.6444936394691467\n",
            "I0420 22:12:58.256168 139904526645120 model_training_utils.py:450] Train Step: 18465/21936  / loss = 1.0006252527236938\n",
            "I0420 22:12:58.794590 139904526645120 model_training_utils.py:450] Train Step: 18466/21936  / loss = 0.7170236110687256\n",
            "I0420 22:12:59.335347 139904526645120 model_training_utils.py:450] Train Step: 18467/21936  / loss = 0.04040934145450592\n",
            "I0420 22:12:59.873914 139904526645120 model_training_utils.py:450] Train Step: 18468/21936  / loss = 0.13814550638198853\n",
            "I0420 22:13:00.408789 139904526645120 keras_utils.py:122] TimeHistory: 26.93 seconds, 14.86 examples/second between steps 29387 and 29437\n",
            "I0420 22:13:00.411648 139904526645120 model_training_utils.py:450] Train Step: 18469/21936  / loss = 1.3703207969665527\n",
            "I0420 22:13:00.949648 139904526645120 model_training_utils.py:450] Train Step: 18470/21936  / loss = 0.5520264506340027\n",
            "I0420 22:13:01.486549 139904526645120 model_training_utils.py:450] Train Step: 18471/21936  / loss = 2.223637819290161\n",
            "I0420 22:13:02.023971 139904526645120 model_training_utils.py:450] Train Step: 18472/21936  / loss = 0.567361056804657\n",
            "I0420 22:13:02.558948 139904526645120 model_training_utils.py:450] Train Step: 18473/21936  / loss = 0.20802836120128632\n",
            "I0420 22:13:03.095179 139904526645120 model_training_utils.py:450] Train Step: 18474/21936  / loss = 0.7990331649780273\n",
            "I0420 22:13:03.631476 139904526645120 model_training_utils.py:450] Train Step: 18475/21936  / loss = 1.2552077770233154\n",
            "I0420 22:13:04.169307 139904526645120 model_training_utils.py:450] Train Step: 18476/21936  / loss = 0.7757983207702637\n",
            "I0420 22:13:04.706392 139904526645120 model_training_utils.py:450] Train Step: 18477/21936  / loss = 0.15954479575157166\n",
            "I0420 22:13:05.243543 139904526645120 model_training_utils.py:450] Train Step: 18478/21936  / loss = 0.4263940453529358\n",
            "I0420 22:13:05.783596 139904526645120 model_training_utils.py:450] Train Step: 18479/21936  / loss = 0.32705825567245483\n",
            "I0420 22:13:06.326221 139904526645120 model_training_utils.py:450] Train Step: 18480/21936  / loss = 0.7253597378730774\n",
            "I0420 22:13:06.865358 139904526645120 model_training_utils.py:450] Train Step: 18481/21936  / loss = 0.5620147585868835\n",
            "I0420 22:13:07.403907 139904526645120 model_training_utils.py:450] Train Step: 18482/21936  / loss = 1.7745362520217896\n",
            "I0420 22:13:07.939473 139904526645120 model_training_utils.py:450] Train Step: 18483/21936  / loss = 1.197758436203003\n",
            "I0420 22:13:08.481968 139904526645120 model_training_utils.py:450] Train Step: 18484/21936  / loss = 1.3662333488464355\n",
            "I0420 22:13:09.019677 139904526645120 model_training_utils.py:450] Train Step: 18485/21936  / loss = 0.541580319404602\n",
            "I0420 22:13:09.562479 139904526645120 model_training_utils.py:450] Train Step: 18486/21936  / loss = 0.4783216714859009\n",
            "I0420 22:13:10.101058 139904526645120 model_training_utils.py:450] Train Step: 18487/21936  / loss = 0.41918566823005676\n",
            "I0420 22:13:10.640243 139904526645120 model_training_utils.py:450] Train Step: 18488/21936  / loss = 0.7450049519538879\n",
            "I0420 22:13:11.180656 139904526645120 model_training_utils.py:450] Train Step: 18489/21936  / loss = 0.12092341482639313\n",
            "I0420 22:13:11.719452 139904526645120 model_training_utils.py:450] Train Step: 18490/21936  / loss = 0.22732311487197876\n",
            "I0420 22:13:12.263342 139904526645120 model_training_utils.py:450] Train Step: 18491/21936  / loss = 1.091163158416748\n",
            "I0420 22:13:12.801775 139904526645120 model_training_utils.py:450] Train Step: 18492/21936  / loss = 0.47839927673339844\n",
            "I0420 22:13:13.339834 139904526645120 model_training_utils.py:450] Train Step: 18493/21936  / loss = 0.23041892051696777\n",
            "I0420 22:13:13.885994 139904526645120 model_training_utils.py:450] Train Step: 18494/21936  / loss = 0.6030493974685669\n",
            "I0420 22:13:14.426137 139904526645120 model_training_utils.py:450] Train Step: 18495/21936  / loss = 0.2632267475128174\n",
            "I0420 22:13:14.964742 139904526645120 model_training_utils.py:450] Train Step: 18496/21936  / loss = 0.5557228326797485\n",
            "I0420 22:13:15.501912 139904526645120 model_training_utils.py:450] Train Step: 18497/21936  / loss = 0.18475104868412018\n",
            "I0420 22:13:16.037281 139904526645120 model_training_utils.py:450] Train Step: 18498/21936  / loss = 0.18369720876216888\n",
            "I0420 22:13:16.572926 139904526645120 model_training_utils.py:450] Train Step: 18499/21936  / loss = 0.2020256370306015\n",
            "I0420 22:13:17.107995 139904526645120 model_training_utils.py:450] Train Step: 18500/21936  / loss = 1.4830187559127808\n",
            "I0420 22:13:17.642505 139904526645120 model_training_utils.py:450] Train Step: 18501/21936  / loss = 1.2778120040893555\n",
            "I0420 22:13:18.178824 139904526645120 model_training_utils.py:450] Train Step: 18502/21936  / loss = 0.3674534857273102\n",
            "I0420 22:13:18.721214 139904526645120 model_training_utils.py:450] Train Step: 18503/21936  / loss = 0.4030226469039917\n",
            "I0420 22:13:19.257999 139904526645120 model_training_utils.py:450] Train Step: 18504/21936  / loss = 0.5904892683029175\n",
            "I0420 22:13:19.793300 139904526645120 model_training_utils.py:450] Train Step: 18505/21936  / loss = 0.46242398023605347\n",
            "I0420 22:13:20.330308 139904526645120 model_training_utils.py:450] Train Step: 18506/21936  / loss = 0.17433826625347137\n",
            "I0420 22:13:20.864515 139904526645120 model_training_utils.py:450] Train Step: 18507/21936  / loss = 0.1158539280295372\n",
            "I0420 22:13:21.402338 139904526645120 model_training_utils.py:450] Train Step: 18508/21936  / loss = 0.14984560012817383\n",
            "I0420 22:13:21.946933 139904526645120 model_training_utils.py:450] Train Step: 18509/21936  / loss = 0.046909112483263016\n",
            "I0420 22:13:22.483318 139904526645120 model_training_utils.py:450] Train Step: 18510/21936  / loss = 0.5188793540000916\n",
            "I0420 22:13:23.018422 139904526645120 model_training_utils.py:450] Train Step: 18511/21936  / loss = 0.4308655261993408\n",
            "I0420 22:13:23.554643 139904526645120 model_training_utils.py:450] Train Step: 18512/21936  / loss = 0.21912458539009094\n",
            "I0420 22:13:24.091982 139904526645120 model_training_utils.py:450] Train Step: 18513/21936  / loss = 0.033510513603687286\n",
            "I0420 22:13:24.628338 139904526645120 model_training_utils.py:450] Train Step: 18514/21936  / loss = 0.31858885288238525\n",
            "I0420 22:13:25.166057 139904526645120 model_training_utils.py:450] Train Step: 18515/21936  / loss = 0.1371917724609375\n",
            "I0420 22:13:25.703549 139904526645120 model_training_utils.py:450] Train Step: 18516/21936  / loss = 0.20204442739486694\n",
            "I0420 22:13:26.242896 139904526645120 model_training_utils.py:450] Train Step: 18517/21936  / loss = 0.06655731797218323\n",
            "I0420 22:13:26.786202 139904526645120 model_training_utils.py:450] Train Step: 18518/21936  / loss = 0.20278362929821014\n",
            "I0420 22:13:27.322783 139904526645120 keras_utils.py:122] TimeHistory: 26.91 seconds, 14.86 examples/second between steps 29437 and 29487\n",
            "I0420 22:13:27.325632 139904526645120 model_training_utils.py:450] Train Step: 18519/21936  / loss = 0.8775759935379028\n",
            "I0420 22:13:27.861170 139904526645120 model_training_utils.py:450] Train Step: 18520/21936  / loss = 0.27601897716522217\n",
            "I0420 22:13:28.400365 139904526645120 model_training_utils.py:450] Train Step: 18521/21936  / loss = 0.3490874171257019\n",
            "I0420 22:13:28.936809 139904526645120 model_training_utils.py:450] Train Step: 18522/21936  / loss = 0.16454127430915833\n",
            "I0420 22:13:29.481847 139904526645120 model_training_utils.py:450] Train Step: 18523/21936  / loss = 0.1809607744216919\n",
            "I0420 22:13:30.030050 139904526645120 model_training_utils.py:450] Train Step: 18524/21936  / loss = 0.14647985994815826\n",
            "I0420 22:13:30.573621 139904526645120 model_training_utils.py:450] Train Step: 18525/21936  / loss = 0.09419862926006317\n",
            "I0420 22:13:31.112496 139904526645120 model_training_utils.py:450] Train Step: 18526/21936  / loss = 0.48662376403808594\n",
            "I0420 22:13:31.651669 139904526645120 model_training_utils.py:450] Train Step: 18527/21936  / loss = 0.4431022107601166\n",
            "I0420 22:13:32.191066 139904526645120 model_training_utils.py:450] Train Step: 18528/21936  / loss = 0.3066592216491699\n",
            "I0420 22:13:32.728606 139904526645120 model_training_utils.py:450] Train Step: 18529/21936  / loss = 0.2694963812828064\n",
            "I0420 22:13:33.265668 139904526645120 model_training_utils.py:450] Train Step: 18530/21936  / loss = 0.06592434644699097\n",
            "I0420 22:13:33.811090 139904526645120 model_training_utils.py:450] Train Step: 18531/21936  / loss = 0.7897504568099976\n",
            "I0420 22:13:34.352999 139904526645120 model_training_utils.py:450] Train Step: 18532/21936  / loss = 0.11454768478870392\n",
            "I0420 22:13:34.893485 139904526645120 model_training_utils.py:450] Train Step: 18533/21936  / loss = 0.04348450154066086\n",
            "I0420 22:13:35.431584 139904526645120 model_training_utils.py:450] Train Step: 18534/21936  / loss = 0.6773567199707031\n",
            "I0420 22:13:35.969987 139904526645120 model_training_utils.py:450] Train Step: 18535/21936  / loss = 1.1206223964691162\n",
            "I0420 22:13:36.508356 139904526645120 model_training_utils.py:450] Train Step: 18536/21936  / loss = 0.19337180256843567\n",
            "I0420 22:13:37.048196 139904526645120 model_training_utils.py:450] Train Step: 18537/21936  / loss = 0.03823939710855484\n",
            "I0420 22:13:37.585741 139904526645120 model_training_utils.py:450] Train Step: 18538/21936  / loss = 0.2475963681936264\n",
            "I0420 22:13:38.124242 139904526645120 model_training_utils.py:450] Train Step: 18539/21936  / loss = 0.07486375421285629\n",
            "I0420 22:13:38.667151 139904526645120 model_training_utils.py:450] Train Step: 18540/21936  / loss = 0.17360542714595795\n",
            "I0420 22:13:39.206354 139904526645120 model_training_utils.py:450] Train Step: 18541/21936  / loss = 0.03221878036856651\n",
            "I0420 22:13:39.741743 139904526645120 model_training_utils.py:450] Train Step: 18542/21936  / loss = 0.07858898490667343\n",
            "I0420 22:13:40.278459 139904526645120 model_training_utils.py:450] Train Step: 18543/21936  / loss = 0.09569214284420013\n",
            "I0420 22:13:40.815085 139904526645120 model_training_utils.py:450] Train Step: 18544/21936  / loss = 0.17063605785369873\n",
            "I0420 22:13:41.352646 139904526645120 model_training_utils.py:450] Train Step: 18545/21936  / loss = 0.030551841482520103\n",
            "I0420 22:13:41.889391 139904526645120 model_training_utils.py:450] Train Step: 18546/21936  / loss = 0.19919924437999725\n",
            "I0420 22:13:42.426205 139904526645120 model_training_utils.py:450] Train Step: 18547/21936  / loss = 0.14300954341888428\n",
            "I0420 22:13:42.964083 139904526645120 model_training_utils.py:450] Train Step: 18548/21936  / loss = 0.13157139718532562\n",
            "I0420 22:13:43.503274 139904526645120 model_training_utils.py:450] Train Step: 18549/21936  / loss = 0.07713819295167923\n",
            "I0420 22:13:44.049457 139904526645120 model_training_utils.py:450] Train Step: 18550/21936  / loss = 0.5142714977264404\n",
            "I0420 22:13:44.585715 139904526645120 model_training_utils.py:450] Train Step: 18551/21936  / loss = 0.08221733570098877\n",
            "I0420 22:13:45.124126 139904526645120 model_training_utils.py:450] Train Step: 18552/21936  / loss = 0.03708434849977493\n",
            "I0420 22:13:45.664491 139904526645120 model_training_utils.py:450] Train Step: 18553/21936  / loss = 0.44221919775009155\n",
            "I0420 22:13:46.203415 139904526645120 model_training_utils.py:450] Train Step: 18554/21936  / loss = 0.05883525311946869\n",
            "I0420 22:13:46.743021 139904526645120 model_training_utils.py:450] Train Step: 18555/21936  / loss = 0.2543407678604126\n",
            "I0420 22:13:47.282058 139904526645120 model_training_utils.py:450] Train Step: 18556/21936  / loss = 0.04576212912797928\n",
            "I0420 22:13:47.821166 139904526645120 model_training_utils.py:450] Train Step: 18557/21936  / loss = 0.12942743301391602\n",
            "I0420 22:13:48.359720 139904526645120 model_training_utils.py:450] Train Step: 18558/21936  / loss = 0.03226353973150253\n",
            "I0420 22:13:48.899230 139904526645120 model_training_utils.py:450] Train Step: 18559/21936  / loss = 0.2847900390625\n",
            "I0420 22:13:49.437413 139904526645120 model_training_utils.py:450] Train Step: 18560/21936  / loss = 0.9953447580337524\n",
            "I0420 22:13:49.977846 139904526645120 model_training_utils.py:450] Train Step: 18561/21936  / loss = 0.06397110223770142\n",
            "I0420 22:13:50.515645 139904526645120 model_training_utils.py:450] Train Step: 18562/21936  / loss = 0.09932638704776764\n",
            "I0420 22:13:51.053850 139904526645120 model_training_utils.py:450] Train Step: 18563/21936  / loss = 0.14348018169403076\n",
            "I0420 22:13:51.599249 139904526645120 model_training_utils.py:450] Train Step: 18564/21936  / loss = 0.011809936724603176\n",
            "I0420 22:13:52.136192 139904526645120 model_training_utils.py:450] Train Step: 18565/21936  / loss = 0.2802988588809967\n",
            "I0420 22:13:52.674808 139904526645120 model_training_utils.py:450] Train Step: 18566/21936  / loss = 0.04519273340702057\n",
            "I0420 22:13:53.214785 139904526645120 model_training_utils.py:450] Train Step: 18567/21936  / loss = 1.1139755249023438\n",
            "I0420 22:13:53.755620 139904526645120 model_training_utils.py:450] Train Step: 18568/21936  / loss = 0.3679085969924927\n",
            "I0420 22:13:54.294245 139904526645120 keras_utils.py:122] TimeHistory: 26.97 seconds, 14.83 examples/second between steps 29487 and 29537\n",
            "I0420 22:13:54.297047 139904526645120 model_training_utils.py:450] Train Step: 18569/21936  / loss = 0.931445837020874\n",
            "I0420 22:13:54.831405 139904526645120 model_training_utils.py:450] Train Step: 18570/21936  / loss = 0.24294047057628632\n",
            "I0420 22:13:55.367876 139904526645120 model_training_utils.py:450] Train Step: 18571/21936  / loss = 0.32577866315841675\n",
            "I0420 22:13:55.906363 139904526645120 model_training_utils.py:450] Train Step: 18572/21936  / loss = 0.38216733932495117\n",
            "I0420 22:13:56.446412 139904526645120 model_training_utils.py:450] Train Step: 18573/21936  / loss = 0.17742480337619781\n",
            "I0420 22:13:56.982732 139904526645120 model_training_utils.py:450] Train Step: 18574/21936  / loss = 0.03482618182897568\n",
            "I0420 22:13:57.522603 139904526645120 model_training_utils.py:450] Train Step: 18575/21936  / loss = 0.24023276567459106\n",
            "I0420 22:13:58.060278 139904526645120 model_training_utils.py:450] Train Step: 18576/21936  / loss = 0.36946824193000793\n",
            "I0420 22:13:58.599696 139904526645120 model_training_utils.py:450] Train Step: 18577/21936  / loss = 0.5666283369064331\n",
            "I0420 22:13:59.139610 139904526645120 model_training_utils.py:450] Train Step: 18578/21936  / loss = 0.13438335061073303\n",
            "I0420 22:13:59.678102 139904526645120 model_training_utils.py:450] Train Step: 18579/21936  / loss = 0.17458948493003845\n",
            "I0420 22:14:00.217890 139904526645120 model_training_utils.py:450] Train Step: 18580/21936  / loss = 0.728455662727356\n",
            "I0420 22:14:00.760417 139904526645120 model_training_utils.py:450] Train Step: 18581/21936  / loss = 0.20246832072734833\n",
            "I0420 22:14:01.298662 139904526645120 model_training_utils.py:450] Train Step: 18582/21936  / loss = 0.08205481618642807\n",
            "I0420 22:14:01.837723 139904526645120 model_training_utils.py:450] Train Step: 18583/21936  / loss = 0.15664418041706085\n",
            "I0420 22:14:02.374715 139904526645120 model_training_utils.py:450] Train Step: 18584/21936  / loss = 0.20770101249217987\n",
            "I0420 22:14:02.911936 139904526645120 model_training_utils.py:450] Train Step: 18585/21936  / loss = 0.1656799614429474\n",
            "I0420 22:14:03.448333 139904526645120 model_training_utils.py:450] Train Step: 18586/21936  / loss = 0.1255870759487152\n",
            "I0420 22:14:03.986850 139904526645120 model_training_utils.py:450] Train Step: 18587/21936  / loss = 0.08708475530147552\n",
            "I0420 22:14:04.524699 139904526645120 model_training_utils.py:450] Train Step: 18588/21936  / loss = 0.16978374123573303\n",
            "I0420 22:14:05.062037 139904526645120 model_training_utils.py:450] Train Step: 18589/21936  / loss = 0.26061075925827026\n",
            "I0420 22:14:05.602499 139904526645120 model_training_utils.py:450] Train Step: 18590/21936  / loss = 0.18029838800430298\n",
            "I0420 22:14:06.143766 139904526645120 model_training_utils.py:450] Train Step: 18591/21936  / loss = 0.18601495027542114\n",
            "I0420 22:14:06.680911 139904526645120 model_training_utils.py:450] Train Step: 18592/21936  / loss = 0.15351346135139465\n",
            "I0420 22:14:07.219357 139904526645120 model_training_utils.py:450] Train Step: 18593/21936  / loss = 0.1920645385980606\n",
            "I0420 22:14:07.758655 139904526645120 model_training_utils.py:450] Train Step: 18594/21936  / loss = 0.08012736588716507\n",
            "I0420 22:14:08.295611 139904526645120 model_training_utils.py:450] Train Step: 18595/21936  / loss = 1.4993395805358887\n",
            "I0420 22:14:08.835823 139904526645120 model_training_utils.py:450] Train Step: 18596/21936  / loss = 0.250454306602478\n",
            "I0420 22:14:09.376320 139904526645120 model_training_utils.py:450] Train Step: 18597/21936  / loss = 0.16047215461730957\n",
            "I0420 22:14:09.915279 139904526645120 model_training_utils.py:450] Train Step: 18598/21936  / loss = 1.2223882675170898\n",
            "I0420 22:14:10.455176 139904526645120 model_training_utils.py:450] Train Step: 18599/21936  / loss = 0.5826375484466553\n",
            "I0420 22:14:10.995439 139904526645120 model_training_utils.py:450] Train Step: 18600/21936  / loss = 0.12376827001571655\n",
            "I0420 22:14:11.531634 139904526645120 model_training_utils.py:450] Train Step: 18601/21936  / loss = 0.456881046295166\n",
            "I0420 22:14:12.069279 139904526645120 model_training_utils.py:450] Train Step: 18602/21936  / loss = 0.8701009154319763\n",
            "I0420 22:14:12.606009 139904526645120 model_training_utils.py:450] Train Step: 18603/21936  / loss = 0.049650296568870544\n",
            "I0420 22:14:13.144998 139904526645120 model_training_utils.py:450] Train Step: 18604/21936  / loss = 0.1528327465057373\n",
            "I0420 22:14:13.683544 139904526645120 model_training_utils.py:450] Train Step: 18605/21936  / loss = 0.17851096391677856\n",
            "I0420 22:14:14.223660 139904526645120 model_training_utils.py:450] Train Step: 18606/21936  / loss = 0.2714529037475586\n",
            "I0420 22:14:14.760350 139904526645120 model_training_utils.py:450] Train Step: 18607/21936  / loss = 0.11380264908075333\n",
            "I0420 22:14:15.298673 139904526645120 model_training_utils.py:450] Train Step: 18608/21936  / loss = 0.4399407207965851\n",
            "I0420 22:14:15.835942 139904526645120 model_training_utils.py:450] Train Step: 18609/21936  / loss = 0.036291204392910004\n",
            "I0420 22:14:16.376429 139904526645120 model_training_utils.py:450] Train Step: 18610/21936  / loss = 0.6736541390419006\n",
            "I0420 22:14:16.917699 139904526645120 model_training_utils.py:450] Train Step: 18611/21936  / loss = 1.1369340419769287\n",
            "I0420 22:14:17.456440 139904526645120 model_training_utils.py:450] Train Step: 18612/21936  / loss = 0.4759516716003418\n",
            "I0420 22:14:17.992666 139904526645120 model_training_utils.py:450] Train Step: 18613/21936  / loss = 0.18837055563926697\n",
            "I0420 22:14:18.530013 139904526645120 model_training_utils.py:450] Train Step: 18614/21936  / loss = 1.0862287282943726\n",
            "I0420 22:14:19.067393 139904526645120 model_training_utils.py:450] Train Step: 18615/21936  / loss = 0.2776367664337158\n",
            "I0420 22:14:19.602042 139904526645120 model_training_utils.py:450] Train Step: 18616/21936  / loss = 0.05888284742832184\n",
            "I0420 22:14:20.139615 139904526645120 model_training_utils.py:450] Train Step: 18617/21936  / loss = 0.015682708472013474\n",
            "I0420 22:14:20.677070 139904526645120 model_training_utils.py:450] Train Step: 18618/21936  / loss = 0.4331675171852112\n",
            "I0420 22:14:21.219122 139904526645120 keras_utils.py:122] TimeHistory: 26.92 seconds, 14.86 examples/second between steps 29537 and 29587\n",
            "I0420 22:14:21.221816 139904526645120 model_training_utils.py:450] Train Step: 18619/21936  / loss = 0.3248779773712158\n",
            "I0420 22:14:21.758727 139904526645120 model_training_utils.py:450] Train Step: 18620/21936  / loss = 0.11588380485773087\n",
            "I0420 22:14:22.293710 139904526645120 model_training_utils.py:450] Train Step: 18621/21936  / loss = 0.32070598006248474\n",
            "I0420 22:14:22.830350 139904526645120 model_training_utils.py:450] Train Step: 18622/21936  / loss = 0.521228551864624\n",
            "I0420 22:14:23.367300 139904526645120 model_training_utils.py:450] Train Step: 18623/21936  / loss = 0.29903391003608704\n",
            "I0420 22:14:23.902875 139904526645120 model_training_utils.py:450] Train Step: 18624/21936  / loss = 0.06311140954494476\n",
            "I0420 22:14:24.440918 139904526645120 model_training_utils.py:450] Train Step: 18625/21936  / loss = 0.14098328351974487\n",
            "I0420 22:14:24.982595 139904526645120 model_training_utils.py:450] Train Step: 18626/21936  / loss = 0.051294147968292236\n",
            "I0420 22:14:25.519578 139904526645120 model_training_utils.py:450] Train Step: 18627/21936  / loss = 0.40586328506469727\n",
            "I0420 22:14:26.056341 139904526645120 model_training_utils.py:450] Train Step: 18628/21936  / loss = 0.1958894431591034\n",
            "I0420 22:14:26.595545 139904526645120 model_training_utils.py:450] Train Step: 18629/21936  / loss = 0.833244800567627\n",
            "I0420 22:14:27.133042 139904526645120 model_training_utils.py:450] Train Step: 18630/21936  / loss = 0.3152380883693695\n",
            "I0420 22:14:27.670482 139904526645120 model_training_utils.py:450] Train Step: 18631/21936  / loss = 0.6071764230728149\n",
            "I0420 22:14:28.207040 139904526645120 model_training_utils.py:450] Train Step: 18632/21936  / loss = 0.3629717230796814\n",
            "I0420 22:14:28.745080 139904526645120 model_training_utils.py:450] Train Step: 18633/21936  / loss = 0.5354512929916382\n",
            "I0420 22:14:29.282640 139904526645120 model_training_utils.py:450] Train Step: 18634/21936  / loss = 0.5626930594444275\n",
            "I0420 22:14:29.817800 139904526645120 model_training_utils.py:450] Train Step: 18635/21936  / loss = 0.29524004459381104\n",
            "I0420 22:14:30.356511 139904526645120 model_training_utils.py:450] Train Step: 18636/21936  / loss = 0.266859769821167\n",
            "I0420 22:14:30.893196 139904526645120 model_training_utils.py:450] Train Step: 18637/21936  / loss = 0.2762603759765625\n",
            "I0420 22:14:31.432248 139904526645120 model_training_utils.py:450] Train Step: 18638/21936  / loss = 0.11462120711803436\n",
            "I0420 22:14:31.968943 139904526645120 model_training_utils.py:450] Train Step: 18639/21936  / loss = 0.3980429172515869\n",
            "I0420 22:14:32.505678 139904526645120 model_training_utils.py:450] Train Step: 18640/21936  / loss = 0.027255095541477203\n",
            "I0420 22:14:33.044463 139904526645120 model_training_utils.py:450] Train Step: 18641/21936  / loss = 0.050352368503808975\n",
            "I0420 22:14:33.583159 139904526645120 model_training_utils.py:450] Train Step: 18642/21936  / loss = 0.17500817775726318\n",
            "I0420 22:14:34.119763 139904526645120 model_training_utils.py:450] Train Step: 18643/21936  / loss = 0.2406759262084961\n",
            "I0420 22:14:34.667618 139904526645120 model_training_utils.py:450] Train Step: 18644/21936  / loss = 0.020325394347310066\n",
            "I0420 22:14:35.205031 139904526645120 model_training_utils.py:450] Train Step: 18645/21936  / loss = 0.043744876980781555\n",
            "I0420 22:14:35.745213 139904526645120 model_training_utils.py:450] Train Step: 18646/21936  / loss = 0.1555100530385971\n",
            "I0420 22:14:36.287883 139904526645120 model_training_utils.py:450] Train Step: 18647/21936  / loss = 0.07139746099710464\n",
            "I0420 22:14:36.832556 139904526645120 model_training_utils.py:450] Train Step: 18648/21936  / loss = 0.9100570678710938\n",
            "I0420 22:14:37.370713 139904526645120 model_training_utils.py:450] Train Step: 18649/21936  / loss = 0.03991098329424858\n",
            "I0420 22:14:37.908903 139904526645120 model_training_utils.py:450] Train Step: 18650/21936  / loss = 0.01148302759975195\n",
            "I0420 22:14:38.444836 139904526645120 model_training_utils.py:450] Train Step: 18651/21936  / loss = 0.8102452754974365\n",
            "I0420 22:14:38.982033 139904526645120 model_training_utils.py:450] Train Step: 18652/21936  / loss = 1.3051344156265259\n",
            "I0420 22:14:39.517215 139904526645120 model_training_utils.py:450] Train Step: 18653/21936  / loss = 0.18174952268600464\n",
            "I0420 22:14:40.053333 139904526645120 model_training_utils.py:450] Train Step: 18654/21936  / loss = 0.03192600607872009\n",
            "I0420 22:14:40.589834 139904526645120 model_training_utils.py:450] Train Step: 18655/21936  / loss = 0.4851798415184021\n",
            "I0420 22:14:41.127914 139904526645120 model_training_utils.py:450] Train Step: 18656/21936  / loss = 0.06696605682373047\n",
            "I0420 22:14:41.666511 139904526645120 model_training_utils.py:450] Train Step: 18657/21936  / loss = 0.021848155185580254\n",
            "I0420 22:14:42.203787 139904526645120 model_training_utils.py:450] Train Step: 18658/21936  / loss = 0.21748419106006622\n",
            "I0420 22:14:42.741278 139904526645120 model_training_utils.py:450] Train Step: 18659/21936  / loss = 0.42504847049713135\n",
            "I0420 22:14:43.277658 139904526645120 model_training_utils.py:450] Train Step: 18660/21936  / loss = 0.04702124744653702\n",
            "I0420 22:14:43.812835 139904526645120 model_training_utils.py:450] Train Step: 18661/21936  / loss = 0.4159493148326874\n",
            "I0420 22:14:44.349006 139904526645120 model_training_utils.py:450] Train Step: 18662/21936  / loss = 0.21576328575611115\n",
            "I0420 22:14:44.886848 139904526645120 model_training_utils.py:450] Train Step: 18663/21936  / loss = 0.11467549204826355\n",
            "I0420 22:14:45.422981 139904526645120 model_training_utils.py:450] Train Step: 18664/21936  / loss = 0.31825345754623413\n",
            "I0420 22:14:45.961126 139904526645120 model_training_utils.py:450] Train Step: 18665/21936  / loss = 0.6862881183624268\n",
            "I0420 22:14:46.507512 139904526645120 model_training_utils.py:450] Train Step: 18666/21936  / loss = 1.1446198225021362\n",
            "I0420 22:14:47.046940 139904526645120 model_training_utils.py:450] Train Step: 18667/21936  / loss = 0.024553298950195312\n",
            "I0420 22:14:47.583538 139904526645120 model_training_utils.py:450] Train Step: 18668/21936  / loss = 0.06305670738220215\n",
            "I0420 22:14:48.120587 139904526645120 keras_utils.py:122] TimeHistory: 26.90 seconds, 14.87 examples/second between steps 29587 and 29637\n",
            "I0420 22:14:48.123434 139904526645120 model_training_utils.py:450] Train Step: 18669/21936  / loss = 0.24446436762809753\n",
            "I0420 22:14:48.660279 139904526645120 model_training_utils.py:450] Train Step: 18670/21936  / loss = 1.1417760848999023\n",
            "I0420 22:14:49.195949 139904526645120 model_training_utils.py:450] Train Step: 18671/21936  / loss = 0.27899324893951416\n",
            "I0420 22:14:49.731652 139904526645120 model_training_utils.py:450] Train Step: 18672/21936  / loss = 0.031159233301877975\n",
            "I0420 22:14:50.268085 139904526645120 model_training_utils.py:450] Train Step: 18673/21936  / loss = 0.30412420630455017\n",
            "I0420 22:14:50.804148 139904526645120 model_training_utils.py:450] Train Step: 18674/21936  / loss = 1.0025769472122192\n",
            "I0420 22:14:51.344059 139904526645120 model_training_utils.py:450] Train Step: 18675/21936  / loss = 0.13145959377288818\n",
            "I0420 22:14:51.884324 139904526645120 model_training_utils.py:450] Train Step: 18676/21936  / loss = 0.44093599915504456\n",
            "I0420 22:14:52.422285 139904526645120 model_training_utils.py:450] Train Step: 18677/21936  / loss = 0.13671647012233734\n",
            "I0420 22:14:52.958518 139904526645120 model_training_utils.py:450] Train Step: 18678/21936  / loss = 0.6659479141235352\n",
            "I0420 22:14:53.497996 139904526645120 model_training_utils.py:450] Train Step: 18679/21936  / loss = 0.4621698260307312\n",
            "I0420 22:14:54.035037 139904526645120 model_training_utils.py:450] Train Step: 18680/21936  / loss = 0.4235750436782837\n",
            "I0420 22:14:54.571364 139904526645120 model_training_utils.py:450] Train Step: 18681/21936  / loss = 0.014330122619867325\n",
            "I0420 22:14:55.109660 139904526645120 model_training_utils.py:450] Train Step: 18682/21936  / loss = 0.8595448732376099\n",
            "I0420 22:14:55.648250 139904526645120 model_training_utils.py:450] Train Step: 18683/21936  / loss = 0.6277125477790833\n",
            "I0420 22:14:56.185642 139904526645120 model_training_utils.py:450] Train Step: 18684/21936  / loss = 0.15318506956100464\n",
            "I0420 22:14:56.722068 139904526645120 model_training_utils.py:450] Train Step: 18685/21936  / loss = 0.12522976100444794\n",
            "I0420 22:14:57.262871 139904526645120 model_training_utils.py:450] Train Step: 18686/21936  / loss = 0.23648396134376526\n",
            "I0420 22:14:57.801488 139904526645120 model_training_utils.py:450] Train Step: 18687/21936  / loss = 0.45199814438819885\n",
            "I0420 22:14:58.340269 139904526645120 model_training_utils.py:450] Train Step: 18688/21936  / loss = 0.0324469730257988\n",
            "I0420 22:14:58.879127 139904526645120 model_training_utils.py:450] Train Step: 18689/21936  / loss = 0.3030383586883545\n",
            "I0420 22:14:59.413627 139904526645120 model_training_utils.py:450] Train Step: 18690/21936  / loss = 0.5680685043334961\n",
            "I0420 22:14:59.949977 139904526645120 model_training_utils.py:450] Train Step: 18691/21936  / loss = 1.8165276050567627\n",
            "I0420 22:15:00.487103 139904526645120 model_training_utils.py:450] Train Step: 18692/21936  / loss = 0.3378511071205139\n",
            "I0420 22:15:01.024036 139904526645120 model_training_utils.py:450] Train Step: 18693/21936  / loss = 0.47027307748794556\n",
            "I0420 22:15:01.558485 139904526645120 model_training_utils.py:450] Train Step: 18694/21936  / loss = 0.25057899951934814\n",
            "I0420 22:15:02.092828 139904526645120 model_training_utils.py:450] Train Step: 18695/21936  / loss = 0.22721949219703674\n",
            "I0420 22:15:02.627395 139904526645120 model_training_utils.py:450] Train Step: 18696/21936  / loss = 0.6641683578491211\n",
            "I0420 22:15:03.163520 139904526645120 model_training_utils.py:450] Train Step: 18697/21936  / loss = 0.3670448958873749\n",
            "I0420 22:15:03.701041 139904526645120 model_training_utils.py:450] Train Step: 18698/21936  / loss = 0.5100324153900146\n",
            "I0420 22:15:04.238954 139904526645120 model_training_utils.py:450] Train Step: 18699/21936  / loss = 0.9243766069412231\n",
            "I0420 22:15:04.776808 139904526645120 model_training_utils.py:450] Train Step: 18700/21936  / loss = 1.5458208322525024\n",
            "I0420 22:15:05.313155 139904526645120 model_training_utils.py:450] Train Step: 18701/21936  / loss = 1.1825029850006104\n",
            "I0420 22:15:05.850424 139904526645120 model_training_utils.py:450] Train Step: 18702/21936  / loss = 0.516063928604126\n",
            "I0420 22:15:06.387319 139904526645120 model_training_utils.py:450] Train Step: 18703/21936  / loss = 0.30111828446388245\n",
            "I0420 22:15:06.925239 139904526645120 model_training_utils.py:450] Train Step: 18704/21936  / loss = 0.6539889574050903\n",
            "I0420 22:15:07.461175 139904526645120 model_training_utils.py:450] Train Step: 18705/21936  / loss = 0.5588958859443665\n",
            "I0420 22:15:07.999215 139904526645120 model_training_utils.py:450] Train Step: 18706/21936  / loss = 0.09718894958496094\n",
            "I0420 22:15:08.537521 139904526645120 model_training_utils.py:450] Train Step: 18707/21936  / loss = 0.21406079828739166\n",
            "I0420 22:15:09.074488 139904526645120 model_training_utils.py:450] Train Step: 18708/21936  / loss = 1.6759066581726074\n",
            "I0420 22:15:09.612489 139904526645120 model_training_utils.py:450] Train Step: 18709/21936  / loss = 0.39443743228912354\n",
            "I0420 22:15:10.147645 139904526645120 model_training_utils.py:450] Train Step: 18710/21936  / loss = 1.6724064350128174\n",
            "I0420 22:15:10.685026 139904526645120 model_training_utils.py:450] Train Step: 18711/21936  / loss = 0.799765408039093\n",
            "I0420 22:15:11.228276 139904526645120 model_training_utils.py:450] Train Step: 18712/21936  / loss = 0.3864147663116455\n",
            "I0420 22:15:11.765399 139904526645120 model_training_utils.py:450] Train Step: 18713/21936  / loss = 0.908484935760498\n",
            "I0420 22:15:12.299421 139904526645120 model_training_utils.py:450] Train Step: 18714/21936  / loss = 0.5389835238456726\n",
            "I0420 22:15:12.834110 139904526645120 model_training_utils.py:450] Train Step: 18715/21936  / loss = 0.9029983282089233\n",
            "I0420 22:15:13.369322 139904526645120 model_training_utils.py:450] Train Step: 18716/21936  / loss = 0.8402442336082458\n",
            "I0420 22:15:13.905225 139904526645120 model_training_utils.py:450] Train Step: 18717/21936  / loss = 0.036140453070402145\n",
            "I0420 22:15:14.440507 139904526645120 model_training_utils.py:450] Train Step: 18718/21936  / loss = 0.6573760509490967\n",
            "I0420 22:15:14.978897 139904526645120 keras_utils.py:122] TimeHistory: 26.85 seconds, 14.90 examples/second between steps 29637 and 29687\n",
            "I0420 22:15:14.982772 139904526645120 model_training_utils.py:450] Train Step: 18719/21936  / loss = 0.6734192967414856\n",
            "I0420 22:15:15.520193 139904526645120 model_training_utils.py:450] Train Step: 18720/21936  / loss = 0.9416688084602356\n",
            "I0420 22:15:16.054619 139904526645120 model_training_utils.py:450] Train Step: 18721/21936  / loss = 1.143083095550537\n",
            "I0420 22:15:16.590636 139904526645120 model_training_utils.py:450] Train Step: 18722/21936  / loss = 0.3010254502296448\n",
            "I0420 22:15:17.125750 139904526645120 model_training_utils.py:450] Train Step: 18723/21936  / loss = 1.1890809535980225\n",
            "I0420 22:15:17.663612 139904526645120 model_training_utils.py:450] Train Step: 18724/21936  / loss = 0.29748648405075073\n",
            "I0420 22:15:18.199715 139904526645120 model_training_utils.py:450] Train Step: 18725/21936  / loss = 0.5882118344306946\n",
            "I0420 22:15:18.735134 139904526645120 model_training_utils.py:450] Train Step: 18726/21936  / loss = 0.2899377942085266\n",
            "I0420 22:15:19.272622 139904526645120 model_training_utils.py:450] Train Step: 18727/21936  / loss = 0.40923911333084106\n",
            "I0420 22:15:19.808222 139904526645120 model_training_utils.py:450] Train Step: 18728/21936  / loss = 0.411382794380188\n",
            "I0420 22:15:20.344204 139904526645120 model_training_utils.py:450] Train Step: 18729/21936  / loss = 0.5324910879135132\n",
            "I0420 22:15:20.880048 139904526645120 model_training_utils.py:450] Train Step: 18730/21936  / loss = 0.35230982303619385\n",
            "I0420 22:15:21.418140 139904526645120 model_training_utils.py:450] Train Step: 18731/21936  / loss = 0.23352152109146118\n",
            "I0420 22:15:21.956451 139904526645120 model_training_utils.py:450] Train Step: 18732/21936  / loss = 0.12394148111343384\n",
            "I0420 22:15:22.491401 139904526645120 model_training_utils.py:450] Train Step: 18733/21936  / loss = 0.1969868689775467\n",
            "I0420 22:15:23.030084 139904526645120 model_training_utils.py:450] Train Step: 18734/21936  / loss = 0.22039516270160675\n",
            "I0420 22:15:23.567814 139904526645120 model_training_utils.py:450] Train Step: 18735/21936  / loss = 0.3866204619407654\n",
            "I0420 22:15:24.103020 139904526645120 model_training_utils.py:450] Train Step: 18736/21936  / loss = 1.0228517055511475\n",
            "I0420 22:15:24.638899 139904526645120 model_training_utils.py:450] Train Step: 18737/21936  / loss = 0.11884966492652893\n",
            "I0420 22:15:25.178083 139904526645120 model_training_utils.py:450] Train Step: 18738/21936  / loss = 0.3570571839809418\n",
            "I0420 22:15:25.716974 139904526645120 model_training_utils.py:450] Train Step: 18739/21936  / loss = 0.2592557668685913\n",
            "I0420 22:15:26.255170 139904526645120 model_training_utils.py:450] Train Step: 18740/21936  / loss = 0.08173619955778122\n",
            "I0420 22:15:26.794025 139904526645120 model_training_utils.py:450] Train Step: 18741/21936  / loss = 0.4595954418182373\n",
            "I0420 22:15:27.330785 139904526645120 model_training_utils.py:450] Train Step: 18742/21936  / loss = 0.38366663455963135\n",
            "I0420 22:15:27.866307 139904526645120 model_training_utils.py:450] Train Step: 18743/21936  / loss = 0.31150227785110474\n",
            "I0420 22:15:28.403937 139904526645120 model_training_utils.py:450] Train Step: 18744/21936  / loss = 0.040547288954257965\n",
            "I0420 22:15:28.942362 139904526645120 model_training_utils.py:450] Train Step: 18745/21936  / loss = 0.01100704912096262\n",
            "I0420 22:15:29.478137 139904526645120 model_training_utils.py:450] Train Step: 18746/21936  / loss = 0.2995154857635498\n",
            "I0420 22:15:30.014805 139904526645120 model_training_utils.py:450] Train Step: 18747/21936  / loss = 0.8631395101547241\n",
            "I0420 22:15:30.552474 139904526645120 model_training_utils.py:450] Train Step: 18748/21936  / loss = 1.0408515930175781\n",
            "I0420 22:15:31.090159 139904526645120 model_training_utils.py:450] Train Step: 18749/21936  / loss = 0.06521186232566833\n",
            "I0420 22:15:31.629151 139904526645120 model_training_utils.py:450] Train Step: 18750/21936  / loss = 0.537247896194458\n",
            "I0420 22:15:32.171942 139904526645120 model_training_utils.py:450] Train Step: 18751/21936  / loss = 0.9952232837677002\n",
            "I0420 22:15:32.709174 139904526645120 model_training_utils.py:450] Train Step: 18752/21936  / loss = 2.0723204612731934\n",
            "I0420 22:15:33.245065 139904526645120 model_training_utils.py:450] Train Step: 18753/21936  / loss = 0.13577108085155487\n",
            "I0420 22:15:33.785720 139904526645120 model_training_utils.py:450] Train Step: 18754/21936  / loss = 0.12106248736381531\n",
            "I0420 22:15:34.324559 139904526645120 model_training_utils.py:450] Train Step: 18755/21936  / loss = 0.0071960873901844025\n",
            "I0420 22:15:34.864678 139904526645120 model_training_utils.py:450] Train Step: 18756/21936  / loss = 0.39245790243148804\n",
            "I0420 22:15:35.401685 139904526645120 model_training_utils.py:450] Train Step: 18757/21936  / loss = 0.09700001031160355\n",
            "I0420 22:15:35.938045 139904526645120 model_training_utils.py:450] Train Step: 18758/21936  / loss = 0.12517672777175903\n",
            "I0420 22:15:36.474024 139904526645120 model_training_utils.py:450] Train Step: 18759/21936  / loss = 0.784174919128418\n",
            "I0420 22:15:37.011460 139904526645120 model_training_utils.py:450] Train Step: 18760/21936  / loss = 0.09918735921382904\n",
            "I0420 22:15:37.549840 139904526645120 model_training_utils.py:450] Train Step: 18761/21936  / loss = 0.6115331649780273\n",
            "I0420 22:15:38.085969 139904526645120 model_training_utils.py:450] Train Step: 18762/21936  / loss = 0.08685818314552307\n",
            "I0420 22:15:38.624712 139904526645120 model_training_utils.py:450] Train Step: 18763/21936  / loss = 0.14240114390850067\n",
            "I0420 22:15:39.163055 139904526645120 model_training_utils.py:450] Train Step: 18764/21936  / loss = 0.08993575721979141\n",
            "I0420 22:15:39.700528 139904526645120 model_training_utils.py:450] Train Step: 18765/21936  / loss = 0.5942696928977966\n",
            "I0420 22:15:40.236850 139904526645120 model_training_utils.py:450] Train Step: 18766/21936  / loss = 0.20068058371543884\n",
            "I0420 22:15:40.772545 139904526645120 model_training_utils.py:450] Train Step: 18767/21936  / loss = 0.3720695972442627\n",
            "I0420 22:15:41.310923 139904526645120 model_training_utils.py:450] Train Step: 18768/21936  / loss = 0.036953382194042206\n",
            "I0420 22:15:41.847355 139904526645120 keras_utils.py:122] TimeHistory: 26.86 seconds, 14.89 examples/second between steps 29687 and 29737\n",
            "I0420 22:15:41.849969 139904526645120 model_training_utils.py:450] Train Step: 18769/21936  / loss = 0.6198957562446594\n",
            "I0420 22:15:42.385710 139904526645120 model_training_utils.py:450] Train Step: 18770/21936  / loss = 0.053779326379299164\n",
            "I0420 22:15:42.920446 139904526645120 model_training_utils.py:450] Train Step: 18771/21936  / loss = 0.15756458044052124\n",
            "I0420 22:15:43.461069 139904526645120 model_training_utils.py:450] Train Step: 18772/21936  / loss = 0.9631977081298828\n",
            "I0420 22:15:43.998136 139904526645120 model_training_utils.py:450] Train Step: 18773/21936  / loss = 0.06532782316207886\n",
            "I0420 22:15:44.536819 139904526645120 model_training_utils.py:450] Train Step: 18774/21936  / loss = 0.28370046615600586\n",
            "I0420 22:15:45.074280 139904526645120 model_training_utils.py:450] Train Step: 18775/21936  / loss = 0.08356080949306488\n",
            "I0420 22:15:45.611146 139904526645120 model_training_utils.py:450] Train Step: 18776/21936  / loss = 0.24017935991287231\n",
            "I0420 22:15:46.149472 139904526645120 model_training_utils.py:450] Train Step: 18777/21936  / loss = 0.3616405725479126\n",
            "I0420 22:15:46.687718 139904526645120 model_training_utils.py:450] Train Step: 18778/21936  / loss = 0.4905525743961334\n",
            "I0420 22:15:47.223798 139904526645120 model_training_utils.py:450] Train Step: 18779/21936  / loss = 0.13127568364143372\n",
            "I0420 22:15:47.760807 139904526645120 model_training_utils.py:450] Train Step: 18780/21936  / loss = 0.3439671993255615\n",
            "I0420 22:15:48.295936 139904526645120 model_training_utils.py:450] Train Step: 18781/21936  / loss = 0.20220495760440826\n",
            "I0420 22:15:48.833174 139904526645120 model_training_utils.py:450] Train Step: 18782/21936  / loss = 0.4357844591140747\n",
            "I0420 22:15:49.367450 139904526645120 model_training_utils.py:450] Train Step: 18783/21936  / loss = 0.19817042350769043\n",
            "I0420 22:15:49.904448 139904526645120 model_training_utils.py:450] Train Step: 18784/21936  / loss = 0.3532540798187256\n",
            "I0420 22:15:50.442002 139904526645120 model_training_utils.py:450] Train Step: 18785/21936  / loss = 0.38522034883499146\n",
            "I0420 22:15:50.978461 139904526645120 model_training_utils.py:450] Train Step: 18786/21936  / loss = 1.531517505645752\n",
            "I0420 22:15:51.515159 139904526645120 model_training_utils.py:450] Train Step: 18787/21936  / loss = 0.9462245106697083\n",
            "I0420 22:15:52.051829 139904526645120 model_training_utils.py:450] Train Step: 18788/21936  / loss = 0.15583981573581696\n",
            "I0420 22:15:52.586636 139904526645120 model_training_utils.py:450] Train Step: 18789/21936  / loss = 0.528934895992279\n",
            "I0420 22:15:53.124456 139904526645120 model_training_utils.py:450] Train Step: 18790/21936  / loss = 0.8062267303466797\n",
            "I0420 22:15:53.661447 139904526645120 model_training_utils.py:450] Train Step: 18791/21936  / loss = 0.9478384256362915\n",
            "I0420 22:15:54.198086 139904526645120 model_training_utils.py:450] Train Step: 18792/21936  / loss = 0.1676155924797058\n",
            "I0420 22:15:54.736559 139904526645120 model_training_utils.py:450] Train Step: 18793/21936  / loss = 1.6949653625488281\n",
            "I0420 22:15:55.274219 139904526645120 model_training_utils.py:450] Train Step: 18794/21936  / loss = 0.20632308721542358\n",
            "I0420 22:15:55.811272 139904526645120 model_training_utils.py:450] Train Step: 18795/21936  / loss = 0.18650230765342712\n",
            "I0420 22:15:56.346313 139904526645120 model_training_utils.py:450] Train Step: 18796/21936  / loss = 0.586739718914032\n",
            "I0420 22:15:56.881866 139904526645120 model_training_utils.py:450] Train Step: 18797/21936  / loss = 0.9581071138381958\n",
            "I0420 22:15:57.419106 139904526645120 model_training_utils.py:450] Train Step: 18798/21936  / loss = 0.6780185103416443\n",
            "I0420 22:15:57.953763 139904526645120 model_training_utils.py:450] Train Step: 18799/21936  / loss = 0.5682038068771362\n",
            "I0420 22:15:58.491689 139904526645120 model_training_utils.py:450] Train Step: 18800/21936  / loss = 1.5561492443084717\n",
            "I0420 22:15:59.027658 139904526645120 model_training_utils.py:450] Train Step: 18801/21936  / loss = 0.9023991823196411\n",
            "I0420 22:15:59.564065 139904526645120 model_training_utils.py:450] Train Step: 18802/21936  / loss = 0.4401286840438843\n",
            "I0420 22:16:00.101976 139904526645120 model_training_utils.py:450] Train Step: 18803/21936  / loss = 0.5082565546035767\n",
            "I0420 22:16:00.639434 139904526645120 model_training_utils.py:450] Train Step: 18804/21936  / loss = 0.0197607334703207\n",
            "I0420 22:16:01.174800 139904526645120 model_training_utils.py:450] Train Step: 18805/21936  / loss = 0.09371486306190491\n",
            "I0420 22:16:01.711022 139904526645120 model_training_utils.py:450] Train Step: 18806/21936  / loss = 1.608792781829834\n",
            "I0420 22:16:02.254951 139904526645120 model_training_utils.py:450] Train Step: 18807/21936  / loss = 0.5304086208343506\n",
            "I0420 22:16:02.789410 139904526645120 model_training_utils.py:450] Train Step: 18808/21936  / loss = 0.7389262914657593\n",
            "I0420 22:16:03.327439 139904526645120 model_training_utils.py:450] Train Step: 18809/21936  / loss = 0.4396594762802124\n",
            "I0420 22:16:03.866752 139904526645120 model_training_utils.py:450] Train Step: 18810/21936  / loss = 0.868171215057373\n",
            "I0420 22:16:04.403498 139904526645120 model_training_utils.py:450] Train Step: 18811/21936  / loss = 0.6847254633903503\n",
            "I0420 22:16:04.940666 139904526645120 model_training_utils.py:450] Train Step: 18812/21936  / loss = 0.5825436115264893\n",
            "I0420 22:16:05.477612 139904526645120 model_training_utils.py:450] Train Step: 18813/21936  / loss = 0.8256775140762329\n",
            "I0420 22:16:06.013192 139904526645120 model_training_utils.py:450] Train Step: 18814/21936  / loss = 0.3207893371582031\n",
            "I0420 22:16:06.561060 139904526645120 model_training_utils.py:450] Train Step: 18815/21936  / loss = 1.0427299737930298\n",
            "I0420 22:16:07.098987 139904526645120 model_training_utils.py:450] Train Step: 18816/21936  / loss = 1.3938106298446655\n",
            "I0420 22:16:07.641345 139904526645120 model_training_utils.py:450] Train Step: 18817/21936  / loss = 0.5119495391845703\n",
            "I0420 22:16:08.182840 139904526645120 model_training_utils.py:450] Train Step: 18818/21936  / loss = 0.607752799987793\n",
            "I0420 22:16:08.722950 139904526645120 keras_utils.py:122] TimeHistory: 26.87 seconds, 14.89 examples/second between steps 29737 and 29787\n",
            "I0420 22:16:08.725516 139904526645120 model_training_utils.py:450] Train Step: 18819/21936  / loss = 0.8940818309783936\n",
            "I0420 22:16:09.264805 139904526645120 model_training_utils.py:450] Train Step: 18820/21936  / loss = 0.9677568674087524\n",
            "I0420 22:16:09.801835 139904526645120 model_training_utils.py:450] Train Step: 18821/21936  / loss = 0.7781191468238831\n",
            "I0420 22:16:10.344262 139904526645120 model_training_utils.py:450] Train Step: 18822/21936  / loss = 0.5260777473449707\n",
            "I0420 22:16:10.881988 139904526645120 model_training_utils.py:450] Train Step: 18823/21936  / loss = 1.4838016033172607\n",
            "I0420 22:16:11.417840 139904526645120 model_training_utils.py:450] Train Step: 18824/21936  / loss = 0.28135770559310913\n",
            "I0420 22:16:11.958834 139904526645120 model_training_utils.py:450] Train Step: 18825/21936  / loss = 0.8911868929862976\n",
            "I0420 22:16:12.495847 139904526645120 model_training_utils.py:450] Train Step: 18826/21936  / loss = 0.8057011365890503\n",
            "I0420 22:16:13.032802 139904526645120 model_training_utils.py:450] Train Step: 18827/21936  / loss = 0.5088814496994019\n",
            "I0420 22:16:13.570874 139904526645120 model_training_utils.py:450] Train Step: 18828/21936  / loss = 0.3349103033542633\n",
            "I0420 22:16:14.109001 139904526645120 model_training_utils.py:450] Train Step: 18829/21936  / loss = 0.32710298895835876\n",
            "I0420 22:16:14.645174 139904526645120 model_training_utils.py:450] Train Step: 18830/21936  / loss = 1.1775336265563965\n",
            "I0420 22:16:15.181587 139904526645120 model_training_utils.py:450] Train Step: 18831/21936  / loss = 0.7335186004638672\n",
            "I0420 22:16:15.717107 139904526645120 model_training_utils.py:450] Train Step: 18832/21936  / loss = 0.2647065222263336\n",
            "I0420 22:16:16.251840 139904526645120 model_training_utils.py:450] Train Step: 18833/21936  / loss = 0.16013002395629883\n",
            "I0420 22:16:16.788765 139904526645120 model_training_utils.py:450] Train Step: 18834/21936  / loss = 0.5717927813529968\n",
            "I0420 22:16:17.330960 139904526645120 model_training_utils.py:450] Train Step: 18835/21936  / loss = 0.9014766812324524\n",
            "I0420 22:16:17.870014 139904526645120 model_training_utils.py:450] Train Step: 18836/21936  / loss = 1.5191092491149902\n",
            "I0420 22:16:18.405980 139904526645120 model_training_utils.py:450] Train Step: 18837/21936  / loss = 1.0293399095535278\n",
            "I0420 22:16:18.942953 139904526645120 model_training_utils.py:450] Train Step: 18838/21936  / loss = 0.405176043510437\n",
            "I0420 22:16:19.481386 139904526645120 model_training_utils.py:450] Train Step: 18839/21936  / loss = 0.21906937658786774\n",
            "I0420 22:16:20.019600 139904526645120 model_training_utils.py:450] Train Step: 18840/21936  / loss = 0.6266714334487915\n",
            "I0420 22:16:20.556908 139904526645120 model_training_utils.py:450] Train Step: 18841/21936  / loss = 0.18817055225372314\n",
            "I0420 22:16:21.093078 139904526645120 model_training_utils.py:450] Train Step: 18842/21936  / loss = 0.3089718818664551\n",
            "I0420 22:16:21.630882 139904526645120 model_training_utils.py:450] Train Step: 18843/21936  / loss = 0.4095880389213562\n",
            "I0420 22:16:22.169631 139904526645120 model_training_utils.py:450] Train Step: 18844/21936  / loss = 0.42645955085754395\n",
            "I0420 22:16:22.707486 139904526645120 model_training_utils.py:450] Train Step: 18845/21936  / loss = 1.258575677871704\n",
            "I0420 22:16:23.245467 139904526645120 model_training_utils.py:450] Train Step: 18846/21936  / loss = 0.6135650873184204\n",
            "I0420 22:16:23.785011 139904526645120 model_training_utils.py:450] Train Step: 18847/21936  / loss = 1.5863580703735352\n",
            "I0420 22:16:24.323131 139904526645120 model_training_utils.py:450] Train Step: 18848/21936  / loss = 0.3499637246131897\n",
            "I0420 22:16:24.860544 139904526645120 model_training_utils.py:450] Train Step: 18849/21936  / loss = 0.29377391934394836\n",
            "I0420 22:16:25.396593 139904526645120 model_training_utils.py:450] Train Step: 18850/21936  / loss = 0.9891736507415771\n",
            "I0420 22:16:25.933132 139904526645120 model_training_utils.py:450] Train Step: 18851/21936  / loss = 0.5662064552307129\n",
            "I0420 22:16:26.470112 139904526645120 model_training_utils.py:450] Train Step: 18852/21936  / loss = 1.5383329391479492\n",
            "I0420 22:16:27.015301 139904526645120 model_training_utils.py:450] Train Step: 18853/21936  / loss = 2.244009494781494\n",
            "I0420 22:16:27.554056 139904526645120 model_training_utils.py:450] Train Step: 18854/21936  / loss = 0.6566991806030273\n",
            "I0420 22:16:28.091256 139904526645120 model_training_utils.py:450] Train Step: 18855/21936  / loss = 0.9811161756515503\n",
            "I0420 22:16:28.627945 139904526645120 model_training_utils.py:450] Train Step: 18856/21936  / loss = 0.6207249164581299\n",
            "I0420 22:16:29.164835 139904526645120 model_training_utils.py:450] Train Step: 18857/21936  / loss = 0.3824605345726013\n",
            "I0420 22:16:29.701598 139904526645120 model_training_utils.py:450] Train Step: 18858/21936  / loss = 0.30568504333496094\n",
            "I0420 22:16:30.239295 139904526645120 model_training_utils.py:450] Train Step: 18859/21936  / loss = 0.47937095165252686\n",
            "I0420 22:16:30.777600 139904526645120 model_training_utils.py:450] Train Step: 18860/21936  / loss = 0.1302068680524826\n",
            "I0420 22:16:31.315115 139904526645120 model_training_utils.py:450] Train Step: 18861/21936  / loss = 0.24131876230239868\n",
            "I0420 22:16:31.860944 139904526645120 model_training_utils.py:450] Train Step: 18862/21936  / loss = 0.23371826112270355\n",
            "I0420 22:16:32.398623 139904526645120 model_training_utils.py:450] Train Step: 18863/21936  / loss = 0.5301651954650879\n",
            "I0420 22:16:32.936275 139904526645120 model_training_utils.py:450] Train Step: 18864/21936  / loss = 0.6563942432403564\n",
            "I0420 22:16:33.473466 139904526645120 model_training_utils.py:450] Train Step: 18865/21936  / loss = 0.8334647417068481\n",
            "I0420 22:16:34.013877 139904526645120 model_training_utils.py:450] Train Step: 18866/21936  / loss = 0.686837911605835\n",
            "I0420 22:16:34.553476 139904526645120 model_training_utils.py:450] Train Step: 18867/21936  / loss = 0.41538840532302856\n",
            "I0420 22:16:35.091633 139904526645120 model_training_utils.py:450] Train Step: 18868/21936  / loss = 0.22029602527618408\n",
            "I0420 22:16:35.630359 139904526645120 keras_utils.py:122] TimeHistory: 26.90 seconds, 14.87 examples/second between steps 29787 and 29837\n",
            "I0420 22:16:35.632930 139904526645120 model_training_utils.py:450] Train Step: 18869/21936  / loss = 0.2650432288646698\n",
            "I0420 22:16:36.170333 139904526645120 model_training_utils.py:450] Train Step: 18870/21936  / loss = 1.053193211555481\n",
            "I0420 22:16:36.708363 139904526645120 model_training_utils.py:450] Train Step: 18871/21936  / loss = 0.1651725322008133\n",
            "I0420 22:16:37.246175 139904526645120 model_training_utils.py:450] Train Step: 18872/21936  / loss = 0.3889920711517334\n",
            "I0420 22:16:37.781905 139904526645120 model_training_utils.py:450] Train Step: 18873/21936  / loss = 0.20476806163787842\n",
            "I0420 22:16:38.321984 139904526645120 model_training_utils.py:450] Train Step: 18874/21936  / loss = 0.33525407314300537\n",
            "I0420 22:16:38.859980 139904526645120 model_training_utils.py:450] Train Step: 18875/21936  / loss = 0.4071597456932068\n",
            "I0420 22:16:39.397849 139904526645120 model_training_utils.py:450] Train Step: 18876/21936  / loss = 0.1356090009212494\n",
            "I0420 22:16:39.940737 139904526645120 model_training_utils.py:450] Train Step: 18877/21936  / loss = 0.35840219259262085\n",
            "I0420 22:16:40.476714 139904526645120 model_training_utils.py:450] Train Step: 18878/21936  / loss = 0.8218538761138916\n",
            "I0420 22:16:41.013463 139904526645120 model_training_utils.py:450] Train Step: 18879/21936  / loss = 0.35959672927856445\n",
            "I0420 22:16:41.552302 139904526645120 model_training_utils.py:450] Train Step: 18880/21936  / loss = 0.4105483889579773\n",
            "I0420 22:16:42.091899 139904526645120 model_training_utils.py:450] Train Step: 18881/21936  / loss = 0.6717905402183533\n",
            "I0420 22:16:42.629024 139904526645120 model_training_utils.py:450] Train Step: 18882/21936  / loss = 0.41829103231430054\n",
            "I0420 22:16:43.165109 139904526645120 model_training_utils.py:450] Train Step: 18883/21936  / loss = 0.41477054357528687\n",
            "I0420 22:16:43.708235 139904526645120 model_training_utils.py:450] Train Step: 18884/21936  / loss = 1.4141215085983276\n",
            "I0420 22:16:44.249976 139904526645120 model_training_utils.py:450] Train Step: 18885/21936  / loss = 0.8694136142730713\n",
            "I0420 22:16:44.794818 139904526645120 model_training_utils.py:450] Train Step: 18886/21936  / loss = 0.38943758606910706\n",
            "I0420 22:16:45.331331 139904526645120 model_training_utils.py:450] Train Step: 18887/21936  / loss = 0.42919111251831055\n",
            "I0420 22:16:45.867403 139904526645120 model_training_utils.py:450] Train Step: 18888/21936  / loss = 0.5528914332389832\n",
            "I0420 22:16:46.406159 139904526645120 model_training_utils.py:450] Train Step: 18889/21936  / loss = 1.1660853624343872\n",
            "I0420 22:16:46.942128 139904526645120 model_training_utils.py:450] Train Step: 18890/21936  / loss = 0.6513503193855286\n",
            "I0420 22:16:47.480078 139904526645120 model_training_utils.py:450] Train Step: 18891/21936  / loss = 0.7769903540611267\n",
            "I0420 22:16:48.027977 139904526645120 model_training_utils.py:450] Train Step: 18892/21936  / loss = 0.5694476366043091\n",
            "I0420 22:16:48.563799 139904526645120 model_training_utils.py:450] Train Step: 18893/21936  / loss = 0.5077683925628662\n",
            "I0420 22:16:49.100415 139904526645120 model_training_utils.py:450] Train Step: 18894/21936  / loss = 0.5487274527549744\n",
            "I0420 22:16:49.642091 139904526645120 model_training_utils.py:450] Train Step: 18895/21936  / loss = 0.9336078763008118\n",
            "I0420 22:16:50.180065 139904526645120 model_training_utils.py:450] Train Step: 18896/21936  / loss = 1.184592843055725\n",
            "I0420 22:16:50.715959 139904526645120 model_training_utils.py:450] Train Step: 18897/21936  / loss = 1.0877611637115479\n",
            "I0420 22:16:51.251130 139904526645120 model_training_utils.py:450] Train Step: 18898/21936  / loss = 2.0019798278808594\n",
            "I0420 22:16:51.789113 139904526645120 model_training_utils.py:450] Train Step: 18899/21936  / loss = 0.861553966999054\n",
            "I0420 22:16:52.323496 139904526645120 model_training_utils.py:450] Train Step: 18900/21936  / loss = 1.5518927574157715\n",
            "I0420 22:16:52.862100 139904526645120 model_training_utils.py:450] Train Step: 18901/21936  / loss = 0.47824227809906006\n",
            "I0420 22:16:53.410184 139904526645120 model_training_utils.py:450] Train Step: 18902/21936  / loss = 0.3093995451927185\n",
            "I0420 22:16:53.947208 139904526645120 model_training_utils.py:450] Train Step: 18903/21936  / loss = 0.47218066453933716\n",
            "I0420 22:16:54.484703 139904526645120 model_training_utils.py:450] Train Step: 18904/21936  / loss = 0.578184962272644\n",
            "I0420 22:16:55.023742 139904526645120 model_training_utils.py:450] Train Step: 18905/21936  / loss = 0.6483029127120972\n",
            "I0420 22:16:55.558919 139904526645120 model_training_utils.py:450] Train Step: 18906/21936  / loss = 0.6049160957336426\n",
            "I0420 22:16:56.098798 139904526645120 model_training_utils.py:450] Train Step: 18907/21936  / loss = 0.6413242816925049\n",
            "I0420 22:16:56.635863 139904526645120 model_training_utils.py:450] Train Step: 18908/21936  / loss = 0.387006938457489\n",
            "I0420 22:16:57.172811 139904526645120 model_training_utils.py:450] Train Step: 18909/21936  / loss = 0.3071146309375763\n",
            "I0420 22:16:57.709644 139904526645120 model_training_utils.py:450] Train Step: 18910/21936  / loss = 0.12940296530723572\n",
            "I0420 22:16:58.245552 139904526645120 model_training_utils.py:450] Train Step: 18911/21936  / loss = 0.09084135293960571\n",
            "I0420 22:16:58.783123 139904526645120 model_training_utils.py:450] Train Step: 18912/21936  / loss = 0.37722504138946533\n",
            "I0420 22:16:59.329205 139904526645120 model_training_utils.py:450] Train Step: 18913/21936  / loss = 0.7319711446762085\n",
            "I0420 22:16:59.867090 139904526645120 model_training_utils.py:450] Train Step: 18914/21936  / loss = 0.18995589017868042\n",
            "I0420 22:17:00.403883 139904526645120 model_training_utils.py:450] Train Step: 18915/21936  / loss = 0.1513775885105133\n",
            "I0420 22:17:00.941389 139904526645120 model_training_utils.py:450] Train Step: 18916/21936  / loss = 0.4099142551422119\n",
            "I0420 22:17:01.480642 139904526645120 model_training_utils.py:450] Train Step: 18917/21936  / loss = 0.59336256980896\n",
            "I0420 22:17:02.018167 139904526645120 model_training_utils.py:450] Train Step: 18918/21936  / loss = 0.32681840658187866\n",
            "I0420 22:17:02.555586 139904526645120 keras_utils.py:122] TimeHistory: 26.92 seconds, 14.86 examples/second between steps 29837 and 29887\n",
            "I0420 22:17:02.558297 139904526645120 model_training_utils.py:450] Train Step: 18919/21936  / loss = 0.7733008861541748\n",
            "I0420 22:17:03.092916 139904526645120 model_training_utils.py:450] Train Step: 18920/21936  / loss = 0.6806110739707947\n",
            "I0420 22:17:03.629532 139904526645120 model_training_utils.py:450] Train Step: 18921/21936  / loss = 0.20236489176750183\n",
            "I0420 22:17:04.166410 139904526645120 model_training_utils.py:450] Train Step: 18922/21936  / loss = 0.31911346316337585\n",
            "I0420 22:17:04.702416 139904526645120 model_training_utils.py:450] Train Step: 18923/21936  / loss = 0.6865777373313904\n",
            "I0420 22:17:05.243624 139904526645120 model_training_utils.py:450] Train Step: 18924/21936  / loss = 0.06223680078983307\n",
            "I0420 22:17:05.779677 139904526645120 model_training_utils.py:450] Train Step: 18925/21936  / loss = 0.10113159567117691\n",
            "I0420 22:17:06.317591 139904526645120 model_training_utils.py:450] Train Step: 18926/21936  / loss = 0.22958043217658997\n",
            "I0420 22:17:06.854253 139904526645120 model_training_utils.py:450] Train Step: 18927/21936  / loss = 0.30668193101882935\n",
            "I0420 22:17:07.390084 139904526645120 model_training_utils.py:450] Train Step: 18928/21936  / loss = 0.313220739364624\n",
            "I0420 22:17:07.926607 139904526645120 model_training_utils.py:450] Train Step: 18929/21936  / loss = 0.3042891025543213\n",
            "I0420 22:17:08.463049 139904526645120 model_training_utils.py:450] Train Step: 18930/21936  / loss = 0.8876057863235474\n",
            "I0420 22:17:08.998027 139904526645120 model_training_utils.py:450] Train Step: 18931/21936  / loss = 0.2636229693889618\n",
            "I0420 22:17:09.532109 139904526645120 model_training_utils.py:450] Train Step: 18932/21936  / loss = 0.5492197871208191\n",
            "I0420 22:17:10.067783 139904526645120 model_training_utils.py:450] Train Step: 18933/21936  / loss = 0.23913191258907318\n",
            "I0420 22:17:10.606336 139904526645120 model_training_utils.py:450] Train Step: 18934/21936  / loss = 0.04544980451464653\n",
            "I0420 22:17:11.141217 139904526645120 model_training_utils.py:450] Train Step: 18935/21936  / loss = 0.1753949671983719\n",
            "I0420 22:17:11.677778 139904526645120 model_training_utils.py:450] Train Step: 18936/21936  / loss = 0.09001357853412628\n",
            "I0420 22:17:12.214390 139904526645120 model_training_utils.py:450] Train Step: 18937/21936  / loss = 0.25914913415908813\n",
            "I0420 22:17:12.751040 139904526645120 model_training_utils.py:450] Train Step: 18938/21936  / loss = 0.41298550367355347\n",
            "I0420 22:17:13.285975 139904526645120 model_training_utils.py:450] Train Step: 18939/21936  / loss = 0.10939149558544159\n",
            "I0420 22:17:13.820036 139904526645120 model_training_utils.py:450] Train Step: 18940/21936  / loss = 0.17740784585475922\n",
            "I0420 22:17:14.357558 139904526645120 model_training_utils.py:450] Train Step: 18941/21936  / loss = 0.7884507179260254\n",
            "I0420 22:17:14.893020 139904526645120 model_training_utils.py:450] Train Step: 18942/21936  / loss = 0.3600580394268036\n",
            "I0420 22:17:15.427671 139904526645120 model_training_utils.py:450] Train Step: 18943/21936  / loss = 0.39255738258361816\n",
            "I0420 22:17:15.965110 139904526645120 model_training_utils.py:450] Train Step: 18944/21936  / loss = 0.6020723581314087\n",
            "I0420 22:17:16.506199 139904526645120 model_training_utils.py:450] Train Step: 18945/21936  / loss = 1.6172163486480713\n",
            "I0420 22:17:17.042454 139904526645120 model_training_utils.py:450] Train Step: 18946/21936  / loss = 0.4079294800758362\n",
            "I0420 22:17:17.581336 139904526645120 model_training_utils.py:450] Train Step: 18947/21936  / loss = 0.3187098503112793\n",
            "I0420 22:17:18.117035 139904526645120 model_training_utils.py:450] Train Step: 18948/21936  / loss = 0.44432681798934937\n",
            "I0420 22:17:18.654108 139904526645120 model_training_utils.py:450] Train Step: 18949/21936  / loss = 0.1263541430234909\n",
            "I0420 22:17:19.190367 139904526645120 model_training_utils.py:450] Train Step: 18950/21936  / loss = 0.03762776777148247\n",
            "I0420 22:17:19.726362 139904526645120 model_training_utils.py:450] Train Step: 18951/21936  / loss = 0.3874328136444092\n",
            "I0420 22:17:20.262765 139904526645120 model_training_utils.py:450] Train Step: 18952/21936  / loss = 0.14771786332130432\n",
            "I0420 22:17:20.800582 139904526645120 model_training_utils.py:450] Train Step: 18953/21936  / loss = 0.09345412254333496\n",
            "I0420 22:17:21.336213 139904526645120 model_training_utils.py:450] Train Step: 18954/21936  / loss = 0.14615082740783691\n",
            "I0420 22:17:21.873267 139904526645120 model_training_utils.py:450] Train Step: 18955/21936  / loss = 0.09037882089614868\n",
            "I0420 22:17:22.409692 139904526645120 model_training_utils.py:450] Train Step: 18956/21936  / loss = 0.21820805966854095\n",
            "I0420 22:17:22.944754 139904526645120 model_training_utils.py:450] Train Step: 18957/21936  / loss = 0.24293413758277893\n",
            "I0420 22:17:23.481913 139904526645120 model_training_utils.py:450] Train Step: 18958/21936  / loss = 0.28109773993492126\n",
            "I0420 22:17:24.017904 139904526645120 model_training_utils.py:450] Train Step: 18959/21936  / loss = 0.4837793707847595\n",
            "I0420 22:17:24.554021 139904526645120 model_training_utils.py:450] Train Step: 18960/21936  / loss = 0.2809344530105591\n",
            "I0420 22:17:25.089775 139904526645120 model_training_utils.py:450] Train Step: 18961/21936  / loss = 1.1162663698196411\n",
            "I0420 22:17:25.626308 139904526645120 model_training_utils.py:450] Train Step: 18962/21936  / loss = 1.8714125156402588\n",
            "I0420 22:17:26.163351 139904526645120 model_training_utils.py:450] Train Step: 18963/21936  / loss = 0.09815863519906998\n",
            "I0420 22:17:26.699682 139904526645120 model_training_utils.py:450] Train Step: 18964/21936  / loss = 0.5324628353118896\n",
            "I0420 22:17:27.236077 139904526645120 model_training_utils.py:450] Train Step: 18965/21936  / loss = 0.4379113018512726\n",
            "I0420 22:17:27.776069 139904526645120 model_training_utils.py:450] Train Step: 18966/21936  / loss = 0.19126832485198975\n",
            "I0420 22:17:28.313767 139904526645120 model_training_utils.py:450] Train Step: 18967/21936  / loss = 0.04971698671579361\n",
            "I0420 22:17:28.851586 139904526645120 model_training_utils.py:450] Train Step: 18968/21936  / loss = 0.3396836519241333\n",
            "I0420 22:17:29.388183 139904526645120 keras_utils.py:122] TimeHistory: 26.83 seconds, 14.91 examples/second between steps 29887 and 29937\n",
            "I0420 22:17:29.390878 139904526645120 model_training_utils.py:450] Train Step: 18969/21936  / loss = 0.1511877030134201\n",
            "I0420 22:17:29.927440 139904526645120 model_training_utils.py:450] Train Step: 18970/21936  / loss = 0.9051312208175659\n",
            "I0420 22:17:30.463626 139904526645120 model_training_utils.py:450] Train Step: 18971/21936  / loss = 0.8950764536857605\n",
            "I0420 22:17:30.999838 139904526645120 model_training_utils.py:450] Train Step: 18972/21936  / loss = 0.08238890022039413\n",
            "I0420 22:17:31.535199 139904526645120 model_training_utils.py:450] Train Step: 18973/21936  / loss = 0.4127030372619629\n",
            "I0420 22:17:32.072264 139904526645120 model_training_utils.py:450] Train Step: 18974/21936  / loss = 0.5838067531585693\n",
            "I0420 22:17:32.609695 139904526645120 model_training_utils.py:450] Train Step: 18975/21936  / loss = 0.4918980002403259\n",
            "I0420 22:17:33.145686 139904526645120 model_training_utils.py:450] Train Step: 18976/21936  / loss = 0.28849825263023376\n",
            "I0420 22:17:33.680254 139904526645120 model_training_utils.py:450] Train Step: 18977/21936  / loss = 0.3492186665534973\n",
            "I0420 22:17:34.217239 139904526645120 model_training_utils.py:450] Train Step: 18978/21936  / loss = 0.176780104637146\n",
            "I0420 22:17:34.757825 139904526645120 model_training_utils.py:450] Train Step: 18979/21936  / loss = 0.12276703119277954\n",
            "I0420 22:17:35.293853 139904526645120 model_training_utils.py:450] Train Step: 18980/21936  / loss = 0.6778119206428528\n",
            "I0420 22:17:35.831646 139904526645120 model_training_utils.py:450] Train Step: 18981/21936  / loss = 0.28743618726730347\n",
            "I0420 22:17:36.367819 139904526645120 model_training_utils.py:450] Train Step: 18982/21936  / loss = 0.6674058437347412\n",
            "I0420 22:17:36.906226 139904526645120 model_training_utils.py:450] Train Step: 18983/21936  / loss = 0.8545283675193787\n",
            "I0420 22:17:37.443541 139904526645120 model_training_utils.py:450] Train Step: 18984/21936  / loss = 0.2714560031890869\n",
            "I0420 22:17:37.980550 139904526645120 model_training_utils.py:450] Train Step: 18985/21936  / loss = 0.7386625409126282\n",
            "I0420 22:17:38.516670 139904526645120 model_training_utils.py:450] Train Step: 18986/21936  / loss = 0.14047370851039886\n",
            "I0420 22:17:39.052455 139904526645120 model_training_utils.py:450] Train Step: 18987/21936  / loss = 0.5951792001724243\n",
            "I0420 22:17:39.586989 139904526645120 model_training_utils.py:450] Train Step: 18988/21936  / loss = 0.4825161397457123\n",
            "I0420 22:17:40.123940 139904526645120 model_training_utils.py:450] Train Step: 18989/21936  / loss = 1.113704800605774\n",
            "I0420 22:17:40.660995 139904526645120 model_training_utils.py:450] Train Step: 18990/21936  / loss = 0.20408231019973755\n",
            "I0420 22:17:41.206765 139904526645120 model_training_utils.py:450] Train Step: 18991/21936  / loss = 0.9643217921257019\n",
            "I0420 22:17:41.743273 139904526645120 model_training_utils.py:450] Train Step: 18992/21936  / loss = 0.3948104679584503\n",
            "I0420 22:17:42.286135 139904526645120 model_training_utils.py:450] Train Step: 18993/21936  / loss = 0.5427134037017822\n",
            "I0420 22:17:42.825976 139904526645120 model_training_utils.py:450] Train Step: 18994/21936  / loss = 0.50278240442276\n",
            "I0420 22:17:43.360208 139904526645120 model_training_utils.py:450] Train Step: 18995/21936  / loss = 0.2580047845840454\n",
            "I0420 22:17:43.904786 139904526645120 model_training_utils.py:450] Train Step: 18996/21936  / loss = 0.4353589117527008\n",
            "I0420 22:17:44.446517 139904526645120 model_training_utils.py:450] Train Step: 18997/21936  / loss = 0.02320779114961624\n",
            "I0420 22:17:44.986834 139904526645120 model_training_utils.py:450] Train Step: 18998/21936  / loss = 0.15980730950832367\n",
            "I0420 22:17:45.525429 139904526645120 model_training_utils.py:450] Train Step: 18999/21936  / loss = 0.23098883032798767\n",
            "I0420 22:17:46.064002 139904526645120 model_training_utils.py:450] Train Step: 19000/21936  / loss = 0.29810237884521484\n",
            "I0420 22:17:46.600815 139904526645120 model_training_utils.py:450] Train Step: 19001/21936  / loss = 0.4193170964717865\n",
            "I0420 22:17:47.138011 139904526645120 model_training_utils.py:450] Train Step: 19002/21936  / loss = 0.23894202709197998\n",
            "I0420 22:17:47.673879 139904526645120 model_training_utils.py:450] Train Step: 19003/21936  / loss = 0.20070743560791016\n",
            "I0420 22:17:48.209900 139904526645120 model_training_utils.py:450] Train Step: 19004/21936  / loss = 0.6288827061653137\n",
            "I0420 22:17:48.744780 139904526645120 model_training_utils.py:450] Train Step: 19005/21936  / loss = 0.1361703872680664\n",
            "I0420 22:17:49.285202 139904526645120 model_training_utils.py:450] Train Step: 19006/21936  / loss = 0.20049160718917847\n",
            "I0420 22:17:49.820190 139904526645120 model_training_utils.py:450] Train Step: 19007/21936  / loss = 1.1211774349212646\n",
            "I0420 22:17:50.357181 139904526645120 model_training_utils.py:450] Train Step: 19008/21936  / loss = 0.19154712557792664\n",
            "I0420 22:17:50.892691 139904526645120 model_training_utils.py:450] Train Step: 19009/21936  / loss = 0.5735148191452026\n",
            "I0420 22:17:51.427949 139904526645120 model_training_utils.py:450] Train Step: 19010/21936  / loss = 0.2380324900150299\n",
            "I0420 22:17:51.965300 139904526645120 model_training_utils.py:450] Train Step: 19011/21936  / loss = 0.23757264018058777\n",
            "I0420 22:17:52.499602 139904526645120 model_training_utils.py:450] Train Step: 19012/21936  / loss = 1.098568320274353\n",
            "I0420 22:17:53.034942 139904526645120 model_training_utils.py:450] Train Step: 19013/21936  / loss = 0.355663001537323\n",
            "I0420 22:17:53.571454 139904526645120 model_training_utils.py:450] Train Step: 19014/21936  / loss = 0.22990721464157104\n",
            "I0420 22:17:54.109240 139904526645120 model_training_utils.py:450] Train Step: 19015/21936  / loss = 0.09080711007118225\n",
            "I0420 22:17:54.646023 139904526645120 model_training_utils.py:450] Train Step: 19016/21936  / loss = 0.43026354908943176\n",
            "I0420 22:17:55.182881 139904526645120 model_training_utils.py:450] Train Step: 19017/21936  / loss = 0.3386937975883484\n",
            "I0420 22:17:55.718667 139904526645120 model_training_utils.py:450] Train Step: 19018/21936  / loss = 0.19872567057609558\n",
            "I0420 22:17:56.255633 139904526645120 keras_utils.py:122] TimeHistory: 26.86 seconds, 14.89 examples/second between steps 29937 and 29987\n",
            "I0420 22:17:56.258364 139904526645120 model_training_utils.py:450] Train Step: 19019/21936  / loss = 0.5097800493240356\n",
            "I0420 22:17:56.794479 139904526645120 model_training_utils.py:450] Train Step: 19020/21936  / loss = 0.30785655975341797\n",
            "I0420 22:17:57.329551 139904526645120 model_training_utils.py:450] Train Step: 19021/21936  / loss = 1.1636041402816772\n",
            "I0420 22:17:57.867532 139904526645120 model_training_utils.py:450] Train Step: 19022/21936  / loss = 0.2658631205558777\n",
            "I0420 22:17:58.404222 139904526645120 model_training_utils.py:450] Train Step: 19023/21936  / loss = 1.7958729267120361\n",
            "I0420 22:17:58.938337 139904526645120 model_training_utils.py:450] Train Step: 19024/21936  / loss = 0.7115063071250916\n",
            "I0420 22:17:59.475236 139904526645120 model_training_utils.py:450] Train Step: 19025/21936  / loss = 1.0072473287582397\n",
            "I0420 22:18:00.013861 139904526645120 model_training_utils.py:450] Train Step: 19026/21936  / loss = 0.5362215638160706\n",
            "I0420 22:18:00.550184 139904526645120 model_training_utils.py:450] Train Step: 19027/21936  / loss = 0.39101868867874146\n",
            "I0420 22:18:01.093654 139904526645120 model_training_utils.py:450] Train Step: 19028/21936  / loss = 0.5754804015159607\n",
            "I0420 22:18:01.627214 139904526645120 model_training_utils.py:450] Train Step: 19029/21936  / loss = 0.46250200271606445\n",
            "I0420 22:18:02.164527 139904526645120 model_training_utils.py:450] Train Step: 19030/21936  / loss = 0.5567821860313416\n",
            "I0420 22:18:02.700828 139904526645120 model_training_utils.py:450] Train Step: 19031/21936  / loss = 0.7387596368789673\n",
            "I0420 22:18:03.235845 139904526645120 model_training_utils.py:450] Train Step: 19032/21936  / loss = 0.4009992182254791\n",
            "I0420 22:18:03.779708 139904526645120 model_training_utils.py:450] Train Step: 19033/21936  / loss = 1.3287501335144043\n",
            "I0420 22:18:04.316499 139904526645120 model_training_utils.py:450] Train Step: 19034/21936  / loss = 0.7849273085594177\n",
            "I0420 22:18:04.852305 139904526645120 model_training_utils.py:450] Train Step: 19035/21936  / loss = 0.4484688639640808\n",
            "I0420 22:18:05.389983 139904526645120 model_training_utils.py:450] Train Step: 19036/21936  / loss = 0.7418362498283386\n",
            "I0420 22:18:05.928307 139904526645120 model_training_utils.py:450] Train Step: 19037/21936  / loss = 1.251914381980896\n",
            "I0420 22:18:06.466703 139904526645120 model_training_utils.py:450] Train Step: 19038/21936  / loss = 0.8072378635406494\n",
            "I0420 22:18:07.001860 139904526645120 model_training_utils.py:450] Train Step: 19039/21936  / loss = 0.24569061398506165\n",
            "I0420 22:18:07.544856 139904526645120 model_training_utils.py:450] Train Step: 19040/21936  / loss = 0.5993494987487793\n",
            "I0420 22:18:08.086455 139904526645120 model_training_utils.py:450] Train Step: 19041/21936  / loss = 0.6280583739280701\n",
            "I0420 22:18:08.622695 139904526645120 model_training_utils.py:450] Train Step: 19042/21936  / loss = 0.3777928352355957\n",
            "I0420 22:18:09.164036 139904526645120 model_training_utils.py:450] Train Step: 19043/21936  / loss = 0.6119318008422852\n",
            "I0420 22:18:09.702125 139904526645120 model_training_utils.py:450] Train Step: 19044/21936  / loss = 0.19022946059703827\n",
            "I0420 22:18:10.238705 139904526645120 model_training_utils.py:450] Train Step: 19045/21936  / loss = 0.7409518957138062\n",
            "I0420 22:18:10.774873 139904526645120 model_training_utils.py:450] Train Step: 19046/21936  / loss = 0.3818556070327759\n",
            "I0420 22:18:11.317302 139904526645120 model_training_utils.py:450] Train Step: 19047/21936  / loss = 0.6160304546356201\n",
            "I0420 22:18:11.853722 139904526645120 model_training_utils.py:450] Train Step: 19048/21936  / loss = 0.17135876417160034\n",
            "I0420 22:18:12.390889 139904526645120 model_training_utils.py:450] Train Step: 19049/21936  / loss = 0.4992770552635193\n",
            "I0420 22:18:12.927456 139904526645120 model_training_utils.py:450] Train Step: 19050/21936  / loss = 0.9831646680831909\n",
            "I0420 22:18:13.462900 139904526645120 model_training_utils.py:450] Train Step: 19051/21936  / loss = 0.6852566003799438\n",
            "I0420 22:18:13.999079 139904526645120 model_training_utils.py:450] Train Step: 19052/21936  / loss = 0.3224733769893646\n",
            "I0420 22:18:14.533884 139904526645120 model_training_utils.py:450] Train Step: 19053/21936  / loss = 0.5294270515441895\n",
            "I0420 22:18:15.069102 139904526645120 model_training_utils.py:450] Train Step: 19054/21936  / loss = 0.671187162399292\n",
            "I0420 22:18:15.604100 139904526645120 model_training_utils.py:450] Train Step: 19055/21936  / loss = 0.813887357711792\n",
            "I0420 22:18:16.141698 139904526645120 model_training_utils.py:450] Train Step: 19056/21936  / loss = 0.17805886268615723\n",
            "I0420 22:18:16.677224 139904526645120 model_training_utils.py:450] Train Step: 19057/21936  / loss = 0.18528541922569275\n",
            "I0420 22:18:17.213362 139904526645120 model_training_utils.py:450] Train Step: 19058/21936  / loss = 0.31966930627822876\n",
            "I0420 22:18:17.748851 139904526645120 model_training_utils.py:450] Train Step: 19059/21936  / loss = 0.4909002184867859\n",
            "I0420 22:18:18.286218 139904526645120 model_training_utils.py:450] Train Step: 19060/21936  / loss = 0.654236376285553\n",
            "I0420 22:18:18.821374 139904526645120 model_training_utils.py:450] Train Step: 19061/21936  / loss = 0.5852192640304565\n",
            "I0420 22:18:19.356901 139904526645120 model_training_utils.py:450] Train Step: 19062/21936  / loss = 0.7127217054367065\n",
            "I0420 22:18:19.893165 139904526645120 model_training_utils.py:450] Train Step: 19063/21936  / loss = 0.3698616623878479\n",
            "I0420 22:18:20.429510 139904526645120 model_training_utils.py:450] Train Step: 19064/21936  / loss = 0.9902651309967041\n",
            "I0420 22:18:20.966246 139904526645120 model_training_utils.py:450] Train Step: 19065/21936  / loss = 0.2890312969684601\n",
            "I0420 22:18:21.500680 139904526645120 model_training_utils.py:450] Train Step: 19066/21936  / loss = 0.9305399656295776\n",
            "I0420 22:18:22.042006 139904526645120 model_training_utils.py:450] Train Step: 19067/21936  / loss = 0.5242540836334229\n",
            "I0420 22:18:22.579192 139904526645120 model_training_utils.py:450] Train Step: 19068/21936  / loss = 0.20173010230064392\n",
            "I0420 22:18:23.116085 139904526645120 keras_utils.py:122] TimeHistory: 26.86 seconds, 14.89 examples/second between steps 29987 and 30037\n",
            "I0420 22:18:23.119058 139904526645120 model_training_utils.py:450] Train Step: 19069/21936  / loss = 0.29016798734664917\n",
            "I0420 22:18:23.656182 139904526645120 model_training_utils.py:450] Train Step: 19070/21936  / loss = 0.5445520877838135\n",
            "I0420 22:18:24.192829 139904526645120 model_training_utils.py:450] Train Step: 19071/21936  / loss = 0.22411328554153442\n",
            "I0420 22:18:24.726991 139904526645120 model_training_utils.py:450] Train Step: 19072/21936  / loss = 0.322713702917099\n",
            "I0420 22:18:25.271528 139904526645120 model_training_utils.py:450] Train Step: 19073/21936  / loss = 0.6094558238983154\n",
            "I0420 22:18:25.806643 139904526645120 model_training_utils.py:450] Train Step: 19074/21936  / loss = 0.09039030969142914\n",
            "I0420 22:18:26.342942 139904526645120 model_training_utils.py:450] Train Step: 19075/21936  / loss = 0.2898983061313629\n",
            "I0420 22:18:26.880167 139904526645120 model_training_utils.py:450] Train Step: 19076/21936  / loss = 0.8432518839836121\n",
            "I0420 22:18:27.416435 139904526645120 model_training_utils.py:450] Train Step: 19077/21936  / loss = 0.1600470244884491\n",
            "I0420 22:18:27.958074 139904526645120 model_training_utils.py:450] Train Step: 19078/21936  / loss = 0.05717073753476143\n",
            "I0420 22:18:28.496487 139904526645120 model_training_utils.py:450] Train Step: 19079/21936  / loss = 0.9914760589599609\n",
            "I0420 22:18:29.031620 139904526645120 model_training_utils.py:450] Train Step: 19080/21936  / loss = 0.33928215503692627\n",
            "I0420 22:18:29.569017 139904526645120 model_training_utils.py:450] Train Step: 19081/21936  / loss = 0.28130248188972473\n",
            "I0420 22:18:30.107060 139904526645120 model_training_utils.py:450] Train Step: 19082/21936  / loss = 0.06940075755119324\n",
            "I0420 22:18:30.644275 139904526645120 model_training_utils.py:450] Train Step: 19083/21936  / loss = 0.16643047332763672\n",
            "I0420 22:18:31.181013 139904526645120 model_training_utils.py:450] Train Step: 19084/21936  / loss = 0.5786839723587036\n",
            "I0420 22:18:31.717284 139904526645120 model_training_utils.py:450] Train Step: 19085/21936  / loss = 0.23572470247745514\n",
            "I0420 22:18:32.255430 139904526645120 model_training_utils.py:450] Train Step: 19086/21936  / loss = 0.15695495903491974\n",
            "I0420 22:18:32.792786 139904526645120 model_training_utils.py:450] Train Step: 19087/21936  / loss = 0.29379895329475403\n",
            "I0420 22:18:33.329530 139904526645120 model_training_utils.py:450] Train Step: 19088/21936  / loss = 0.24521902203559875\n",
            "I0420 22:18:33.867544 139904526645120 model_training_utils.py:450] Train Step: 19089/21936  / loss = 0.2786032557487488\n",
            "I0420 22:18:34.404945 139904526645120 model_training_utils.py:450] Train Step: 19090/21936  / loss = 0.05159347504377365\n",
            "I0420 22:18:34.939648 139904526645120 model_training_utils.py:450] Train Step: 19091/21936  / loss = 0.1236751452088356\n",
            "I0420 22:18:35.480847 139904526645120 model_training_utils.py:450] Train Step: 19092/21936  / loss = 0.062071628868579865\n",
            "I0420 22:18:36.015763 139904526645120 model_training_utils.py:450] Train Step: 19093/21936  / loss = 0.07752509415149689\n",
            "I0420 22:18:36.552634 139904526645120 model_training_utils.py:450] Train Step: 19094/21936  / loss = 0.26058802008628845\n",
            "I0420 22:18:37.089690 139904526645120 model_training_utils.py:450] Train Step: 19095/21936  / loss = 0.13723108172416687\n",
            "I0420 22:18:37.627631 139904526645120 model_training_utils.py:450] Train Step: 19096/21936  / loss = 0.2533983290195465\n",
            "I0420 22:18:38.163851 139904526645120 model_training_utils.py:450] Train Step: 19097/21936  / loss = 0.20483863353729248\n",
            "I0420 22:18:38.698485 139904526645120 model_training_utils.py:450] Train Step: 19098/21936  / loss = 0.12480857223272324\n",
            "I0420 22:18:39.236735 139904526645120 model_training_utils.py:450] Train Step: 19099/21936  / loss = 0.21279579401016235\n",
            "I0420 22:18:39.773132 139904526645120 model_training_utils.py:450] Train Step: 19100/21936  / loss = 0.35697174072265625\n",
            "I0420 22:18:40.308856 139904526645120 model_training_utils.py:450] Train Step: 19101/21936  / loss = 0.3097273111343384\n",
            "I0420 22:18:40.846109 139904526645120 model_training_utils.py:450] Train Step: 19102/21936  / loss = 0.5888190865516663\n",
            "I0420 22:18:41.386867 139904526645120 model_training_utils.py:450] Train Step: 19103/21936  / loss = 0.46942102909088135\n",
            "I0420 22:18:41.933909 139904526645120 model_training_utils.py:450] Train Step: 19104/21936  / loss = 0.44432321190834045\n",
            "I0420 22:18:42.472751 139904526645120 model_training_utils.py:450] Train Step: 19105/21936  / loss = 0.24220424890518188\n",
            "I0420 22:18:43.010093 139904526645120 model_training_utils.py:450] Train Step: 19106/21936  / loss = 0.2105734944343567\n",
            "I0420 22:18:43.548136 139904526645120 model_training_utils.py:450] Train Step: 19107/21936  / loss = 0.2415323257446289\n",
            "I0420 22:18:44.087123 139904526645120 model_training_utils.py:450] Train Step: 19108/21936  / loss = 0.16256827116012573\n",
            "I0420 22:18:44.623754 139904526645120 model_training_utils.py:450] Train Step: 19109/21936  / loss = 0.796929121017456\n",
            "I0420 22:18:45.159338 139904526645120 model_training_utils.py:450] Train Step: 19110/21936  / loss = 0.15312474966049194\n",
            "I0420 22:18:45.695864 139904526645120 model_training_utils.py:450] Train Step: 19111/21936  / loss = 0.08202759921550751\n",
            "I0420 22:18:46.230265 139904526645120 model_training_utils.py:450] Train Step: 19112/21936  / loss = 0.3357834815979004\n",
            "I0420 22:18:46.766629 139904526645120 model_training_utils.py:450] Train Step: 19113/21936  / loss = 0.2552335858345032\n",
            "I0420 22:18:47.303413 139904526645120 model_training_utils.py:450] Train Step: 19114/21936  / loss = 0.2050822675228119\n",
            "I0420 22:18:47.842447 139904526645120 model_training_utils.py:450] Train Step: 19115/21936  / loss = 0.08756659179925919\n",
            "I0420 22:18:48.377822 139904526645120 model_training_utils.py:450] Train Step: 19116/21936  / loss = 0.4838358759880066\n",
            "I0420 22:18:48.916069 139904526645120 model_training_utils.py:450] Train Step: 19117/21936  / loss = 0.4269326329231262\n",
            "I0420 22:18:49.462883 139904526645120 model_training_utils.py:450] Train Step: 19118/21936  / loss = 0.36755919456481934\n",
            "I0420 22:18:49.999070 139904526645120 keras_utils.py:122] TimeHistory: 26.88 seconds, 14.88 examples/second between steps 30037 and 30087\n",
            "I0420 22:18:50.001925 139904526645120 model_training_utils.py:450] Train Step: 19119/21936  / loss = 0.31533122062683105\n",
            "I0420 22:18:50.536428 139904526645120 model_training_utils.py:450] Train Step: 19120/21936  / loss = 0.17360228300094604\n",
            "I0420 22:18:51.074344 139904526645120 model_training_utils.py:450] Train Step: 19121/21936  / loss = 0.1979069858789444\n",
            "I0420 22:18:51.611438 139904526645120 model_training_utils.py:450] Train Step: 19122/21936  / loss = 0.4089173376560211\n",
            "I0420 22:18:52.149199 139904526645120 model_training_utils.py:450] Train Step: 19123/21936  / loss = 0.4350782632827759\n",
            "I0420 22:18:52.687866 139904526645120 model_training_utils.py:450] Train Step: 19124/21936  / loss = 0.23501792550086975\n",
            "I0420 22:18:53.223763 139904526645120 model_training_utils.py:450] Train Step: 19125/21936  / loss = 0.17473621666431427\n",
            "I0420 22:18:53.760731 139904526645120 model_training_utils.py:450] Train Step: 19126/21936  / loss = 0.19111773371696472\n",
            "I0420 22:18:54.298490 139904526645120 model_training_utils.py:450] Train Step: 19127/21936  / loss = 0.2231580913066864\n",
            "I0420 22:18:54.833878 139904526645120 model_training_utils.py:450] Train Step: 19128/21936  / loss = 0.4862779378890991\n",
            "I0420 22:18:55.369168 139904526645120 model_training_utils.py:450] Train Step: 19129/21936  / loss = 0.08893570303916931\n",
            "I0420 22:18:55.904390 139904526645120 model_training_utils.py:450] Train Step: 19130/21936  / loss = 0.03399910032749176\n",
            "I0420 22:18:56.443244 139904526645120 model_training_utils.py:450] Train Step: 19131/21936  / loss = 0.11987379193305969\n",
            "I0420 22:18:56.979961 139904526645120 model_training_utils.py:450] Train Step: 19132/21936  / loss = 0.13676995038986206\n",
            "I0420 22:18:57.517027 139904526645120 model_training_utils.py:450] Train Step: 19133/21936  / loss = 0.10223127901554108\n",
            "I0420 22:18:58.059544 139904526645120 model_training_utils.py:450] Train Step: 19134/21936  / loss = 0.24291616678237915\n",
            "I0420 22:18:58.594795 139904526645120 model_training_utils.py:450] Train Step: 19135/21936  / loss = 0.27830979228019714\n",
            "I0420 22:18:59.143588 139904526645120 model_training_utils.py:450] Train Step: 19136/21936  / loss = 0.22076788544654846\n",
            "I0420 22:18:59.682008 139904526645120 model_training_utils.py:450] Train Step: 19137/21936  / loss = 0.19560900330543518\n",
            "I0420 22:19:00.219981 139904526645120 model_training_utils.py:450] Train Step: 19138/21936  / loss = 0.20101502537727356\n",
            "I0420 22:19:00.759990 139904526645120 model_training_utils.py:450] Train Step: 19139/21936  / loss = 0.7840021848678589\n",
            "I0420 22:19:01.304199 139904526645120 model_training_utils.py:450] Train Step: 19140/21936  / loss = 0.026143904775381088\n",
            "I0420 22:19:01.839117 139904526645120 model_training_utils.py:450] Train Step: 19141/21936  / loss = 0.07149097323417664\n",
            "I0420 22:19:02.376873 139904526645120 model_training_utils.py:450] Train Step: 19142/21936  / loss = 0.29208609461784363\n",
            "I0420 22:19:02.915046 139904526645120 model_training_utils.py:450] Train Step: 19143/21936  / loss = 0.2758468985557556\n",
            "I0420 22:19:03.454180 139904526645120 model_training_utils.py:450] Train Step: 19144/21936  / loss = 0.14180463552474976\n",
            "I0420 22:19:03.989882 139904526645120 model_training_utils.py:450] Train Step: 19145/21936  / loss = 0.3244085907936096\n",
            "I0420 22:19:04.527064 139904526645120 model_training_utils.py:450] Train Step: 19146/21936  / loss = 0.35708898305892944\n",
            "I0420 22:19:05.062636 139904526645120 model_training_utils.py:450] Train Step: 19147/21936  / loss = 0.2623535394668579\n",
            "I0420 22:19:05.600364 139904526645120 model_training_utils.py:450] Train Step: 19148/21936  / loss = 0.056801687926054\n",
            "I0420 22:19:06.137004 139904526645120 model_training_utils.py:450] Train Step: 19149/21936  / loss = 0.14500094950199127\n",
            "I0420 22:19:06.675257 139904526645120 model_training_utils.py:450] Train Step: 19150/21936  / loss = 0.038517940789461136\n",
            "I0420 22:19:07.210453 139904526645120 model_training_utils.py:450] Train Step: 19151/21936  / loss = 0.21401457488536835\n",
            "I0420 22:19:07.746218 139904526645120 model_training_utils.py:450] Train Step: 19152/21936  / loss = 0.14653626084327698\n",
            "I0420 22:19:08.285046 139904526645120 model_training_utils.py:450] Train Step: 19153/21936  / loss = 0.4331321716308594\n",
            "I0420 22:19:08.825082 139904526645120 model_training_utils.py:450] Train Step: 19154/21936  / loss = 0.16465063393115997\n",
            "I0420 22:19:09.362444 139904526645120 model_training_utils.py:450] Train Step: 19155/21936  / loss = 0.3858959674835205\n",
            "I0420 22:19:09.899271 139904526645120 model_training_utils.py:450] Train Step: 19156/21936  / loss = 0.10059540718793869\n",
            "I0420 22:19:10.438783 139904526645120 model_training_utils.py:450] Train Step: 19157/21936  / loss = 0.045891717076301575\n",
            "I0420 22:19:10.977979 139904526645120 model_training_utils.py:450] Train Step: 19158/21936  / loss = 0.13242480158805847\n",
            "I0420 22:19:11.514890 139904526645120 model_training_utils.py:450] Train Step: 19159/21936  / loss = 0.27646130323410034\n",
            "I0420 22:19:12.054190 139904526645120 model_training_utils.py:450] Train Step: 19160/21936  / loss = 0.1286877691745758\n",
            "I0420 22:19:12.591279 139904526645120 model_training_utils.py:450] Train Step: 19161/21936  / loss = 0.11119013279676437\n",
            "I0420 22:19:13.127537 139904526645120 model_training_utils.py:450] Train Step: 19162/21936  / loss = 0.07075531780719757\n",
            "I0420 22:19:13.662927 139904526645120 model_training_utils.py:450] Train Step: 19163/21936  / loss = 0.2708960771560669\n",
            "I0420 22:19:14.199371 139904526645120 model_training_utils.py:450] Train Step: 19164/21936  / loss = 0.1959438920021057\n",
            "I0420 22:19:14.737500 139904526645120 model_training_utils.py:450] Train Step: 19165/21936  / loss = 0.17990490794181824\n",
            "I0420 22:19:15.279813 139904526645120 model_training_utils.py:450] Train Step: 19166/21936  / loss = 0.3976217210292816\n",
            "I0420 22:19:15.818819 139904526645120 model_training_utils.py:450] Train Step: 19167/21936  / loss = 0.21448539197444916\n",
            "I0420 22:19:16.355987 139904526645120 model_training_utils.py:450] Train Step: 19168/21936  / loss = 0.07500465214252472\n",
            "I0420 22:19:16.906012 139904526645120 keras_utils.py:122] TimeHistory: 26.90 seconds, 14.87 examples/second between steps 30087 and 30137\n",
            "I0420 22:19:16.908999 139904526645120 model_training_utils.py:450] Train Step: 19169/21936  / loss = 0.38722559809684753\n",
            "I0420 22:19:17.445740 139904526645120 model_training_utils.py:450] Train Step: 19170/21936  / loss = 0.011318625882267952\n",
            "I0420 22:19:17.983127 139904526645120 model_training_utils.py:450] Train Step: 19171/21936  / loss = 0.1861017644405365\n",
            "I0420 22:19:18.520802 139904526645120 model_training_utils.py:450] Train Step: 19172/21936  / loss = 0.6668416261672974\n",
            "I0420 22:19:19.058786 139904526645120 model_training_utils.py:450] Train Step: 19173/21936  / loss = 0.13412222266197205\n",
            "I0420 22:19:19.596132 139904526645120 model_training_utils.py:450] Train Step: 19174/21936  / loss = 0.7921988368034363\n",
            "I0420 22:19:20.132993 139904526645120 model_training_utils.py:450] Train Step: 19175/21936  / loss = 0.26661741733551025\n",
            "I0420 22:19:20.673256 139904526645120 model_training_utils.py:450] Train Step: 19176/21936  / loss = 0.09045341610908508\n",
            "I0420 22:19:21.214184 139904526645120 model_training_utils.py:450] Train Step: 19177/21936  / loss = 0.5849005579948425\n",
            "I0420 22:19:21.749370 139904526645120 model_training_utils.py:450] Train Step: 19178/21936  / loss = 0.07858582586050034\n",
            "I0420 22:19:22.286543 139904526645120 model_training_utils.py:450] Train Step: 19179/21936  / loss = 0.28229448199272156\n",
            "I0420 22:19:22.823771 139904526645120 model_training_utils.py:450] Train Step: 19180/21936  / loss = 0.06055336445569992\n",
            "I0420 22:19:23.361949 139904526645120 model_training_utils.py:450] Train Step: 19181/21936  / loss = 0.34061628580093384\n",
            "I0420 22:19:23.899644 139904526645120 model_training_utils.py:450] Train Step: 19182/21936  / loss = 0.04682736098766327\n",
            "I0420 22:19:24.442551 139904526645120 model_training_utils.py:450] Train Step: 19183/21936  / loss = 0.02562190219759941\n",
            "I0420 22:19:24.982170 139904526645120 model_training_utils.py:450] Train Step: 19184/21936  / loss = 0.49040576815605164\n",
            "I0420 22:19:25.516233 139904526645120 model_training_utils.py:450] Train Step: 19185/21936  / loss = 0.49965161085128784\n",
            "I0420 22:19:26.053483 139904526645120 model_training_utils.py:450] Train Step: 19186/21936  / loss = 0.01241399347782135\n",
            "I0420 22:19:26.591375 139904526645120 model_training_utils.py:450] Train Step: 19187/21936  / loss = 0.0833137184381485\n",
            "I0420 22:19:27.133085 139904526645120 model_training_utils.py:450] Train Step: 19188/21936  / loss = 0.032546497881412506\n",
            "I0420 22:19:27.668993 139904526645120 model_training_utils.py:450] Train Step: 19189/21936  / loss = 0.7111843228340149\n",
            "I0420 22:19:28.202582 139904526645120 model_training_utils.py:450] Train Step: 19190/21936  / loss = 0.3488140106201172\n",
            "I0420 22:19:28.742306 139904526645120 model_training_utils.py:450] Train Step: 19191/21936  / loss = 0.33098793029785156\n",
            "I0420 22:19:29.280205 139904526645120 model_training_utils.py:450] Train Step: 19192/21936  / loss = 0.14330771565437317\n",
            "I0420 22:19:29.816369 139904526645120 model_training_utils.py:450] Train Step: 19193/21936  / loss = 0.763660728931427\n",
            "I0420 22:19:30.352889 139904526645120 model_training_utils.py:450] Train Step: 19194/21936  / loss = 0.04005143418908119\n",
            "I0420 22:19:30.890396 139904526645120 model_training_utils.py:450] Train Step: 19195/21936  / loss = 0.03271518647670746\n",
            "I0420 22:19:31.435641 139904526645120 model_training_utils.py:450] Train Step: 19196/21936  / loss = 0.06398089230060577\n",
            "I0420 22:19:31.973454 139904526645120 model_training_utils.py:450] Train Step: 19197/21936  / loss = 0.9123396873474121\n",
            "I0420 22:19:32.510851 139904526645120 model_training_utils.py:450] Train Step: 19198/21936  / loss = 0.8265422582626343\n",
            "I0420 22:19:33.053393 139904526645120 model_training_utils.py:450] Train Step: 19199/21936  / loss = 0.06911894679069519\n",
            "I0420 22:19:33.590793 139904526645120 model_training_utils.py:450] Train Step: 19200/21936  / loss = 0.01102643832564354\n",
            "I0420 22:19:34.126279 139904526645120 model_training_utils.py:450] Train Step: 19201/21936  / loss = 0.11059652268886566\n",
            "I0420 22:19:34.660619 139904526645120 model_training_utils.py:450] Train Step: 19202/21936  / loss = 0.28949499130249023\n",
            "I0420 22:19:35.197993 139904526645120 model_training_utils.py:450] Train Step: 19203/21936  / loss = 0.06299323588609695\n",
            "I0420 22:19:35.740649 139904526645120 model_training_utils.py:450] Train Step: 19204/21936  / loss = 0.13940301537513733\n",
            "I0420 22:19:36.278881 139904526645120 model_training_utils.py:450] Train Step: 19205/21936  / loss = 0.15065275132656097\n",
            "I0420 22:19:36.814496 139904526645120 model_training_utils.py:450] Train Step: 19206/21936  / loss = 0.6176184415817261\n",
            "I0420 22:19:37.352849 139904526645120 model_training_utils.py:450] Train Step: 19207/21936  / loss = 0.2808464765548706\n",
            "I0420 22:19:37.888056 139904526645120 model_training_utils.py:450] Train Step: 19208/21936  / loss = 0.12475895136594772\n",
            "I0420 22:19:38.424081 139904526645120 model_training_utils.py:450] Train Step: 19209/21936  / loss = 0.01508641242980957\n",
            "I0420 22:19:38.961142 139904526645120 model_training_utils.py:450] Train Step: 19210/21936  / loss = 0.29171204566955566\n",
            "I0420 22:19:39.496150 139904526645120 model_training_utils.py:450] Train Step: 19211/21936  / loss = 0.0580892488360405\n",
            "I0420 22:19:40.032447 139904526645120 model_training_utils.py:450] Train Step: 19212/21936  / loss = 0.5585419535636902\n",
            "I0420 22:19:40.571039 139904526645120 model_training_utils.py:450] Train Step: 19213/21936  / loss = 0.48037123680114746\n",
            "I0420 22:19:41.109287 139904526645120 model_training_utils.py:450] Train Step: 19214/21936  / loss = 0.9870859384536743\n",
            "I0420 22:19:41.651901 139904526645120 model_training_utils.py:450] Train Step: 19215/21936  / loss = 0.5749021768569946\n",
            "I0420 22:19:42.189411 139904526645120 model_training_utils.py:450] Train Step: 19216/21936  / loss = 0.48749375343322754\n",
            "I0420 22:19:42.726654 139904526645120 model_training_utils.py:450] Train Step: 19217/21936  / loss = 0.5888590812683105\n",
            "I0420 22:19:43.264210 139904526645120 model_training_utils.py:450] Train Step: 19218/21936  / loss = 0.510560929775238\n",
            "I0420 22:19:43.800585 139904526645120 keras_utils.py:122] TimeHistory: 26.89 seconds, 14.88 examples/second between steps 30137 and 30187\n",
            "I0420 22:19:43.803243 139904526645120 model_training_utils.py:450] Train Step: 19219/21936  / loss = 1.1317715644836426\n",
            "I0420 22:19:44.341306 139904526645120 model_training_utils.py:450] Train Step: 19220/21936  / loss = 1.9002830982208252\n",
            "I0420 22:19:44.880399 139904526645120 model_training_utils.py:450] Train Step: 19221/21936  / loss = 0.9995821118354797\n",
            "I0420 22:19:45.418163 139904526645120 model_training_utils.py:450] Train Step: 19222/21936  / loss = 0.8518427610397339\n",
            "I0420 22:19:45.956459 139904526645120 model_training_utils.py:450] Train Step: 19223/21936  / loss = 1.8485937118530273\n",
            "I0420 22:19:46.494269 139904526645120 model_training_utils.py:450] Train Step: 19224/21936  / loss = 1.7827281951904297\n",
            "I0420 22:19:47.030601 139904526645120 model_training_utils.py:450] Train Step: 19225/21936  / loss = 2.1918935775756836\n",
            "I0420 22:19:47.573925 139904526645120 model_training_utils.py:450] Train Step: 19226/21936  / loss = 0.3252853751182556\n",
            "I0420 22:19:48.113979 139904526645120 model_training_utils.py:450] Train Step: 19227/21936  / loss = 1.1396085023880005\n",
            "I0420 22:19:48.658025 139904526645120 model_training_utils.py:450] Train Step: 19228/21936  / loss = 0.5656826496124268\n",
            "I0420 22:19:49.200677 139904526645120 model_training_utils.py:450] Train Step: 19229/21936  / loss = 0.13984550535678864\n",
            "I0420 22:19:49.740263 139904526645120 model_training_utils.py:450] Train Step: 19230/21936  / loss = 1.733264684677124\n",
            "I0420 22:19:50.277377 139904526645120 model_training_utils.py:450] Train Step: 19231/21936  / loss = 0.8859291076660156\n",
            "I0420 22:19:50.815221 139904526645120 model_training_utils.py:450] Train Step: 19232/21936  / loss = 0.7534121870994568\n",
            "I0420 22:19:51.350960 139904526645120 model_training_utils.py:450] Train Step: 19233/21936  / loss = 1.0366437435150146\n",
            "I0420 22:19:51.886746 139904526645120 model_training_utils.py:450] Train Step: 19234/21936  / loss = 0.20993800461292267\n",
            "I0420 22:19:52.424405 139904526645120 model_training_utils.py:450] Train Step: 19235/21936  / loss = 0.6329734325408936\n",
            "I0420 22:19:52.962538 139904526645120 model_training_utils.py:450] Train Step: 19236/21936  / loss = 0.28040796518325806\n",
            "I0420 22:19:53.499337 139904526645120 model_training_utils.py:450] Train Step: 19237/21936  / loss = 0.09751157462596893\n",
            "I0420 22:19:54.037062 139904526645120 model_training_utils.py:450] Train Step: 19238/21936  / loss = 0.45937734842300415\n",
            "I0420 22:19:54.575539 139904526645120 model_training_utils.py:450] Train Step: 19239/21936  / loss = 0.3775780200958252\n",
            "I0420 22:19:55.112672 139904526645120 model_training_utils.py:450] Train Step: 19240/21936  / loss = 0.8620867133140564\n",
            "I0420 22:19:55.648818 139904526645120 model_training_utils.py:450] Train Step: 19241/21936  / loss = 0.28716325759887695\n",
            "I0420 22:19:56.184199 139904526645120 model_training_utils.py:450] Train Step: 19242/21936  / loss = 0.7122749090194702\n",
            "I0420 22:19:56.732847 139904526645120 model_training_utils.py:450] Train Step: 19243/21936  / loss = 0.41641342639923096\n",
            "I0420 22:19:57.270224 139904526645120 model_training_utils.py:450] Train Step: 19244/21936  / loss = 0.9092836380004883\n",
            "I0420 22:19:57.806848 139904526645120 model_training_utils.py:450] Train Step: 19245/21936  / loss = 0.34906792640686035\n",
            "I0420 22:19:58.344772 139904526645120 model_training_utils.py:450] Train Step: 19246/21936  / loss = 0.6688168048858643\n",
            "I0420 22:19:58.882689 139904526645120 model_training_utils.py:450] Train Step: 19247/21936  / loss = 0.16550318896770477\n",
            "I0420 22:19:59.420199 139904526645120 model_training_utils.py:450] Train Step: 19248/21936  / loss = 0.3539849519729614\n",
            "I0420 22:19:59.956674 139904526645120 model_training_utils.py:450] Train Step: 19249/21936  / loss = 1.1290172338485718\n",
            "I0420 22:20:00.493545 139904526645120 model_training_utils.py:450] Train Step: 19250/21936  / loss = 0.3875938057899475\n",
            "I0420 22:20:01.030149 139904526645120 model_training_utils.py:450] Train Step: 19251/21936  / loss = 0.9910933971405029\n",
            "I0420 22:20:01.569977 139904526645120 model_training_utils.py:450] Train Step: 19252/21936  / loss = 1.056480050086975\n",
            "I0420 22:20:02.105164 139904526645120 model_training_utils.py:450] Train Step: 19253/21936  / loss = 0.4417461156845093\n",
            "I0420 22:20:02.642335 139904526645120 model_training_utils.py:450] Train Step: 19254/21936  / loss = 0.6035328507423401\n",
            "I0420 22:20:03.185328 139904526645120 model_training_utils.py:450] Train Step: 19255/21936  / loss = 0.49409860372543335\n",
            "I0420 22:20:03.727912 139904526645120 model_training_utils.py:450] Train Step: 19256/21936  / loss = 0.2534434199333191\n",
            "I0420 22:20:04.270926 139904526645120 model_training_utils.py:450] Train Step: 19257/21936  / loss = 0.25810861587524414\n",
            "I0420 22:20:04.809866 139904526645120 model_training_utils.py:450] Train Step: 19258/21936  / loss = 0.23805996775627136\n",
            "I0420 22:20:05.347996 139904526645120 model_training_utils.py:450] Train Step: 19259/21936  / loss = 0.13675878942012787\n",
            "I0420 22:20:05.883710 139904526645120 model_training_utils.py:450] Train Step: 19260/21936  / loss = 0.19260188937187195\n",
            "I0420 22:20:06.420377 139904526645120 model_training_utils.py:450] Train Step: 19261/21936  / loss = 0.43465185165405273\n",
            "I0420 22:20:06.956329 139904526645120 model_training_utils.py:450] Train Step: 19262/21936  / loss = 0.5055505037307739\n",
            "I0420 22:20:07.491971 139904526645120 model_training_utils.py:450] Train Step: 19263/21936  / loss = 0.7823326587677002\n",
            "I0420 22:20:08.031219 139904526645120 model_training_utils.py:450] Train Step: 19264/21936  / loss = 0.3007041811943054\n",
            "I0420 22:20:08.570967 139904526645120 model_training_utils.py:450] Train Step: 19265/21936  / loss = 0.11526258289813995\n",
            "I0420 22:20:09.115603 139904526645120 model_training_utils.py:450] Train Step: 19266/21936  / loss = 0.2261979877948761\n",
            "I0420 22:20:09.653003 139904526645120 model_training_utils.py:450] Train Step: 19267/21936  / loss = 0.6803954839706421\n",
            "I0420 22:20:10.191228 139904526645120 model_training_utils.py:450] Train Step: 19268/21936  / loss = 0.567681610584259\n",
            "I0420 22:20:10.728169 139904526645120 keras_utils.py:122] TimeHistory: 26.92 seconds, 14.86 examples/second between steps 30187 and 30237\n",
            "I0420 22:20:10.730916 139904526645120 model_training_utils.py:450] Train Step: 19269/21936  / loss = 1.6313751935958862\n",
            "I0420 22:20:11.266225 139904526645120 model_training_utils.py:450] Train Step: 19270/21936  / loss = 0.41460201144218445\n",
            "I0420 22:20:11.806460 139904526645120 model_training_utils.py:450] Train Step: 19271/21936  / loss = 0.3179563581943512\n",
            "I0420 22:20:12.345068 139904526645120 model_training_utils.py:450] Train Step: 19272/21936  / loss = 0.2547008991241455\n",
            "I0420 22:20:12.884365 139904526645120 model_training_utils.py:450] Train Step: 19273/21936  / loss = 0.11116629838943481\n",
            "I0420 22:20:13.418657 139904526645120 model_training_utils.py:450] Train Step: 19274/21936  / loss = 0.47775721549987793\n",
            "I0420 22:20:13.956535 139904526645120 model_training_utils.py:450] Train Step: 19275/21936  / loss = 0.5347667932510376\n",
            "I0420 22:20:14.494940 139904526645120 model_training_utils.py:450] Train Step: 19276/21936  / loss = 0.827663242816925\n",
            "I0420 22:20:15.031291 139904526645120 model_training_utils.py:450] Train Step: 19277/21936  / loss = 0.1228938177227974\n",
            "I0420 22:20:15.568931 139904526645120 model_training_utils.py:450] Train Step: 19278/21936  / loss = 0.1382807046175003\n",
            "I0420 22:20:16.104552 139904526645120 model_training_utils.py:450] Train Step: 19279/21936  / loss = 0.3809012174606323\n",
            "I0420 22:20:16.640715 139904526645120 model_training_utils.py:450] Train Step: 19280/21936  / loss = 0.7710520029067993\n",
            "I0420 22:20:17.178850 139904526645120 model_training_utils.py:450] Train Step: 19281/21936  / loss = 0.5042331218719482\n",
            "I0420 22:20:17.713840 139904526645120 model_training_utils.py:450] Train Step: 19282/21936  / loss = 0.310912549495697\n",
            "I0420 22:20:18.250047 139904526645120 model_training_utils.py:450] Train Step: 19283/21936  / loss = 0.3182518482208252\n",
            "I0420 22:20:18.786223 139904526645120 model_training_utils.py:450] Train Step: 19284/21936  / loss = 0.5902977585792542\n",
            "I0420 22:20:19.323117 139904526645120 model_training_utils.py:450] Train Step: 19285/21936  / loss = 1.230346441268921\n",
            "I0420 22:20:19.862080 139904526645120 model_training_utils.py:450] Train Step: 19286/21936  / loss = 0.9406898617744446\n",
            "I0420 22:20:20.396296 139904526645120 model_training_utils.py:450] Train Step: 19287/21936  / loss = 0.2525274157524109\n",
            "I0420 22:20:20.931868 139904526645120 model_training_utils.py:450] Train Step: 19288/21936  / loss = 0.5645056366920471\n",
            "I0420 22:20:21.467594 139904526645120 model_training_utils.py:450] Train Step: 19289/21936  / loss = 1.601759672164917\n",
            "I0420 22:20:22.005034 139904526645120 model_training_utils.py:450] Train Step: 19290/21936  / loss = 1.232088565826416\n",
            "I0420 22:20:22.541623 139904526645120 model_training_utils.py:450] Train Step: 19291/21936  / loss = 0.7624521255493164\n",
            "I0420 22:20:23.080912 139904526645120 model_training_utils.py:450] Train Step: 19292/21936  / loss = 0.97716224193573\n",
            "I0420 22:20:23.618503 139904526645120 model_training_utils.py:450] Train Step: 19293/21936  / loss = 1.4570361375808716\n",
            "I0420 22:20:24.155982 139904526645120 model_training_utils.py:450] Train Step: 19294/21936  / loss = 0.4264366328716278\n",
            "I0420 22:20:24.692948 139904526645120 model_training_utils.py:450] Train Step: 19295/21936  / loss = 0.22060896456241608\n",
            "I0420 22:20:25.232463 139904526645120 model_training_utils.py:450] Train Step: 19296/21936  / loss = 0.3978571593761444\n",
            "I0420 22:20:25.771297 139904526645120 model_training_utils.py:450] Train Step: 19297/21936  / loss = 1.2124193906784058\n",
            "I0420 22:20:26.311021 139904526645120 model_training_utils.py:450] Train Step: 19298/21936  / loss = 0.45144614577293396\n",
            "I0420 22:20:26.852632 139904526645120 model_training_utils.py:450] Train Step: 19299/21936  / loss = 0.6363692283630371\n",
            "I0420 22:20:27.389770 139904526645120 model_training_utils.py:450] Train Step: 19300/21936  / loss = 1.034510850906372\n",
            "I0420 22:20:27.931089 139904526645120 model_training_utils.py:450] Train Step: 19301/21936  / loss = 0.37365198135375977\n",
            "I0420 22:20:28.469512 139904526645120 model_training_utils.py:450] Train Step: 19302/21936  / loss = 0.5630934238433838\n",
            "I0420 22:20:29.007405 139904526645120 model_training_utils.py:450] Train Step: 19303/21936  / loss = 0.9330073595046997\n",
            "I0420 22:20:29.543767 139904526645120 model_training_utils.py:450] Train Step: 19304/21936  / loss = 0.5523039102554321\n",
            "I0420 22:20:30.079479 139904526645120 model_training_utils.py:450] Train Step: 19305/21936  / loss = 0.38130807876586914\n",
            "I0420 22:20:30.613437 139904526645120 model_training_utils.py:450] Train Step: 19306/21936  / loss = 0.6864148378372192\n",
            "I0420 22:20:31.151661 139904526645120 model_training_utils.py:450] Train Step: 19307/21936  / loss = 0.5903463363647461\n",
            "I0420 22:20:31.686652 139904526645120 model_training_utils.py:450] Train Step: 19308/21936  / loss = 0.5804866552352905\n",
            "I0420 22:20:32.223501 139904526645120 model_training_utils.py:450] Train Step: 19309/21936  / loss = 0.11971072852611542\n",
            "I0420 22:20:32.760285 139904526645120 model_training_utils.py:450] Train Step: 19310/21936  / loss = 0.24152502417564392\n",
            "I0420 22:20:33.297985 139904526645120 model_training_utils.py:450] Train Step: 19311/21936  / loss = 1.0327104330062866\n",
            "I0420 22:20:33.836641 139904526645120 model_training_utils.py:450] Train Step: 19312/21936  / loss = 0.1248718649148941\n",
            "I0420 22:20:34.379844 139904526645120 model_training_utils.py:450] Train Step: 19313/21936  / loss = 0.20088595151901245\n",
            "I0420 22:20:34.922758 139904526645120 model_training_utils.py:450] Train Step: 19314/21936  / loss = 0.25050806999206543\n",
            "I0420 22:20:35.460899 139904526645120 model_training_utils.py:450] Train Step: 19315/21936  / loss = 0.583381712436676\n",
            "I0420 22:20:35.997172 139904526645120 model_training_utils.py:450] Train Step: 19316/21936  / loss = 0.19524717330932617\n",
            "I0420 22:20:36.536717 139904526645120 model_training_utils.py:450] Train Step: 19317/21936  / loss = 0.08101782202720642\n",
            "I0420 22:20:37.075272 139904526645120 model_training_utils.py:450] Train Step: 19318/21936  / loss = 0.6079140901565552\n",
            "I0420 22:20:37.615583 139904526645120 keras_utils.py:122] TimeHistory: 26.88 seconds, 14.88 examples/second between steps 30237 and 30287\n",
            "I0420 22:20:37.618259 139904526645120 model_training_utils.py:450] Train Step: 19319/21936  / loss = 0.35513389110565186\n",
            "I0420 22:20:38.163419 139904526645120 model_training_utils.py:450] Train Step: 19320/21936  / loss = 0.17162826657295227\n",
            "I0420 22:20:38.699919 139904526645120 model_training_utils.py:450] Train Step: 19321/21936  / loss = 0.46175694465637207\n",
            "I0420 22:20:39.237675 139904526645120 model_training_utils.py:450] Train Step: 19322/21936  / loss = 0.12547236680984497\n",
            "I0420 22:20:39.776039 139904526645120 model_training_utils.py:450] Train Step: 19323/21936  / loss = 0.30135834217071533\n",
            "I0420 22:20:40.314797 139904526645120 model_training_utils.py:450] Train Step: 19324/21936  / loss = 0.9508779048919678\n",
            "I0420 22:20:40.854000 139904526645120 model_training_utils.py:450] Train Step: 19325/21936  / loss = 0.3141358196735382\n",
            "I0420 22:20:41.396513 139904526645120 model_training_utils.py:450] Train Step: 19326/21936  / loss = 0.9918922781944275\n",
            "I0420 22:20:41.940053 139904526645120 model_training_utils.py:450] Train Step: 19327/21936  / loss = 0.37444016337394714\n",
            "I0420 22:20:42.478843 139904526645120 model_training_utils.py:450] Train Step: 19328/21936  / loss = 0.7542638778686523\n",
            "I0420 22:20:43.019029 139904526645120 model_training_utils.py:450] Train Step: 19329/21936  / loss = 0.2835334241390228\n",
            "I0420 22:20:43.560128 139904526645120 model_training_utils.py:450] Train Step: 19330/21936  / loss = 0.259422242641449\n",
            "I0420 22:20:44.098379 139904526645120 model_training_utils.py:450] Train Step: 19331/21936  / loss = 1.7671639919281006\n",
            "I0420 22:20:44.638220 139904526645120 model_training_utils.py:450] Train Step: 19332/21936  / loss = 0.12852877378463745\n",
            "I0420 22:20:45.178581 139904526645120 model_training_utils.py:450] Train Step: 19333/21936  / loss = 0.407170832157135\n",
            "I0420 22:20:45.715795 139904526645120 model_training_utils.py:450] Train Step: 19334/21936  / loss = 0.5201254487037659\n",
            "I0420 22:20:46.250905 139904526645120 model_training_utils.py:450] Train Step: 19335/21936  / loss = 0.14515763521194458\n",
            "I0420 22:20:46.788583 139904526645120 model_training_utils.py:450] Train Step: 19336/21936  / loss = 0.49575474858283997\n",
            "I0420 22:20:47.328877 139904526645120 model_training_utils.py:450] Train Step: 19337/21936  / loss = 0.2938339114189148\n",
            "I0420 22:20:47.869220 139904526645120 model_training_utils.py:450] Train Step: 19338/21936  / loss = 0.20025129616260529\n",
            "I0420 22:20:48.406430 139904526645120 model_training_utils.py:450] Train Step: 19339/21936  / loss = 1.341626524925232\n",
            "I0420 22:20:48.941439 139904526645120 model_training_utils.py:450] Train Step: 19340/21936  / loss = 0.324505090713501\n",
            "I0420 22:20:49.478113 139904526645120 model_training_utils.py:450] Train Step: 19341/21936  / loss = 0.2460588812828064\n",
            "I0420 22:20:50.014825 139904526645120 model_training_utils.py:450] Train Step: 19342/21936  / loss = 0.4924849569797516\n",
            "I0420 22:20:50.557360 139904526645120 model_training_utils.py:450] Train Step: 19343/21936  / loss = 0.43968522548675537\n",
            "I0420 22:20:51.092756 139904526645120 model_training_utils.py:450] Train Step: 19344/21936  / loss = 0.7364933490753174\n",
            "I0420 22:20:51.630083 139904526645120 model_training_utils.py:450] Train Step: 19345/21936  / loss = 0.5126736164093018\n",
            "I0420 22:20:52.167895 139904526645120 model_training_utils.py:450] Train Step: 19346/21936  / loss = 0.44200748205184937\n",
            "I0420 22:20:52.704010 139904526645120 model_training_utils.py:450] Train Step: 19347/21936  / loss = 0.1381203532218933\n",
            "I0420 22:20:53.240768 139904526645120 model_training_utils.py:450] Train Step: 19348/21936  / loss = 0.13423766195774078\n",
            "I0420 22:20:53.777825 139904526645120 model_training_utils.py:450] Train Step: 19349/21936  / loss = 0.7753781676292419\n",
            "I0420 22:20:54.314157 139904526645120 model_training_utils.py:450] Train Step: 19350/21936  / loss = 0.07615814357995987\n",
            "I0420 22:20:54.852894 139904526645120 model_training_utils.py:450] Train Step: 19351/21936  / loss = 0.37406936287879944\n",
            "I0420 22:20:55.387897 139904526645120 model_training_utils.py:450] Train Step: 19352/21936  / loss = 1.1004085540771484\n",
            "I0420 22:20:55.934481 139904526645120 model_training_utils.py:450] Train Step: 19353/21936  / loss = 0.45159411430358887\n",
            "I0420 22:20:56.472912 139904526645120 model_training_utils.py:450] Train Step: 19354/21936  / loss = 0.6015224456787109\n",
            "I0420 22:20:57.009243 139904526645120 model_training_utils.py:450] Train Step: 19355/21936  / loss = 0.2971101701259613\n",
            "I0420 22:20:57.550310 139904526645120 model_training_utils.py:450] Train Step: 19356/21936  / loss = 0.32614773511886597\n",
            "I0420 22:20:58.087902 139904526645120 model_training_utils.py:450] Train Step: 19357/21936  / loss = 0.9595541954040527\n",
            "I0420 22:20:58.628514 139904526645120 model_training_utils.py:450] Train Step: 19358/21936  / loss = 0.6508542895317078\n",
            "I0420 22:20:59.168674 139904526645120 model_training_utils.py:450] Train Step: 19359/21936  / loss = 0.4675983190536499\n",
            "I0420 22:20:59.706231 139904526645120 model_training_utils.py:450] Train Step: 19360/21936  / loss = 0.1896015703678131\n",
            "I0420 22:21:00.244739 139904526645120 model_training_utils.py:450] Train Step: 19361/21936  / loss = 0.737794041633606\n",
            "I0420 22:21:00.782081 139904526645120 model_training_utils.py:450] Train Step: 19362/21936  / loss = 0.7355630397796631\n",
            "I0420 22:21:01.317611 139904526645120 model_training_utils.py:450] Train Step: 19363/21936  / loss = 0.6822075247764587\n",
            "I0420 22:21:01.854782 139904526645120 model_training_utils.py:450] Train Step: 19364/21936  / loss = 0.3638926148414612\n",
            "I0420 22:21:02.390913 139904526645120 model_training_utils.py:450] Train Step: 19365/21936  / loss = 0.3431697487831116\n",
            "I0420 22:21:02.928886 139904526645120 model_training_utils.py:450] Train Step: 19366/21936  / loss = 0.5392357707023621\n",
            "I0420 22:21:03.466514 139904526645120 model_training_utils.py:450] Train Step: 19367/21936  / loss = 0.3870662748813629\n",
            "I0420 22:21:04.004894 139904526645120 model_training_utils.py:450] Train Step: 19368/21936  / loss = 0.63148033618927\n",
            "I0420 22:21:04.540611 139904526645120 keras_utils.py:122] TimeHistory: 26.92 seconds, 14.86 examples/second between steps 30287 and 30337\n",
            "I0420 22:21:04.543142 139904526645120 model_training_utils.py:450] Train Step: 19369/21936  / loss = 0.3610801100730896\n",
            "I0420 22:21:05.079231 139904526645120 model_training_utils.py:450] Train Step: 19370/21936  / loss = 0.27022385597229004\n",
            "I0420 22:21:05.615881 139904526645120 model_training_utils.py:450] Train Step: 19371/21936  / loss = 0.44646507501602173\n",
            "I0420 22:21:06.152175 139904526645120 model_training_utils.py:450] Train Step: 19372/21936  / loss = 0.3745534420013428\n",
            "I0420 22:21:06.690613 139904526645120 model_training_utils.py:450] Train Step: 19373/21936  / loss = 0.5085741281509399\n",
            "I0420 22:21:07.227557 139904526645120 model_training_utils.py:450] Train Step: 19374/21936  / loss = 0.7324059009552002\n",
            "I0420 22:21:07.767528 139904526645120 model_training_utils.py:450] Train Step: 19375/21936  / loss = 0.49466919898986816\n",
            "I0420 22:21:08.304256 139904526645120 model_training_utils.py:450] Train Step: 19376/21936  / loss = 0.3383770287036896\n",
            "I0420 22:21:08.841129 139904526645120 model_training_utils.py:450] Train Step: 19377/21936  / loss = 0.30704110860824585\n",
            "I0420 22:21:09.377326 139904526645120 model_training_utils.py:450] Train Step: 19378/21936  / loss = 0.38981473445892334\n",
            "I0420 22:21:09.913043 139904526645120 model_training_utils.py:450] Train Step: 19379/21936  / loss = 0.24998213350772858\n",
            "I0420 22:21:10.452533 139904526645120 model_training_utils.py:450] Train Step: 19380/21936  / loss = 0.14652352035045624\n",
            "I0420 22:21:10.991048 139904526645120 model_training_utils.py:450] Train Step: 19381/21936  / loss = 0.7183773517608643\n",
            "I0420 22:21:11.528105 139904526645120 model_training_utils.py:450] Train Step: 19382/21936  / loss = 0.39551034569740295\n",
            "I0420 22:21:12.066842 139904526645120 model_training_utils.py:450] Train Step: 19383/21936  / loss = 0.447052538394928\n",
            "I0420 22:21:12.601847 139904526645120 model_training_utils.py:450] Train Step: 19384/21936  / loss = 0.6489927172660828\n",
            "I0420 22:21:13.138930 139904526645120 model_training_utils.py:450] Train Step: 19385/21936  / loss = 0.9175252914428711\n",
            "I0420 22:21:13.678645 139904526645120 model_training_utils.py:450] Train Step: 19386/21936  / loss = 0.44823896884918213\n",
            "I0420 22:21:14.217263 139904526645120 model_training_utils.py:450] Train Step: 19387/21936  / loss = 0.3158167004585266\n",
            "I0420 22:21:14.755098 139904526645120 model_training_utils.py:450] Train Step: 19388/21936  / loss = 0.637545108795166\n",
            "I0420 22:21:15.292764 139904526645120 model_training_utils.py:450] Train Step: 19389/21936  / loss = 0.1676633358001709\n",
            "I0420 22:21:15.831720 139904526645120 model_training_utils.py:450] Train Step: 19390/21936  / loss = 0.6916082501411438\n",
            "I0420 22:21:16.369030 139904526645120 model_training_utils.py:450] Train Step: 19391/21936  / loss = 0.4853878915309906\n",
            "I0420 22:21:16.905957 139904526645120 model_training_utils.py:450] Train Step: 19392/21936  / loss = 0.16333454847335815\n",
            "I0420 22:21:17.443355 139904526645120 model_training_utils.py:450] Train Step: 19393/21936  / loss = 1.0111048221588135\n",
            "I0420 22:21:17.979820 139904526645120 model_training_utils.py:450] Train Step: 19394/21936  / loss = 0.7781469225883484\n",
            "I0420 22:21:18.516794 139904526645120 model_training_utils.py:450] Train Step: 19395/21936  / loss = 0.6881726980209351\n",
            "I0420 22:21:19.059131 139904526645120 model_training_utils.py:450] Train Step: 19396/21936  / loss = 0.5401148796081543\n",
            "I0420 22:21:19.595755 139904526645120 model_training_utils.py:450] Train Step: 19397/21936  / loss = 0.22521737217903137\n",
            "I0420 22:21:20.133530 139904526645120 model_training_utils.py:450] Train Step: 19398/21936  / loss = 0.2119385302066803\n",
            "I0420 22:21:20.671890 139904526645120 model_training_utils.py:450] Train Step: 19399/21936  / loss = 0.2919742465019226\n",
            "I0420 22:21:21.211708 139904526645120 model_training_utils.py:450] Train Step: 19400/21936  / loss = 0.10108292102813721\n",
            "I0420 22:21:21.749498 139904526645120 model_training_utils.py:450] Train Step: 19401/21936  / loss = 0.2835862934589386\n",
            "I0420 22:21:22.287940 139904526645120 model_training_utils.py:450] Train Step: 19402/21936  / loss = 0.6027262806892395\n",
            "I0420 22:21:22.824292 139904526645120 model_training_utils.py:450] Train Step: 19403/21936  / loss = 0.06412619352340698\n",
            "I0420 22:21:23.361528 139904526645120 model_training_utils.py:450] Train Step: 19404/21936  / loss = 0.3188326358795166\n",
            "I0420 22:21:23.905664 139904526645120 model_training_utils.py:450] Train Step: 19405/21936  / loss = 0.2079860121011734\n",
            "I0420 22:21:24.443204 139904526645120 model_training_utils.py:450] Train Step: 19406/21936  / loss = 1.365188479423523\n",
            "I0420 22:21:24.978383 139904526645120 model_training_utils.py:450] Train Step: 19407/21936  / loss = 1.1753551959991455\n",
            "I0420 22:21:25.515047 139904526645120 model_training_utils.py:450] Train Step: 19408/21936  / loss = 0.1344870626926422\n",
            "I0420 22:21:26.053590 139904526645120 model_training_utils.py:450] Train Step: 19409/21936  / loss = 0.2749841511249542\n",
            "I0420 22:21:26.597099 139904526645120 model_training_utils.py:450] Train Step: 19410/21936  / loss = 0.6517461538314819\n",
            "I0420 22:21:27.136956 139904526645120 model_training_utils.py:450] Train Step: 19411/21936  / loss = 0.28493791818618774\n",
            "I0420 22:21:27.673000 139904526645120 model_training_utils.py:450] Train Step: 19412/21936  / loss = 0.7481712698936462\n",
            "I0420 22:21:28.217634 139904526645120 model_training_utils.py:450] Train Step: 19413/21936  / loss = 0.18318206071853638\n",
            "I0420 22:21:28.754257 139904526645120 model_training_utils.py:450] Train Step: 19414/21936  / loss = 0.23453962802886963\n",
            "I0420 22:21:29.290250 139904526645120 model_training_utils.py:450] Train Step: 19415/21936  / loss = 0.24488678574562073\n",
            "I0420 22:21:29.830183 139904526645120 model_training_utils.py:450] Train Step: 19416/21936  / loss = 0.11517021059989929\n",
            "I0420 22:21:30.367533 139904526645120 model_training_utils.py:450] Train Step: 19417/21936  / loss = 0.03744089603424072\n",
            "I0420 22:21:30.904247 139904526645120 model_training_utils.py:450] Train Step: 19418/21936  / loss = 0.9029840230941772\n",
            "I0420 22:21:31.442359 139904526645120 keras_utils.py:122] TimeHistory: 26.90 seconds, 14.87 examples/second between steps 30337 and 30387\n",
            "I0420 22:21:31.444914 139904526645120 model_training_utils.py:450] Train Step: 19419/21936  / loss = 0.6101935505867004\n",
            "I0420 22:21:31.982468 139904526645120 model_training_utils.py:450] Train Step: 19420/21936  / loss = 0.7049728631973267\n",
            "I0420 22:21:32.520486 139904526645120 model_training_utils.py:450] Train Step: 19421/21936  / loss = 0.016326185315847397\n",
            "I0420 22:21:33.057892 139904526645120 model_training_utils.py:450] Train Step: 19422/21936  / loss = 0.0303853340446949\n",
            "I0420 22:21:33.598082 139904526645120 model_training_utils.py:450] Train Step: 19423/21936  / loss = 0.05074833333492279\n",
            "I0420 22:21:34.137599 139904526645120 model_training_utils.py:450] Train Step: 19424/21936  / loss = 0.23765183985233307\n",
            "I0420 22:21:34.674289 139904526645120 model_training_utils.py:450] Train Step: 19425/21936  / loss = 0.5749675631523132\n",
            "I0420 22:21:35.211114 139904526645120 model_training_utils.py:450] Train Step: 19426/21936  / loss = 0.34965306520462036\n",
            "I0420 22:21:35.747946 139904526645120 model_training_utils.py:450] Train Step: 19427/21936  / loss = 0.6932946443557739\n",
            "I0420 22:21:36.282129 139904526645120 model_training_utils.py:450] Train Step: 19428/21936  / loss = 0.11591920256614685\n",
            "I0420 22:21:36.818465 139904526645120 model_training_utils.py:450] Train Step: 19429/21936  / loss = 0.44899365305900574\n",
            "I0420 22:21:37.358712 139904526645120 model_training_utils.py:450] Train Step: 19430/21936  / loss = 0.15335851907730103\n",
            "I0420 22:21:37.898195 139904526645120 model_training_utils.py:450] Train Step: 19431/21936  / loss = 0.11234074085950851\n",
            "I0420 22:21:38.433061 139904526645120 model_training_utils.py:450] Train Step: 19432/21936  / loss = 0.04168388992547989\n",
            "I0420 22:21:38.970828 139904526645120 model_training_utils.py:450] Train Step: 19433/21936  / loss = 0.055681925266981125\n",
            "I0420 22:21:39.507088 139904526645120 model_training_utils.py:450] Train Step: 19434/21936  / loss = 0.32126811146736145\n",
            "I0420 22:21:40.045496 139904526645120 model_training_utils.py:450] Train Step: 19435/21936  / loss = 0.4040306508541107\n",
            "I0420 22:21:40.581866 139904526645120 model_training_utils.py:450] Train Step: 19436/21936  / loss = 0.3857705593109131\n",
            "I0420 22:21:41.120863 139904526645120 model_training_utils.py:450] Train Step: 19437/21936  / loss = 0.1440303921699524\n",
            "I0420 22:21:41.658932 139904526645120 model_training_utils.py:450] Train Step: 19438/21936  / loss = 0.29287874698638916\n",
            "I0420 22:21:42.196341 139904526645120 model_training_utils.py:450] Train Step: 19439/21936  / loss = 0.1448509842157364\n",
            "I0420 22:21:42.733791 139904526645120 model_training_utils.py:450] Train Step: 19440/21936  / loss = 0.32998114824295044\n",
            "I0420 22:21:43.275682 139904526645120 model_training_utils.py:450] Train Step: 19441/21936  / loss = 0.25596678256988525\n",
            "I0420 22:21:43.813863 139904526645120 model_training_utils.py:450] Train Step: 19442/21936  / loss = 0.2931867837905884\n",
            "I0420 22:21:44.352953 139904526645120 model_training_utils.py:450] Train Step: 19443/21936  / loss = 0.39452266693115234\n",
            "I0420 22:21:44.889608 139904526645120 model_training_utils.py:450] Train Step: 19444/21936  / loss = 0.6983433961868286\n",
            "I0420 22:21:45.427446 139904526645120 model_training_utils.py:450] Train Step: 19445/21936  / loss = 0.37873944640159607\n",
            "I0420 22:21:45.966019 139904526645120 model_training_utils.py:450] Train Step: 19446/21936  / loss = 0.7021464705467224\n",
            "I0420 22:21:46.506435 139904526645120 model_training_utils.py:450] Train Step: 19447/21936  / loss = 0.133005753159523\n",
            "I0420 22:21:47.055246 139904526645120 model_training_utils.py:450] Train Step: 19448/21936  / loss = 0.968192458152771\n",
            "I0420 22:21:47.594205 139904526645120 model_training_utils.py:450] Train Step: 19449/21936  / loss = 0.28675633668899536\n",
            "I0420 22:21:48.133589 139904526645120 model_training_utils.py:450] Train Step: 19450/21936  / loss = 0.27368757128715515\n",
            "I0420 22:21:48.673275 139904526645120 model_training_utils.py:450] Train Step: 19451/21936  / loss = 0.5829828977584839\n",
            "I0420 22:21:49.215342 139904526645120 model_training_utils.py:450] Train Step: 19452/21936  / loss = 0.24198700487613678\n",
            "I0420 22:21:49.754318 139904526645120 model_training_utils.py:450] Train Step: 19453/21936  / loss = 0.28036126494407654\n",
            "I0420 22:21:50.291671 139904526645120 model_training_utils.py:450] Train Step: 19454/21936  / loss = 0.58113694190979\n",
            "I0420 22:21:50.830083 139904526645120 model_training_utils.py:450] Train Step: 19455/21936  / loss = 0.49567651748657227\n",
            "I0420 22:21:51.368043 139904526645120 model_training_utils.py:450] Train Step: 19456/21936  / loss = 0.4597015380859375\n",
            "I0420 22:21:51.903512 139904526645120 model_training_utils.py:450] Train Step: 19457/21936  / loss = 0.5347099900245667\n",
            "I0420 22:21:52.438549 139904526645120 model_training_utils.py:450] Train Step: 19458/21936  / loss = 0.5092154741287231\n",
            "I0420 22:21:52.977184 139904526645120 model_training_utils.py:450] Train Step: 19459/21936  / loss = 0.216795414686203\n",
            "I0420 22:21:53.514202 139904526645120 model_training_utils.py:450] Train Step: 19460/21936  / loss = 0.7226864695549011\n",
            "I0420 22:21:54.053096 139904526645120 model_training_utils.py:450] Train Step: 19461/21936  / loss = 0.9597643613815308\n",
            "I0420 22:21:54.589695 139904526645120 model_training_utils.py:450] Train Step: 19462/21936  / loss = 0.42002543807029724\n",
            "I0420 22:21:55.130187 139904526645120 model_training_utils.py:450] Train Step: 19463/21936  / loss = 0.4290977418422699\n",
            "I0420 22:21:55.671358 139904526645120 model_training_utils.py:450] Train Step: 19464/21936  / loss = 0.19458693265914917\n",
            "I0420 22:21:56.209126 139904526645120 model_training_utils.py:450] Train Step: 19465/21936  / loss = 0.41827234625816345\n",
            "I0420 22:21:56.748351 139904526645120 model_training_utils.py:450] Train Step: 19466/21936  / loss = 1.006881594657898\n",
            "I0420 22:21:57.287285 139904526645120 model_training_utils.py:450] Train Step: 19467/21936  / loss = 0.5107667446136475\n",
            "I0420 22:21:57.824924 139904526645120 model_training_utils.py:450] Train Step: 19468/21936  / loss = 1.0924816131591797\n",
            "I0420 22:21:58.362331 139904526645120 keras_utils.py:122] TimeHistory: 26.92 seconds, 14.86 examples/second between steps 30387 and 30437\n",
            "I0420 22:21:58.365112 139904526645120 model_training_utils.py:450] Train Step: 19469/21936  / loss = 0.4012944996356964\n",
            "I0420 22:21:58.903226 139904526645120 model_training_utils.py:450] Train Step: 19470/21936  / loss = 0.6756054162979126\n",
            "I0420 22:21:59.440782 139904526645120 model_training_utils.py:450] Train Step: 19471/21936  / loss = 0.8651340007781982\n",
            "I0420 22:21:59.976112 139904526645120 model_training_utils.py:450] Train Step: 19472/21936  / loss = 0.31448090076446533\n",
            "I0420 22:22:00.514091 139904526645120 model_training_utils.py:450] Train Step: 19473/21936  / loss = 0.45864808559417725\n",
            "I0420 22:22:01.050624 139904526645120 model_training_utils.py:450] Train Step: 19474/21936  / loss = 1.7847695350646973\n",
            "I0420 22:22:01.589251 139904526645120 model_training_utils.py:450] Train Step: 19475/21936  / loss = 0.5118219256401062\n",
            "I0420 22:22:02.127237 139904526645120 model_training_utils.py:450] Train Step: 19476/21936  / loss = 0.15390580892562866\n",
            "I0420 22:22:02.664712 139904526645120 model_training_utils.py:450] Train Step: 19477/21936  / loss = 0.9921396374702454\n",
            "I0420 22:22:03.200824 139904526645120 model_training_utils.py:450] Train Step: 19478/21936  / loss = 0.2629907727241516\n",
            "I0420 22:22:03.736418 139904526645120 model_training_utils.py:450] Train Step: 19479/21936  / loss = 0.4038117229938507\n",
            "I0420 22:22:04.274397 139904526645120 model_training_utils.py:450] Train Step: 19480/21936  / loss = 0.3934406638145447\n",
            "I0420 22:22:04.811417 139904526645120 model_training_utils.py:450] Train Step: 19481/21936  / loss = 0.562160849571228\n",
            "I0420 22:22:05.348006 139904526645120 model_training_utils.py:450] Train Step: 19482/21936  / loss = 0.38752615451812744\n",
            "I0420 22:22:05.883977 139904526645120 model_training_utils.py:450] Train Step: 19483/21936  / loss = 0.1966467797756195\n",
            "I0420 22:22:06.421689 139904526645120 model_training_utils.py:450] Train Step: 19484/21936  / loss = 0.5507247447967529\n",
            "I0420 22:22:06.959829 139904526645120 model_training_utils.py:450] Train Step: 19485/21936  / loss = 0.8350034356117249\n",
            "I0420 22:22:07.496868 139904526645120 model_training_utils.py:450] Train Step: 19486/21936  / loss = 0.26365870237350464\n",
            "I0420 22:22:08.033629 139904526645120 model_training_utils.py:450] Train Step: 19487/21936  / loss = 0.25945520401000977\n",
            "I0420 22:22:08.569643 139904526645120 model_training_utils.py:450] Train Step: 19488/21936  / loss = 1.5303022861480713\n",
            "I0420 22:22:09.110163 139904526645120 model_training_utils.py:450] Train Step: 19489/21936  / loss = 0.18594680726528168\n",
            "I0420 22:22:09.646362 139904526645120 model_training_utils.py:450] Train Step: 19490/21936  / loss = 0.0637868270277977\n",
            "I0420 22:22:10.182276 139904526645120 model_training_utils.py:450] Train Step: 19491/21936  / loss = 0.06708122044801712\n",
            "I0420 22:22:10.718815 139904526645120 model_training_utils.py:450] Train Step: 19492/21936  / loss = 0.10066019743680954\n",
            "I0420 22:22:11.254988 139904526645120 model_training_utils.py:450] Train Step: 19493/21936  / loss = 0.07340417802333832\n",
            "I0420 22:22:11.797345 139904526645120 model_training_utils.py:450] Train Step: 19494/21936  / loss = 0.2933136820793152\n",
            "I0420 22:22:12.334825 139904526645120 model_training_utils.py:450] Train Step: 19495/21936  / loss = 0.1999240219593048\n",
            "I0420 22:22:12.871921 139904526645120 model_training_utils.py:450] Train Step: 19496/21936  / loss = 0.8932203054428101\n",
            "I0420 22:22:13.408068 139904526645120 model_training_utils.py:450] Train Step: 19497/21936  / loss = 0.09809551388025284\n",
            "I0420 22:22:13.946019 139904526645120 model_training_utils.py:450] Train Step: 19498/21936  / loss = 0.3031173348426819\n",
            "I0420 22:22:14.481858 139904526645120 model_training_utils.py:450] Train Step: 19499/21936  / loss = 0.40195170044898987\n",
            "I0420 22:22:15.022559 139904526645120 model_training_utils.py:450] Train Step: 19500/21936  / loss = 0.3348451852798462\n",
            "I0420 22:22:15.559695 139904526645120 model_training_utils.py:450] Train Step: 19501/21936  / loss = 0.17898692190647125\n",
            "I0420 22:22:16.096359 139904526645120 model_training_utils.py:450] Train Step: 19502/21936  / loss = 0.9168477058410645\n",
            "I0420 22:22:16.634532 139904526645120 model_training_utils.py:450] Train Step: 19503/21936  / loss = 0.6700961589813232\n",
            "I0420 22:22:17.172302 139904526645120 model_training_utils.py:450] Train Step: 19504/21936  / loss = 0.5328322649002075\n",
            "I0420 22:22:17.708273 139904526645120 model_training_utils.py:450] Train Step: 19505/21936  / loss = 0.2802650034427643\n",
            "I0420 22:22:18.243922 139904526645120 model_training_utils.py:450] Train Step: 19506/21936  / loss = 0.180383563041687\n",
            "I0420 22:22:18.784790 139904526645120 model_training_utils.py:450] Train Step: 19507/21936  / loss = 0.09351978451013565\n",
            "I0420 22:22:19.319655 139904526645120 model_training_utils.py:450] Train Step: 19508/21936  / loss = 1.0075998306274414\n",
            "I0420 22:22:19.855179 139904526645120 model_training_utils.py:450] Train Step: 19509/21936  / loss = 0.47915613651275635\n",
            "I0420 22:22:20.391199 139904526645120 model_training_utils.py:450] Train Step: 19510/21936  / loss = 0.8228120803833008\n",
            "I0420 22:22:20.926899 139904526645120 model_training_utils.py:450] Train Step: 19511/21936  / loss = 0.8080739974975586\n",
            "I0420 22:22:21.463035 139904526645120 model_training_utils.py:450] Train Step: 19512/21936  / loss = 0.20427513122558594\n",
            "I0420 22:22:21.997545 139904526645120 model_training_utils.py:450] Train Step: 19513/21936  / loss = 0.3072049021720886\n",
            "I0420 22:22:22.538432 139904526645120 model_training_utils.py:450] Train Step: 19514/21936  / loss = 0.5337914824485779\n",
            "I0420 22:22:23.076197 139904526645120 model_training_utils.py:450] Train Step: 19515/21936  / loss = 0.4983534812927246\n",
            "I0420 22:22:23.612100 139904526645120 model_training_utils.py:450] Train Step: 19516/21936  / loss = 1.4899879693984985\n",
            "I0420 22:22:24.146943 139904526645120 model_training_utils.py:450] Train Step: 19517/21936  / loss = 0.514358401298523\n",
            "I0420 22:22:24.682631 139904526645120 model_training_utils.py:450] Train Step: 19518/21936  / loss = 2.1714539527893066\n",
            "I0420 22:22:25.218624 139904526645120 keras_utils.py:122] TimeHistory: 26.85 seconds, 14.90 examples/second between steps 30437 and 30487\n",
            "I0420 22:22:25.221272 139904526645120 model_training_utils.py:450] Train Step: 19519/21936  / loss = 0.6991000771522522\n",
            "I0420 22:22:25.756504 139904526645120 model_training_utils.py:450] Train Step: 19520/21936  / loss = 0.4948022961616516\n",
            "I0420 22:22:26.291451 139904526645120 model_training_utils.py:450] Train Step: 19521/21936  / loss = 0.6504366993904114\n",
            "I0420 22:22:26.829632 139904526645120 model_training_utils.py:450] Train Step: 19522/21936  / loss = 1.1259042024612427\n",
            "I0420 22:22:27.364551 139904526645120 model_training_utils.py:450] Train Step: 19523/21936  / loss = 0.3167802691459656\n",
            "I0420 22:22:27.899312 139904526645120 model_training_utils.py:450] Train Step: 19524/21936  / loss = 0.6812914609909058\n",
            "I0420 22:22:28.432920 139904526645120 model_training_utils.py:450] Train Step: 19525/21936  / loss = 0.08539216965436935\n",
            "I0420 22:22:28.972458 139904526645120 model_training_utils.py:450] Train Step: 19526/21936  / loss = 0.5007469058036804\n",
            "I0420 22:22:29.509065 139904526645120 model_training_utils.py:450] Train Step: 19527/21936  / loss = 1.3239262104034424\n",
            "I0420 22:22:30.044186 139904526645120 model_training_utils.py:450] Train Step: 19528/21936  / loss = 0.7972027659416199\n",
            "I0420 22:22:30.580848 139904526645120 model_training_utils.py:450] Train Step: 19529/21936  / loss = 0.4165946841239929\n",
            "I0420 22:22:31.120206 139904526645120 model_training_utils.py:450] Train Step: 19530/21936  / loss = 0.5037152767181396\n",
            "I0420 22:22:31.655828 139904526645120 model_training_utils.py:450] Train Step: 19531/21936  / loss = 0.43846362829208374\n",
            "I0420 22:22:32.202461 139904526645120 model_training_utils.py:450] Train Step: 19532/21936  / loss = 0.6611946225166321\n",
            "I0420 22:22:32.739444 139904526645120 model_training_utils.py:450] Train Step: 19533/21936  / loss = 0.2899028956890106\n",
            "I0420 22:22:33.275123 139904526645120 model_training_utils.py:450] Train Step: 19534/21936  / loss = 1.5007179975509644\n",
            "I0420 22:22:33.809171 139904526645120 model_training_utils.py:450] Train Step: 19535/21936  / loss = 1.311230182647705\n",
            "I0420 22:22:34.345033 139904526645120 model_training_utils.py:450] Train Step: 19536/21936  / loss = 0.8904273509979248\n",
            "I0420 22:22:34.881954 139904526645120 model_training_utils.py:450] Train Step: 19537/21936  / loss = 0.9090261459350586\n",
            "I0420 22:22:35.418875 139904526645120 model_training_utils.py:450] Train Step: 19538/21936  / loss = 0.5524682998657227\n",
            "I0420 22:22:35.955726 139904526645120 model_training_utils.py:450] Train Step: 19539/21936  / loss = 0.2837843894958496\n",
            "I0420 22:22:36.490147 139904526645120 model_training_utils.py:450] Train Step: 19540/21936  / loss = 0.14521802961826324\n",
            "I0420 22:22:37.033627 139904526645120 model_training_utils.py:450] Train Step: 19541/21936  / loss = 0.20559194684028625\n",
            "I0420 22:22:37.570111 139904526645120 model_training_utils.py:450] Train Step: 19542/21936  / loss = 0.9651596546173096\n",
            "I0420 22:22:38.106589 139904526645120 model_training_utils.py:450] Train Step: 19543/21936  / loss = 0.3849344849586487\n",
            "I0420 22:22:38.642230 139904526645120 model_training_utils.py:450] Train Step: 19544/21936  / loss = 0.3546390235424042\n",
            "I0420 22:22:39.182450 139904526645120 model_training_utils.py:450] Train Step: 19545/21936  / loss = 0.7091023921966553\n",
            "I0420 22:22:39.718867 139904526645120 model_training_utils.py:450] Train Step: 19546/21936  / loss = 0.33169281482696533\n",
            "I0420 22:22:40.258068 139904526645120 model_training_utils.py:450] Train Step: 19547/21936  / loss = 0.2317325323820114\n",
            "I0420 22:22:40.794223 139904526645120 model_training_utils.py:450] Train Step: 19548/21936  / loss = 0.4173840582370758\n",
            "I0420 22:22:41.332310 139904526645120 model_training_utils.py:450] Train Step: 19549/21936  / loss = 0.6786342859268188\n",
            "I0420 22:22:41.872467 139904526645120 model_training_utils.py:450] Train Step: 19550/21936  / loss = 0.6721641421318054\n",
            "I0420 22:22:42.414352 139904526645120 model_training_utils.py:450] Train Step: 19551/21936  / loss = 0.27680617570877075\n",
            "I0420 22:22:42.950604 139904526645120 model_training_utils.py:450] Train Step: 19552/21936  / loss = 0.14199218153953552\n",
            "I0420 22:22:43.488943 139904526645120 model_training_utils.py:450] Train Step: 19553/21936  / loss = 0.37011754512786865\n",
            "I0420 22:22:44.026241 139904526645120 model_training_utils.py:450] Train Step: 19554/21936  / loss = 0.15030936896800995\n",
            "I0420 22:22:44.560800 139904526645120 model_training_utils.py:450] Train Step: 19555/21936  / loss = 0.8525368571281433\n",
            "I0420 22:22:45.098720 139904526645120 model_training_utils.py:450] Train Step: 19556/21936  / loss = 0.5720188617706299\n",
            "I0420 22:22:45.636209 139904526645120 model_training_utils.py:450] Train Step: 19557/21936  / loss = 0.41557472944259644\n",
            "I0420 22:22:46.173176 139904526645120 model_training_utils.py:450] Train Step: 19558/21936  / loss = 0.15522585809230804\n",
            "I0420 22:22:46.709911 139904526645120 model_training_utils.py:450] Train Step: 19559/21936  / loss = 0.16798049211502075\n",
            "I0420 22:22:47.246633 139904526645120 model_training_utils.py:450] Train Step: 19560/21936  / loss = 0.6381750702857971\n",
            "I0420 22:22:47.785001 139904526645120 model_training_utils.py:450] Train Step: 19561/21936  / loss = 0.3104914426803589\n",
            "I0420 22:22:48.330341 139904526645120 model_training_utils.py:450] Train Step: 19562/21936  / loss = 0.47049906849861145\n",
            "I0420 22:22:48.865104 139904526645120 model_training_utils.py:450] Train Step: 19563/21936  / loss = 0.27159422636032104\n",
            "I0420 22:22:49.403325 139904526645120 model_training_utils.py:450] Train Step: 19564/21936  / loss = 0.6749805808067322\n",
            "I0420 22:22:49.938353 139904526645120 model_training_utils.py:450] Train Step: 19565/21936  / loss = 0.4538012444972992\n",
            "I0420 22:22:50.475949 139904526645120 model_training_utils.py:450] Train Step: 19566/21936  / loss = 0.09986831992864609\n",
            "I0420 22:22:51.014279 139904526645120 model_training_utils.py:450] Train Step: 19567/21936  / loss = 0.48202747106552124\n",
            "I0420 22:22:51.552417 139904526645120 model_training_utils.py:450] Train Step: 19568/21936  / loss = 0.310163676738739\n",
            "I0420 22:22:52.090560 139904526645120 keras_utils.py:122] TimeHistory: 26.87 seconds, 14.89 examples/second between steps 30487 and 30537\n",
            "I0420 22:22:52.093538 139904526645120 model_training_utils.py:450] Train Step: 19569/21936  / loss = 1.2500157356262207\n",
            "I0420 22:22:52.631073 139904526645120 model_training_utils.py:450] Train Step: 19570/21936  / loss = 0.17037653923034668\n",
            "I0420 22:22:53.169142 139904526645120 model_training_utils.py:450] Train Step: 19571/21936  / loss = 0.048208087682724\n",
            "I0420 22:22:53.706343 139904526645120 model_training_utils.py:450] Train Step: 19572/21936  / loss = 0.18881583213806152\n",
            "I0420 22:22:54.243367 139904526645120 model_training_utils.py:450] Train Step: 19573/21936  / loss = 0.6807376146316528\n",
            "I0420 22:22:54.780776 139904526645120 model_training_utils.py:450] Train Step: 19574/21936  / loss = 0.19748681783676147\n",
            "I0420 22:22:55.319763 139904526645120 model_training_utils.py:450] Train Step: 19575/21936  / loss = 0.3169384300708771\n",
            "I0420 22:22:55.858791 139904526645120 model_training_utils.py:450] Train Step: 19576/21936  / loss = 0.16857901215553284\n",
            "I0420 22:22:56.404198 139904526645120 model_training_utils.py:450] Train Step: 19577/21936  / loss = 0.6348382234573364\n",
            "I0420 22:22:56.938637 139904526645120 model_training_utils.py:450] Train Step: 19578/21936  / loss = 1.0263668298721313\n",
            "I0420 22:22:57.478994 139904526645120 model_training_utils.py:450] Train Step: 19579/21936  / loss = 0.10104702413082123\n",
            "I0420 22:22:58.015218 139904526645120 model_training_utils.py:450] Train Step: 19580/21936  / loss = 0.1236809492111206\n",
            "I0420 22:22:58.551848 139904526645120 model_training_utils.py:450] Train Step: 19581/21936  / loss = 1.0476033687591553\n",
            "I0420 22:22:59.089483 139904526645120 model_training_utils.py:450] Train Step: 19582/21936  / loss = 0.5739843845367432\n",
            "I0420 22:22:59.630913 139904526645120 model_training_utils.py:450] Train Step: 19583/21936  / loss = 0.38001635670661926\n",
            "I0420 22:23:00.168280 139904526645120 model_training_utils.py:450] Train Step: 19584/21936  / loss = 0.41061216592788696\n",
            "I0420 22:23:00.706419 139904526645120 model_training_utils.py:450] Train Step: 19585/21936  / loss = 0.3580669164657593\n",
            "I0420 22:23:01.242738 139904526645120 model_training_utils.py:450] Train Step: 19586/21936  / loss = 0.5075741410255432\n",
            "I0420 22:23:01.778486 139904526645120 model_training_utils.py:450] Train Step: 19587/21936  / loss = 0.6658366322517395\n",
            "I0420 22:23:02.313254 139904526645120 model_training_utils.py:450] Train Step: 19588/21936  / loss = 0.3695928156375885\n",
            "I0420 22:23:02.850618 139904526645120 model_training_utils.py:450] Train Step: 19589/21936  / loss = 0.2480367124080658\n",
            "I0420 22:23:03.391427 139904526645120 model_training_utils.py:450] Train Step: 19590/21936  / loss = 0.3449498116970062\n",
            "I0420 22:23:03.928216 139904526645120 model_training_utils.py:450] Train Step: 19591/21936  / loss = 0.5805407762527466\n",
            "I0420 22:23:04.465140 139904526645120 model_training_utils.py:450] Train Step: 19592/21936  / loss = 0.6199981570243835\n",
            "I0420 22:23:05.004286 139904526645120 model_training_utils.py:450] Train Step: 19593/21936  / loss = 0.22348296642303467\n",
            "I0420 22:23:05.542309 139904526645120 model_training_utils.py:450] Train Step: 19594/21936  / loss = 0.3196724057197571\n",
            "I0420 22:23:06.080074 139904526645120 model_training_utils.py:450] Train Step: 19595/21936  / loss = 0.7429841756820679\n",
            "I0420 22:23:06.618196 139904526645120 model_training_utils.py:450] Train Step: 19596/21936  / loss = 0.6546542048454285\n",
            "I0420 22:23:07.158637 139904526645120 model_training_utils.py:450] Train Step: 19597/21936  / loss = 0.645142674446106\n",
            "I0420 22:23:07.694474 139904526645120 model_training_utils.py:450] Train Step: 19598/21936  / loss = 0.12115339934825897\n",
            "I0420 22:23:08.228228 139904526645120 model_training_utils.py:450] Train Step: 19599/21936  / loss = 0.19938084483146667\n",
            "I0420 22:23:08.765224 139904526645120 model_training_utils.py:450] Train Step: 19600/21936  / loss = 0.4365276098251343\n",
            "I0420 22:23:09.303524 139904526645120 model_training_utils.py:450] Train Step: 19601/21936  / loss = 0.642807126045227\n",
            "I0420 22:23:09.841753 139904526645120 model_training_utils.py:450] Train Step: 19602/21936  / loss = 0.1597939133644104\n",
            "I0420 22:23:10.380249 139904526645120 model_training_utils.py:450] Train Step: 19603/21936  / loss = 0.36381617188453674\n",
            "I0420 22:23:10.918204 139904526645120 model_training_utils.py:450] Train Step: 19604/21936  / loss = 0.8955923318862915\n",
            "I0420 22:23:11.457254 139904526645120 model_training_utils.py:450] Train Step: 19605/21936  / loss = 0.5593544840812683\n",
            "I0420 22:23:11.996039 139904526645120 model_training_utils.py:450] Train Step: 19606/21936  / loss = 0.40142688155174255\n",
            "I0420 22:23:12.534097 139904526645120 model_training_utils.py:450] Train Step: 19607/21936  / loss = 0.31761547923088074\n",
            "I0420 22:23:13.073220 139904526645120 model_training_utils.py:450] Train Step: 19608/21936  / loss = 0.5246105194091797\n",
            "I0420 22:23:13.616851 139904526645120 model_training_utils.py:450] Train Step: 19609/21936  / loss = 0.6322829127311707\n",
            "I0420 22:23:14.153340 139904526645120 model_training_utils.py:450] Train Step: 19610/21936  / loss = 0.5202518701553345\n",
            "I0420 22:23:14.690657 139904526645120 model_training_utils.py:450] Train Step: 19611/21936  / loss = 0.48955708742141724\n",
            "I0420 22:23:15.226464 139904526645120 model_training_utils.py:450] Train Step: 19612/21936  / loss = 0.9662688374519348\n",
            "I0420 22:23:15.766642 139904526645120 model_training_utils.py:450] Train Step: 19613/21936  / loss = 0.4153660535812378\n",
            "I0420 22:23:16.305730 139904526645120 model_training_utils.py:450] Train Step: 19614/21936  / loss = 0.2804545760154724\n",
            "I0420 22:23:16.844127 139904526645120 model_training_utils.py:450] Train Step: 19615/21936  / loss = 1.0855629444122314\n",
            "I0420 22:23:17.381391 139904526645120 model_training_utils.py:450] Train Step: 19616/21936  / loss = 0.2153300940990448\n",
            "I0420 22:23:17.915929 139904526645120 model_training_utils.py:450] Train Step: 19617/21936  / loss = 0.1665056347846985\n",
            "I0420 22:23:18.452544 139904526645120 model_training_utils.py:450] Train Step: 19618/21936  / loss = 0.35633862018585205\n",
            "I0420 22:23:18.990926 139904526645120 keras_utils.py:122] TimeHistory: 26.90 seconds, 14.87 examples/second between steps 30537 and 30587\n",
            "I0420 22:23:18.993760 139904526645120 model_training_utils.py:450] Train Step: 19619/21936  / loss = 0.197675883769989\n",
            "I0420 22:23:19.530878 139904526645120 model_training_utils.py:450] Train Step: 19620/21936  / loss = 0.08573075383901596\n",
            "I0420 22:23:20.068795 139904526645120 model_training_utils.py:450] Train Step: 19621/21936  / loss = 0.6075854301452637\n",
            "I0420 22:23:20.607595 139904526645120 model_training_utils.py:450] Train Step: 19622/21936  / loss = 0.1232052594423294\n",
            "I0420 22:23:21.142203 139904526645120 model_training_utils.py:450] Train Step: 19623/21936  / loss = 0.36844602227211\n",
            "I0420 22:23:21.680125 139904526645120 model_training_utils.py:450] Train Step: 19624/21936  / loss = 0.2629089057445526\n",
            "I0420 22:23:22.214821 139904526645120 model_training_utils.py:450] Train Step: 19625/21936  / loss = 0.12287022918462753\n",
            "I0420 22:23:22.752229 139904526645120 model_training_utils.py:450] Train Step: 19626/21936  / loss = 0.1925802230834961\n",
            "I0420 22:23:23.296154 139904526645120 model_training_utils.py:450] Train Step: 19627/21936  / loss = 0.43229514360427856\n",
            "I0420 22:23:23.836232 139904526645120 model_training_utils.py:450] Train Step: 19628/21936  / loss = 0.5606001615524292\n",
            "I0420 22:23:24.380738 139904526645120 model_training_utils.py:450] Train Step: 19629/21936  / loss = 0.5587506890296936\n",
            "I0420 22:23:24.919262 139904526645120 model_training_utils.py:450] Train Step: 19630/21936  / loss = 0.6808605194091797\n",
            "I0420 22:23:25.455951 139904526645120 model_training_utils.py:450] Train Step: 19631/21936  / loss = 0.3471890389919281\n",
            "I0420 22:23:25.991950 139904526645120 model_training_utils.py:450] Train Step: 19632/21936  / loss = 0.8341225981712341\n",
            "I0420 22:23:26.530209 139904526645120 model_training_utils.py:450] Train Step: 19633/21936  / loss = 0.13462984561920166\n",
            "I0420 22:23:27.068058 139904526645120 model_training_utils.py:450] Train Step: 19634/21936  / loss = 0.04964009299874306\n",
            "I0420 22:23:27.605342 139904526645120 model_training_utils.py:450] Train Step: 19635/21936  / loss = 0.5076794028282166\n",
            "I0420 22:23:28.141232 139904526645120 model_training_utils.py:450] Train Step: 19636/21936  / loss = 0.09532743692398071\n",
            "I0420 22:23:28.675696 139904526645120 model_training_utils.py:450] Train Step: 19637/21936  / loss = 0.7010993361473083\n",
            "I0420 22:23:29.210383 139904526645120 model_training_utils.py:450] Train Step: 19638/21936  / loss = 1.201859712600708\n",
            "I0420 22:23:29.747452 139904526645120 model_training_utils.py:450] Train Step: 19639/21936  / loss = 0.35849833488464355\n",
            "I0420 22:23:30.284850 139904526645120 model_training_utils.py:450] Train Step: 19640/21936  / loss = 1.2984179258346558\n",
            "I0420 22:23:30.820680 139904526645120 model_training_utils.py:450] Train Step: 19641/21936  / loss = 0.6441137790679932\n",
            "I0420 22:23:31.358048 139904526645120 model_training_utils.py:450] Train Step: 19642/21936  / loss = 0.27888643741607666\n",
            "I0420 22:23:31.894481 139904526645120 model_training_utils.py:450] Train Step: 19643/21936  / loss = 0.10918623208999634\n",
            "I0420 22:23:32.431596 139904526645120 model_training_utils.py:450] Train Step: 19644/21936  / loss = 0.6277930736541748\n",
            "I0420 22:23:32.970226 139904526645120 model_training_utils.py:450] Train Step: 19645/21936  / loss = 0.08935081213712692\n",
            "I0420 22:23:33.508046 139904526645120 model_training_utils.py:450] Train Step: 19646/21936  / loss = 0.737835168838501\n",
            "I0420 22:23:34.045033 139904526645120 model_training_utils.py:450] Train Step: 19647/21936  / loss = 0.8643604516983032\n",
            "I0420 22:23:34.582530 139904526645120 model_training_utils.py:450] Train Step: 19648/21936  / loss = 0.5824824571609497\n",
            "I0420 22:23:35.120081 139904526645120 model_training_utils.py:450] Train Step: 19649/21936  / loss = 0.7499867677688599\n",
            "I0420 22:23:35.656051 139904526645120 model_training_utils.py:450] Train Step: 19650/21936  / loss = 0.1766672432422638\n",
            "I0420 22:23:36.201156 139904526645120 model_training_utils.py:450] Train Step: 19651/21936  / loss = 0.4830278754234314\n",
            "I0420 22:23:36.746790 139904526645120 model_training_utils.py:450] Train Step: 19652/21936  / loss = 0.8182837963104248\n",
            "I0420 22:23:37.284477 139904526645120 model_training_utils.py:450] Train Step: 19653/21936  / loss = 0.4578458070755005\n",
            "I0420 22:23:37.823695 139904526645120 model_training_utils.py:450] Train Step: 19654/21936  / loss = 0.3202114701271057\n",
            "I0420 22:23:38.360765 139904526645120 model_training_utils.py:450] Train Step: 19655/21936  / loss = 0.6026970148086548\n",
            "I0420 22:23:38.896380 139904526645120 model_training_utils.py:450] Train Step: 19656/21936  / loss = 1.006446361541748\n",
            "I0420 22:23:39.433005 139904526645120 model_training_utils.py:450] Train Step: 19657/21936  / loss = 0.18389323353767395\n",
            "I0420 22:23:39.969822 139904526645120 model_training_utils.py:450] Train Step: 19658/21936  / loss = 0.42507702112197876\n",
            "I0420 22:23:40.506199 139904526645120 model_training_utils.py:450] Train Step: 19659/21936  / loss = 0.2769317030906677\n",
            "I0420 22:23:41.041045 139904526645120 model_training_utils.py:450] Train Step: 19660/21936  / loss = 0.4134901463985443\n",
            "I0420 22:23:41.581831 139904526645120 model_training_utils.py:450] Train Step: 19661/21936  / loss = 0.24780547618865967\n",
            "I0420 22:23:42.120234 139904526645120 model_training_utils.py:450] Train Step: 19662/21936  / loss = 0.7121284008026123\n",
            "I0420 22:23:42.657124 139904526645120 model_training_utils.py:450] Train Step: 19663/21936  / loss = 0.32422515749931335\n",
            "I0420 22:23:43.193093 139904526645120 model_training_utils.py:450] Train Step: 19664/21936  / loss = 0.12420888990163803\n",
            "I0420 22:23:43.727482 139904526645120 model_training_utils.py:450] Train Step: 19665/21936  / loss = 0.4875369966030121\n",
            "I0420 22:23:44.263854 139904526645120 model_training_utils.py:450] Train Step: 19666/21936  / loss = 0.20610712468624115\n",
            "I0420 22:23:44.800648 139904526645120 model_training_utils.py:450] Train Step: 19667/21936  / loss = 0.35085105895996094\n",
            "I0420 22:23:45.338514 139904526645120 model_training_utils.py:450] Train Step: 19668/21936  / loss = 0.28849780559539795\n",
            "I0420 22:23:45.874010 139904526645120 keras_utils.py:122] TimeHistory: 26.88 seconds, 14.88 examples/second between steps 30587 and 30637\n",
            "I0420 22:23:45.882660 139904526645120 model_training_utils.py:450] Train Step: 19669/21936  / loss = 0.9391134977340698\n",
            "I0420 22:23:46.419706 139904526645120 model_training_utils.py:450] Train Step: 19670/21936  / loss = 0.18822436034679413\n",
            "I0420 22:23:46.954267 139904526645120 model_training_utils.py:450] Train Step: 19671/21936  / loss = 0.7673986554145813\n",
            "I0420 22:23:47.489173 139904526645120 model_training_utils.py:450] Train Step: 19672/21936  / loss = 0.28645384311676025\n",
            "I0420 22:23:48.026841 139904526645120 model_training_utils.py:450] Train Step: 19673/21936  / loss = 0.3861894905567169\n",
            "I0420 22:23:48.566820 139904526645120 model_training_utils.py:450] Train Step: 19674/21936  / loss = 0.8517199754714966\n",
            "I0420 22:23:49.103945 139904526645120 model_training_utils.py:450] Train Step: 19675/21936  / loss = 0.2825409770011902\n",
            "I0420 22:23:49.646181 139904526645120 model_training_utils.py:450] Train Step: 19676/21936  / loss = 0.5614767670631409\n",
            "I0420 22:23:50.183676 139904526645120 model_training_utils.py:450] Train Step: 19677/21936  / loss = 0.8896109461784363\n",
            "I0420 22:23:50.718848 139904526645120 model_training_utils.py:450] Train Step: 19678/21936  / loss = 0.43805065751075745\n",
            "I0420 22:23:51.254764 139904526645120 model_training_utils.py:450] Train Step: 19679/21936  / loss = 0.48075583577156067\n",
            "I0420 22:23:51.792937 139904526645120 model_training_utils.py:450] Train Step: 19680/21936  / loss = 1.673410415649414\n",
            "I0420 22:23:52.329538 139904526645120 model_training_utils.py:450] Train Step: 19681/21936  / loss = 0.5106009244918823\n",
            "I0420 22:23:52.867281 139904526645120 model_training_utils.py:450] Train Step: 19682/21936  / loss = 0.51799076795578\n",
            "I0420 22:23:53.404058 139904526645120 model_training_utils.py:450] Train Step: 19683/21936  / loss = 0.9667324423789978\n",
            "I0420 22:23:53.942253 139904526645120 model_training_utils.py:450] Train Step: 19684/21936  / loss = 0.29777467250823975\n",
            "I0420 22:23:54.481731 139904526645120 model_training_utils.py:450] Train Step: 19685/21936  / loss = 0.429885596036911\n",
            "I0420 22:23:55.020093 139904526645120 model_training_utils.py:450] Train Step: 19686/21936  / loss = 0.33354589343070984\n",
            "I0420 22:23:55.557701 139904526645120 model_training_utils.py:450] Train Step: 19687/21936  / loss = 0.5198061466217041\n",
            "I0420 22:23:56.095509 139904526645120 model_training_utils.py:450] Train Step: 19688/21936  / loss = 0.17786860466003418\n",
            "I0420 22:23:56.636498 139904526645120 model_training_utils.py:450] Train Step: 19689/21936  / loss = 0.25866466760635376\n",
            "I0420 22:23:57.171727 139904526645120 model_training_utils.py:450] Train Step: 19690/21936  / loss = 0.3594302237033844\n",
            "I0420 22:23:57.721965 139904526645120 model_training_utils.py:450] Train Step: 19691/21936  / loss = 0.24301636219024658\n",
            "I0420 22:23:58.260050 139904526645120 model_training_utils.py:450] Train Step: 19692/21936  / loss = 0.3397020101547241\n",
            "I0420 22:23:58.797523 139904526645120 model_training_utils.py:450] Train Step: 19693/21936  / loss = 0.3111676573753357\n",
            "I0420 22:23:59.331770 139904526645120 model_training_utils.py:450] Train Step: 19694/21936  / loss = 0.03991894796490669\n",
            "I0420 22:23:59.867376 139904526645120 model_training_utils.py:450] Train Step: 19695/21936  / loss = 0.3636566996574402\n",
            "I0420 22:24:00.406922 139904526645120 model_training_utils.py:450] Train Step: 19696/21936  / loss = 0.4531393051147461\n",
            "I0420 22:24:00.944645 139904526645120 model_training_utils.py:450] Train Step: 19697/21936  / loss = 0.815295934677124\n",
            "I0420 22:24:01.480838 139904526645120 model_training_utils.py:450] Train Step: 19698/21936  / loss = 0.03725931793451309\n",
            "I0420 22:24:02.016591 139904526645120 model_training_utils.py:450] Train Step: 19699/21936  / loss = 0.09027460217475891\n",
            "I0420 22:24:02.553883 139904526645120 model_training_utils.py:450] Train Step: 19700/21936  / loss = 0.20369692146778107\n",
            "I0420 22:24:03.091271 139904526645120 model_training_utils.py:450] Train Step: 19701/21936  / loss = 0.3858027756214142\n",
            "I0420 22:24:03.628382 139904526645120 model_training_utils.py:450] Train Step: 19702/21936  / loss = 0.11983907222747803\n",
            "I0420 22:24:04.167246 139904526645120 model_training_utils.py:450] Train Step: 19703/21936  / loss = 0.7432315349578857\n",
            "I0420 22:24:04.702638 139904526645120 model_training_utils.py:450] Train Step: 19704/21936  / loss = 0.4459015727043152\n",
            "I0420 22:24:05.247477 139904526645120 model_training_utils.py:450] Train Step: 19705/21936  / loss = 0.21387779712677002\n",
            "I0420 22:24:05.784361 139904526645120 model_training_utils.py:450] Train Step: 19706/21936  / loss = 1.1038275957107544\n",
            "I0420 22:24:06.320974 139904526645120 model_training_utils.py:450] Train Step: 19707/21936  / loss = 0.12767449021339417\n",
            "I0420 22:24:06.859610 139904526645120 model_training_utils.py:450] Train Step: 19708/21936  / loss = 0.3255392014980316\n",
            "I0420 22:24:07.398280 139904526645120 model_training_utils.py:450] Train Step: 19709/21936  / loss = 0.09926271438598633\n",
            "I0420 22:24:07.937079 139904526645120 model_training_utils.py:450] Train Step: 19710/21936  / loss = 0.3021199703216553\n",
            "I0420 22:24:08.474256 139904526645120 model_training_utils.py:450] Train Step: 19711/21936  / loss = 0.21959729492664337\n",
            "I0420 22:24:09.010501 139904526645120 model_training_utils.py:450] Train Step: 19712/21936  / loss = 0.14006003737449646\n",
            "I0420 22:24:09.549810 139904526645120 model_training_utils.py:450] Train Step: 19713/21936  / loss = 0.37975063920021057\n",
            "I0420 22:24:10.087208 139904526645120 model_training_utils.py:450] Train Step: 19714/21936  / loss = 0.3148506283760071\n",
            "I0420 22:24:10.623799 139904526645120 model_training_utils.py:450] Train Step: 19715/21936  / loss = 0.2307426929473877\n",
            "I0420 22:24:11.162478 139904526645120 model_training_utils.py:450] Train Step: 19716/21936  / loss = 0.22648799419403076\n",
            "I0420 22:24:11.699259 139904526645120 model_training_utils.py:450] Train Step: 19717/21936  / loss = 0.20589987933635712\n",
            "I0420 22:24:12.239175 139904526645120 model_training_utils.py:450] Train Step: 19718/21936  / loss = 0.7630982398986816\n",
            "I0420 22:24:12.775510 139904526645120 keras_utils.py:122] TimeHistory: 26.89 seconds, 14.87 examples/second between steps 30637 and 30687\n",
            "I0420 22:24:12.778140 139904526645120 model_training_utils.py:450] Train Step: 19719/21936  / loss = 0.3692864179611206\n",
            "I0420 22:24:13.313861 139904526645120 model_training_utils.py:450] Train Step: 19720/21936  / loss = 0.08092918992042542\n",
            "I0420 22:24:13.849341 139904526645120 model_training_utils.py:450] Train Step: 19721/21936  / loss = 0.20357245206832886\n",
            "I0420 22:24:14.387448 139904526645120 model_training_utils.py:450] Train Step: 19722/21936  / loss = 0.3824014961719513\n",
            "I0420 22:24:14.927955 139904526645120 model_training_utils.py:450] Train Step: 19723/21936  / loss = 0.7029182314872742\n",
            "I0420 22:24:15.465153 139904526645120 model_training_utils.py:450] Train Step: 19724/21936  / loss = 0.4656035304069519\n",
            "I0420 22:24:16.003824 139904526645120 model_training_utils.py:450] Train Step: 19725/21936  / loss = 0.033593207597732544\n",
            "I0420 22:24:16.542057 139904526645120 model_training_utils.py:450] Train Step: 19726/21936  / loss = 0.10998313128948212\n",
            "I0420 22:24:17.079405 139904526645120 model_training_utils.py:450] Train Step: 19727/21936  / loss = 0.3856894373893738\n",
            "I0420 22:24:17.618301 139904526645120 model_training_utils.py:450] Train Step: 19728/21936  / loss = 0.5982343554496765\n",
            "I0420 22:24:18.157636 139904526645120 model_training_utils.py:450] Train Step: 19729/21936  / loss = 0.388666570186615\n",
            "I0420 22:24:18.696093 139904526645120 model_training_utils.py:450] Train Step: 19730/21936  / loss = 0.16485284268856049\n",
            "I0420 22:24:19.235198 139904526645120 model_training_utils.py:450] Train Step: 19731/21936  / loss = 0.8679033517837524\n",
            "I0420 22:24:19.770110 139904526645120 model_training_utils.py:450] Train Step: 19732/21936  / loss = 0.33478695154190063\n",
            "I0420 22:24:20.306918 139904526645120 model_training_utils.py:450] Train Step: 19733/21936  / loss = 1.0382649898529053\n",
            "I0420 22:24:20.845795 139904526645120 model_training_utils.py:450] Train Step: 19734/21936  / loss = 0.3019062578678131\n",
            "I0420 22:24:21.384301 139904526645120 model_training_utils.py:450] Train Step: 19735/21936  / loss = 0.13919736444950104\n",
            "I0420 22:24:21.922762 139904526645120 model_training_utils.py:450] Train Step: 19736/21936  / loss = 0.7917012572288513\n",
            "I0420 22:24:22.461139 139904526645120 model_training_utils.py:450] Train Step: 19737/21936  / loss = 1.274064302444458\n",
            "I0420 22:24:22.997358 139904526645120 model_training_utils.py:450] Train Step: 19738/21936  / loss = 0.16818861663341522\n",
            "I0420 22:24:23.534827 139904526645120 model_training_utils.py:450] Train Step: 19739/21936  / loss = 1.1909852027893066\n",
            "I0420 22:24:24.074268 139904526645120 model_training_utils.py:450] Train Step: 19740/21936  / loss = 0.45854079723358154\n",
            "I0420 22:24:24.615458 139904526645120 model_training_utils.py:450] Train Step: 19741/21936  / loss = 0.2535335123538971\n",
            "I0420 22:24:25.155634 139904526645120 model_training_utils.py:450] Train Step: 19742/21936  / loss = 0.6469228267669678\n",
            "I0420 22:24:25.692724 139904526645120 model_training_utils.py:450] Train Step: 19743/21936  / loss = 0.2967441976070404\n",
            "I0420 22:24:26.230728 139904526645120 model_training_utils.py:450] Train Step: 19744/21936  / loss = 0.6881353855133057\n",
            "I0420 22:24:26.768908 139904526645120 model_training_utils.py:450] Train Step: 19745/21936  / loss = 0.09946119785308838\n",
            "I0420 22:24:27.307150 139904526645120 model_training_utils.py:450] Train Step: 19746/21936  / loss = 1.229786992073059\n",
            "I0420 22:24:27.846059 139904526645120 model_training_utils.py:450] Train Step: 19747/21936  / loss = 0.6336194276809692\n",
            "I0420 22:24:28.385516 139904526645120 model_training_utils.py:450] Train Step: 19748/21936  / loss = 0.8832494616508484\n",
            "I0420 22:24:28.929731 139904526645120 model_training_utils.py:450] Train Step: 19749/21936  / loss = 1.0773136615753174\n",
            "I0420 22:24:29.468439 139904526645120 model_training_utils.py:450] Train Step: 19750/21936  / loss = 1.9894990921020508\n",
            "I0420 22:24:30.004808 139904526645120 model_training_utils.py:450] Train Step: 19751/21936  / loss = 3.18503475189209\n",
            "I0420 22:24:30.542977 139904526645120 model_training_utils.py:450] Train Step: 19752/21936  / loss = 2.3412275314331055\n",
            "I0420 22:24:31.079269 139904526645120 model_training_utils.py:450] Train Step: 19753/21936  / loss = 0.15239286422729492\n",
            "I0420 22:24:31.618349 139904526645120 model_training_utils.py:450] Train Step: 19754/21936  / loss = 3.0558674335479736\n",
            "I0420 22:24:32.156363 139904526645120 model_training_utils.py:450] Train Step: 19755/21936  / loss = 2.5864388942718506\n",
            "I0420 22:24:32.692231 139904526645120 model_training_utils.py:450] Train Step: 19756/21936  / loss = 1.198169231414795\n",
            "I0420 22:24:33.229089 139904526645120 model_training_utils.py:450] Train Step: 19757/21936  / loss = 1.8943862915039062\n",
            "I0420 22:24:33.765300 139904526645120 model_training_utils.py:450] Train Step: 19758/21936  / loss = 1.7303636074066162\n",
            "I0420 22:24:34.302751 139904526645120 model_training_utils.py:450] Train Step: 19759/21936  / loss = 3.140188455581665\n",
            "I0420 22:24:34.841693 139904526645120 model_training_utils.py:450] Train Step: 19760/21936  / loss = 1.4511700868606567\n",
            "I0420 22:24:35.379034 139904526645120 model_training_utils.py:450] Train Step: 19761/21936  / loss = 1.044534683227539\n",
            "I0420 22:24:35.918444 139904526645120 model_training_utils.py:450] Train Step: 19762/21936  / loss = 0.7634739279747009\n",
            "I0420 22:24:36.455555 139904526645120 model_training_utils.py:450] Train Step: 19763/21936  / loss = 2.739015579223633\n",
            "I0420 22:24:36.994148 139904526645120 model_training_utils.py:450] Train Step: 19764/21936  / loss = 1.317395567893982\n",
            "I0420 22:24:37.532698 139904526645120 model_training_utils.py:450] Train Step: 19765/21936  / loss = 1.2306723594665527\n",
            "I0420 22:24:38.069925 139904526645120 model_training_utils.py:450] Train Step: 19766/21936  / loss = 1.6940168142318726\n",
            "I0420 22:24:38.607771 139904526645120 model_training_utils.py:450] Train Step: 19767/21936  / loss = 0.8407063484191895\n",
            "I0420 22:24:39.143169 139904526645120 model_training_utils.py:450] Train Step: 19768/21936  / loss = 2.5841612815856934\n",
            "I0420 22:24:39.680738 139904526645120 keras_utils.py:122] TimeHistory: 26.90 seconds, 14.87 examples/second between steps 30687 and 30737\n",
            "I0420 22:24:39.683378 139904526645120 model_training_utils.py:450] Train Step: 19769/21936  / loss = 1.7805368900299072\n",
            "I0420 22:24:40.229151 139904526645120 model_training_utils.py:450] Train Step: 19770/21936  / loss = 1.8888487815856934\n",
            "I0420 22:24:40.767122 139904526645120 model_training_utils.py:450] Train Step: 19771/21936  / loss = 2.304074764251709\n",
            "I0420 22:24:41.305444 139904526645120 model_training_utils.py:450] Train Step: 19772/21936  / loss = 2.0825297832489014\n",
            "I0420 22:24:41.842123 139904526645120 model_training_utils.py:450] Train Step: 19773/21936  / loss = 2.700573205947876\n",
            "I0420 22:24:42.378734 139904526645120 model_training_utils.py:450] Train Step: 19774/21936  / loss = 1.7833762168884277\n",
            "I0420 22:24:42.919671 139904526645120 model_training_utils.py:450] Train Step: 19775/21936  / loss = 2.6795380115509033\n",
            "I0420 22:24:43.457581 139904526645120 model_training_utils.py:450] Train Step: 19776/21936  / loss = 1.6071124076843262\n",
            "I0420 22:24:44.001093 139904526645120 model_training_utils.py:450] Train Step: 19777/21936  / loss = 2.5796279907226562\n",
            "I0420 22:24:44.537070 139904526645120 model_training_utils.py:450] Train Step: 19778/21936  / loss = 1.7855098247528076\n",
            "I0420 22:24:45.074113 139904526645120 model_training_utils.py:450] Train Step: 19779/21936  / loss = 1.3306406736373901\n",
            "I0420 22:24:45.613440 139904526645120 model_training_utils.py:450] Train Step: 19780/21936  / loss = 1.859915018081665\n",
            "I0420 22:24:46.151225 139904526645120 model_training_utils.py:450] Train Step: 19781/21936  / loss = 1.9484100341796875\n",
            "I0420 22:24:46.690103 139904526645120 model_training_utils.py:450] Train Step: 19782/21936  / loss = 1.0769898891448975\n",
            "I0420 22:24:47.226336 139904526645120 model_training_utils.py:450] Train Step: 19783/21936  / loss = 1.5048394203186035\n",
            "I0420 22:24:47.766555 139904526645120 model_training_utils.py:450] Train Step: 19784/21936  / loss = 1.881059169769287\n",
            "I0420 22:24:48.309265 139904526645120 model_training_utils.py:450] Train Step: 19785/21936  / loss = 1.6818941831588745\n",
            "I0420 22:24:48.848242 139904526645120 model_training_utils.py:450] Train Step: 19786/21936  / loss = 0.9429663419723511\n",
            "I0420 22:24:49.386842 139904526645120 model_training_utils.py:450] Train Step: 19787/21936  / loss = 1.1334681510925293\n",
            "I0420 22:24:49.924872 139904526645120 model_training_utils.py:450] Train Step: 19788/21936  / loss = 1.501439094543457\n",
            "I0420 22:24:50.463100 139904526645120 model_training_utils.py:450] Train Step: 19789/21936  / loss = 0.8630843162536621\n",
            "I0420 22:24:51.003338 139904526645120 model_training_utils.py:450] Train Step: 19790/21936  / loss = 0.7560582160949707\n",
            "I0420 22:24:51.541620 139904526645120 model_training_utils.py:450] Train Step: 19791/21936  / loss = 0.5251363515853882\n",
            "I0420 22:24:52.082182 139904526645120 model_training_utils.py:450] Train Step: 19792/21936  / loss = 1.364851951599121\n",
            "I0420 22:24:52.623125 139904526645120 model_training_utils.py:450] Train Step: 19793/21936  / loss = 1.1962950229644775\n",
            "I0420 22:24:53.159691 139904526645120 model_training_utils.py:450] Train Step: 19794/21936  / loss = 0.8786660432815552\n",
            "I0420 22:24:53.697782 139904526645120 model_training_utils.py:450] Train Step: 19795/21936  / loss = 0.7186870574951172\n",
            "I0420 22:24:54.238333 139904526645120 model_training_utils.py:450] Train Step: 19796/21936  / loss = 0.6269513368606567\n",
            "I0420 22:24:54.774197 139904526645120 model_training_utils.py:450] Train Step: 19797/21936  / loss = 0.7533644437789917\n",
            "I0420 22:24:55.314170 139904526645120 model_training_utils.py:450] Train Step: 19798/21936  / loss = 0.1819031834602356\n",
            "I0420 22:24:55.858818 139904526645120 model_training_utils.py:450] Train Step: 19799/21936  / loss = 0.7102440595626831\n",
            "I0420 22:24:56.396440 139904526645120 model_training_utils.py:450] Train Step: 19800/21936  / loss = 0.2979618310928345\n",
            "I0420 22:24:56.935798 139904526645120 model_training_utils.py:450] Train Step: 19801/21936  / loss = 0.5667521953582764\n",
            "I0420 22:24:57.473051 139904526645120 model_training_utils.py:450] Train Step: 19802/21936  / loss = 0.43353450298309326\n",
            "I0420 22:24:58.011142 139904526645120 model_training_utils.py:450] Train Step: 19803/21936  / loss = 0.6427440047264099\n",
            "I0420 22:24:58.547203 139904526645120 model_training_utils.py:450] Train Step: 19804/21936  / loss = 0.5233255624771118\n",
            "I0420 22:24:59.084272 139904526645120 model_training_utils.py:450] Train Step: 19805/21936  / loss = 0.6768386363983154\n",
            "I0420 22:24:59.623796 139904526645120 model_training_utils.py:450] Train Step: 19806/21936  / loss = 0.3028027415275574\n",
            "I0420 22:25:00.163326 139904526645120 model_training_utils.py:450] Train Step: 19807/21936  / loss = 1.072984218597412\n",
            "I0420 22:25:00.700211 139904526645120 model_training_utils.py:450] Train Step: 19808/21936  / loss = 1.1564648151397705\n",
            "I0420 22:25:01.237971 139904526645120 model_training_utils.py:450] Train Step: 19809/21936  / loss = 1.720515251159668\n",
            "I0420 22:25:01.778327 139904526645120 model_training_utils.py:450] Train Step: 19810/21936  / loss = 1.1789608001708984\n",
            "I0420 22:25:02.318156 139904526645120 model_training_utils.py:450] Train Step: 19811/21936  / loss = 0.13483551144599915\n",
            "I0420 22:25:02.858900 139904526645120 model_training_utils.py:450] Train Step: 19812/21936  / loss = 1.069570779800415\n",
            "I0420 22:25:03.395235 139904526645120 model_training_utils.py:450] Train Step: 19813/21936  / loss = 1.2456525564193726\n",
            "I0420 22:25:03.933358 139904526645120 model_training_utils.py:450] Train Step: 19814/21936  / loss = 1.7378320693969727\n",
            "I0420 22:25:04.470038 139904526645120 model_training_utils.py:450] Train Step: 19815/21936  / loss = 1.136019229888916\n",
            "I0420 22:25:05.005627 139904526645120 model_training_utils.py:450] Train Step: 19816/21936  / loss = 0.6471447944641113\n",
            "I0420 22:25:05.542190 139904526645120 model_training_utils.py:450] Train Step: 19817/21936  / loss = 0.6670302152633667\n",
            "I0420 22:25:06.079556 139904526645120 model_training_utils.py:450] Train Step: 19818/21936  / loss = 0.6510026454925537\n",
            "I0420 22:25:06.617869 139904526645120 keras_utils.py:122] TimeHistory: 26.93 seconds, 14.85 examples/second between steps 30737 and 30787\n",
            "I0420 22:25:06.620447 139904526645120 model_training_utils.py:450] Train Step: 19819/21936  / loss = 0.8839300870895386\n",
            "I0420 22:25:07.158444 139904526645120 model_training_utils.py:450] Train Step: 19820/21936  / loss = 1.2314192056655884\n",
            "I0420 22:25:07.695304 139904526645120 model_training_utils.py:450] Train Step: 19821/21936  / loss = 1.0812615156173706\n",
            "I0420 22:25:08.231989 139904526645120 model_training_utils.py:450] Train Step: 19822/21936  / loss = 0.6946530342102051\n",
            "I0420 22:25:08.768039 139904526645120 model_training_utils.py:450] Train Step: 19823/21936  / loss = 1.7688218355178833\n",
            "I0420 22:25:09.304408 139904526645120 model_training_utils.py:450] Train Step: 19824/21936  / loss = 1.0591259002685547\n",
            "I0420 22:25:09.841081 139904526645120 model_training_utils.py:450] Train Step: 19825/21936  / loss = 0.4605322480201721\n",
            "I0420 22:25:10.383861 139904526645120 model_training_utils.py:450] Train Step: 19826/21936  / loss = 1.4401341676712036\n",
            "I0420 22:25:10.921359 139904526645120 model_training_utils.py:450] Train Step: 19827/21936  / loss = 0.6741015315055847\n",
            "I0420 22:25:11.458306 139904526645120 model_training_utils.py:450] Train Step: 19828/21936  / loss = 0.6806627511978149\n",
            "I0420 22:25:11.996803 139904526645120 model_training_utils.py:450] Train Step: 19829/21936  / loss = 0.2846646308898926\n",
            "I0420 22:25:12.535054 139904526645120 model_training_utils.py:450] Train Step: 19830/21936  / loss = 0.7095692157745361\n",
            "I0420 22:25:13.071782 139904526645120 model_training_utils.py:450] Train Step: 19831/21936  / loss = 0.2771473228931427\n",
            "I0420 22:25:13.611234 139904526645120 model_training_utils.py:450] Train Step: 19832/21936  / loss = 1.424160122871399\n",
            "I0420 22:25:14.149339 139904526645120 model_training_utils.py:450] Train Step: 19833/21936  / loss = 1.2641370296478271\n",
            "I0420 22:25:14.697324 139904526645120 model_training_utils.py:450] Train Step: 19834/21936  / loss = 1.0719269514083862\n",
            "I0420 22:25:15.234486 139904526645120 model_training_utils.py:450] Train Step: 19835/21936  / loss = 0.9128592014312744\n",
            "I0420 22:25:15.770483 139904526645120 model_training_utils.py:450] Train Step: 19836/21936  / loss = 0.43603697419166565\n",
            "I0420 22:25:16.305266 139904526645120 model_training_utils.py:450] Train Step: 19837/21936  / loss = 0.1754380762577057\n",
            "I0420 22:25:16.842482 139904526645120 model_training_utils.py:450] Train Step: 19838/21936  / loss = 0.43414759635925293\n",
            "I0420 22:25:17.380979 139904526645120 model_training_utils.py:450] Train Step: 19839/21936  / loss = 1.0191859006881714\n",
            "I0420 22:25:17.918495 139904526645120 model_training_utils.py:450] Train Step: 19840/21936  / loss = 1.0391520261764526\n",
            "I0420 22:25:18.460893 139904526645120 model_training_utils.py:450] Train Step: 19841/21936  / loss = 1.5483617782592773\n",
            "I0420 22:25:18.997119 139904526645120 model_training_utils.py:450] Train Step: 19842/21936  / loss = 0.16544854640960693\n",
            "I0420 22:25:19.534518 139904526645120 model_training_utils.py:450] Train Step: 19843/21936  / loss = 0.6307006478309631\n",
            "I0420 22:25:20.071427 139904526645120 model_training_utils.py:450] Train Step: 19844/21936  / loss = 0.959662139415741\n",
            "I0420 22:25:20.612075 139904526645120 model_training_utils.py:450] Train Step: 19845/21936  / loss = 0.11641225218772888\n",
            "I0420 22:25:21.150662 139904526645120 model_training_utils.py:450] Train Step: 19846/21936  / loss = 0.4128989577293396\n",
            "I0420 22:25:21.690226 139904526645120 model_training_utils.py:450] Train Step: 19847/21936  / loss = 0.36049193143844604\n",
            "I0420 22:25:22.233383 139904526645120 model_training_utils.py:450] Train Step: 19848/21936  / loss = 0.5893260836601257\n",
            "I0420 22:25:22.770605 139904526645120 model_training_utils.py:450] Train Step: 19849/21936  / loss = 0.2675846815109253\n",
            "I0420 22:25:23.306531 139904526645120 model_training_utils.py:450] Train Step: 19850/21936  / loss = 0.3989250659942627\n",
            "I0420 22:25:23.843420 139904526645120 model_training_utils.py:450] Train Step: 19851/21936  / loss = 0.2597683370113373\n",
            "I0420 22:25:24.380123 139904526645120 model_training_utils.py:450] Train Step: 19852/21936  / loss = 0.07635226100683212\n",
            "I0420 22:25:24.916592 139904526645120 model_training_utils.py:450] Train Step: 19853/21936  / loss = 0.4178692102432251\n",
            "I0420 22:25:25.451922 139904526645120 model_training_utils.py:450] Train Step: 19854/21936  / loss = 0.42386478185653687\n",
            "I0420 22:25:25.988150 139904526645120 model_training_utils.py:450] Train Step: 19855/21936  / loss = 0.23070083558559418\n",
            "I0420 22:25:26.523308 139904526645120 model_training_utils.py:450] Train Step: 19856/21936  / loss = 0.22028884291648865\n",
            "I0420 22:25:27.061206 139904526645120 model_training_utils.py:450] Train Step: 19857/21936  / loss = 0.18510553240776062\n",
            "I0420 22:25:27.597061 139904526645120 model_training_utils.py:450] Train Step: 19858/21936  / loss = 0.09969975054264069\n",
            "I0420 22:25:28.133143 139904526645120 model_training_utils.py:450] Train Step: 19859/21936  / loss = 0.22570842504501343\n",
            "I0420 22:25:28.670260 139904526645120 model_training_utils.py:450] Train Step: 19860/21936  / loss = 0.13190887868404388\n",
            "I0420 22:25:29.209732 139904526645120 model_training_utils.py:450] Train Step: 19861/21936  / loss = 0.05761445313692093\n",
            "I0420 22:25:29.747088 139904526645120 model_training_utils.py:450] Train Step: 19862/21936  / loss = 0.36266663670539856\n",
            "I0420 22:25:30.283486 139904526645120 model_training_utils.py:450] Train Step: 19863/21936  / loss = 0.21146145462989807\n",
            "I0420 22:25:30.822045 139904526645120 model_training_utils.py:450] Train Step: 19864/21936  / loss = 0.03436008468270302\n",
            "I0420 22:25:31.359680 139904526645120 model_training_utils.py:450] Train Step: 19865/21936  / loss = 0.06442014873027802\n",
            "I0420 22:25:31.898799 139904526645120 model_training_utils.py:450] Train Step: 19866/21936  / loss = 0.38519513607025146\n",
            "I0420 22:25:32.434427 139904526645120 model_training_utils.py:450] Train Step: 19867/21936  / loss = 0.24662445485591888\n",
            "I0420 22:25:32.983664 139904526645120 model_training_utils.py:450] Train Step: 19868/21936  / loss = 0.07268871366977692\n",
            "I0420 22:25:33.524047 139904526645120 keras_utils.py:122] TimeHistory: 26.90 seconds, 14.87 examples/second between steps 30787 and 30837\n",
            "I0420 22:25:33.526701 139904526645120 model_training_utils.py:450] Train Step: 19869/21936  / loss = 0.14193081855773926\n",
            "I0420 22:25:34.064374 139904526645120 model_training_utils.py:450] Train Step: 19870/21936  / loss = 0.15339073538780212\n",
            "I0420 22:25:34.603144 139904526645120 model_training_utils.py:450] Train Step: 19871/21936  / loss = 0.16469651460647583\n",
            "I0420 22:25:35.139316 139904526645120 model_training_utils.py:450] Train Step: 19872/21936  / loss = 0.0908733457326889\n",
            "I0420 22:25:35.676912 139904526645120 model_training_utils.py:450] Train Step: 19873/21936  / loss = 1.0907424688339233\n",
            "I0420 22:25:36.214580 139904526645120 model_training_utils.py:450] Train Step: 19874/21936  / loss = 0.20093819499015808\n",
            "I0420 22:25:36.752772 139904526645120 model_training_utils.py:450] Train Step: 19875/21936  / loss = 0.27955424785614014\n",
            "I0420 22:25:37.290393 139904526645120 model_training_utils.py:450] Train Step: 19876/21936  / loss = 0.10182441025972366\n",
            "I0420 22:25:37.829351 139904526645120 model_training_utils.py:450] Train Step: 19877/21936  / loss = 0.09570413082838058\n",
            "I0420 22:25:38.364431 139904526645120 model_training_utils.py:450] Train Step: 19878/21936  / loss = 0.43928951025009155\n",
            "I0420 22:25:38.901553 139904526645120 model_training_utils.py:450] Train Step: 19879/21936  / loss = 0.46674099564552307\n",
            "I0420 22:25:39.437610 139904526645120 model_training_utils.py:450] Train Step: 19880/21936  / loss = 0.03266642242670059\n",
            "I0420 22:25:39.974116 139904526645120 model_training_utils.py:450] Train Step: 19881/21936  / loss = 0.4688434600830078\n",
            "I0420 22:25:40.512376 139904526645120 model_training_utils.py:450] Train Step: 19882/21936  / loss = 0.14235028624534607\n",
            "I0420 22:25:41.051650 139904526645120 model_training_utils.py:450] Train Step: 19883/21936  / loss = 0.4633176326751709\n",
            "I0420 22:25:41.590633 139904526645120 model_training_utils.py:450] Train Step: 19884/21936  / loss = 0.461366206407547\n",
            "I0420 22:25:42.134186 139904526645120 model_training_utils.py:450] Train Step: 19885/21936  / loss = 0.26172858476638794\n",
            "I0420 22:25:42.673679 139904526645120 model_training_utils.py:450] Train Step: 19886/21936  / loss = 0.17676375806331635\n",
            "I0420 22:25:43.210505 139904526645120 model_training_utils.py:450] Train Step: 19887/21936  / loss = 0.05772041529417038\n",
            "I0420 22:25:43.754224 139904526645120 model_training_utils.py:450] Train Step: 19888/21936  / loss = 0.3779745399951935\n",
            "I0420 22:25:44.293097 139904526645120 model_training_utils.py:450] Train Step: 19889/21936  / loss = 0.04643400013446808\n",
            "I0420 22:25:44.827980 139904526645120 model_training_utils.py:450] Train Step: 19890/21936  / loss = 0.33620497584342957\n",
            "I0420 22:25:45.365001 139904526645120 model_training_utils.py:450] Train Step: 19891/21936  / loss = 0.7537415027618408\n",
            "I0420 22:25:45.902292 139904526645120 model_training_utils.py:450] Train Step: 19892/21936  / loss = 0.841246485710144\n",
            "I0420 22:25:46.438871 139904526645120 model_training_utils.py:450] Train Step: 19893/21936  / loss = 0.39485031366348267\n",
            "I0420 22:25:46.978387 139904526645120 model_training_utils.py:450] Train Step: 19894/21936  / loss = 0.24543723464012146\n",
            "I0420 22:25:47.515427 139904526645120 model_training_utils.py:450] Train Step: 19895/21936  / loss = 1.1536961793899536\n",
            "I0420 22:25:48.051732 139904526645120 model_training_utils.py:450] Train Step: 19896/21936  / loss = 0.4796730875968933\n",
            "I0420 22:25:48.589010 139904526645120 model_training_utils.py:450] Train Step: 19897/21936  / loss = 0.04668012261390686\n",
            "I0420 22:25:49.127753 139904526645120 model_training_utils.py:450] Train Step: 19898/21936  / loss = 0.27205145359039307\n",
            "I0420 22:25:49.664857 139904526645120 model_training_utils.py:450] Train Step: 19899/21936  / loss = 0.041803453117609024\n",
            "I0420 22:25:50.203711 139904526645120 model_training_utils.py:450] Train Step: 19900/21936  / loss = 0.31359028816223145\n",
            "I0420 22:25:50.742168 139904526645120 model_training_utils.py:450] Train Step: 19901/21936  / loss = 0.18836042284965515\n",
            "I0420 22:25:51.277853 139904526645120 model_training_utils.py:450] Train Step: 19902/21936  / loss = 0.4937468469142914\n",
            "I0420 22:25:51.816822 139904526645120 model_training_utils.py:450] Train Step: 19903/21936  / loss = 0.01780049502849579\n",
            "I0420 22:25:52.352301 139904526645120 model_training_utils.py:450] Train Step: 19904/21936  / loss = 0.12604625523090363\n",
            "I0420 22:25:52.890412 139904526645120 model_training_utils.py:450] Train Step: 19905/21936  / loss = 0.42588192224502563\n",
            "I0420 22:25:53.428736 139904526645120 model_training_utils.py:450] Train Step: 19906/21936  / loss = 0.19493988156318665\n",
            "I0420 22:25:53.965212 139904526645120 model_training_utils.py:450] Train Step: 19907/21936  / loss = 0.33883535861968994\n",
            "I0420 22:25:54.502099 139904526645120 model_training_utils.py:450] Train Step: 19908/21936  / loss = 0.2520837187767029\n",
            "I0420 22:25:55.039903 139904526645120 model_training_utils.py:450] Train Step: 19909/21936  / loss = 0.1643930971622467\n",
            "I0420 22:25:55.586106 139904526645120 model_training_utils.py:450] Train Step: 19910/21936  / loss = 0.353030264377594\n",
            "I0420 22:25:56.127124 139904526645120 model_training_utils.py:450] Train Step: 19911/21936  / loss = 0.7014734745025635\n",
            "I0420 22:25:56.669106 139904526645120 model_training_utils.py:450] Train Step: 19912/21936  / loss = 0.07399125397205353\n",
            "I0420 22:25:57.209505 139904526645120 model_training_utils.py:450] Train Step: 19913/21936  / loss = 0.11198008060455322\n",
            "I0420 22:25:57.751756 139904526645120 model_training_utils.py:450] Train Step: 19914/21936  / loss = 0.07390502095222473\n",
            "I0420 22:25:58.289713 139904526645120 model_training_utils.py:450] Train Step: 19915/21936  / loss = 0.22931449115276337\n",
            "I0420 22:25:58.828721 139904526645120 model_training_utils.py:450] Train Step: 19916/21936  / loss = 0.4251366853713989\n",
            "I0420 22:25:59.366643 139904526645120 model_training_utils.py:450] Train Step: 19917/21936  / loss = 0.004579193890094757\n",
            "I0420 22:25:59.907250 139904526645120 model_training_utils.py:450] Train Step: 19918/21936  / loss = 0.08634307980537415\n",
            "I0420 22:26:00.443171 139904526645120 keras_utils.py:122] TimeHistory: 26.92 seconds, 14.86 examples/second between steps 30837 and 30887\n",
            "I0420 22:26:00.446072 139904526645120 model_training_utils.py:450] Train Step: 19919/21936  / loss = 0.23421049118041992\n",
            "I0420 22:26:00.981631 139904526645120 model_training_utils.py:450] Train Step: 19920/21936  / loss = 0.1183222159743309\n",
            "I0420 22:26:01.519066 139904526645120 model_training_utils.py:450] Train Step: 19921/21936  / loss = 0.6838856935501099\n",
            "I0420 22:26:02.056287 139904526645120 model_training_utils.py:450] Train Step: 19922/21936  / loss = 0.2073151171207428\n",
            "I0420 22:26:02.592758 139904526645120 model_training_utils.py:450] Train Step: 19923/21936  / loss = 0.2925945222377777\n",
            "I0420 22:26:03.132258 139904526645120 model_training_utils.py:450] Train Step: 19924/21936  / loss = 0.16835640370845795\n",
            "I0420 22:26:03.667301 139904526645120 model_training_utils.py:450] Train Step: 19925/21936  / loss = 0.0455358512699604\n",
            "I0420 22:26:04.205406 139904526645120 model_training_utils.py:450] Train Step: 19926/21936  / loss = 0.5043672919273376\n",
            "I0420 22:26:04.744944 139904526645120 model_training_utils.py:450] Train Step: 19927/21936  / loss = 0.22687670588493347\n",
            "I0420 22:26:05.283294 139904526645120 model_training_utils.py:450] Train Step: 19928/21936  / loss = 0.24693602323532104\n",
            "I0420 22:26:05.821466 139904526645120 model_training_utils.py:450] Train Step: 19929/21936  / loss = 0.04065737500786781\n",
            "I0420 22:26:06.357732 139904526645120 model_training_utils.py:450] Train Step: 19930/21936  / loss = 0.1221427470445633\n",
            "I0420 22:26:06.894874 139904526645120 model_training_utils.py:450] Train Step: 19931/21936  / loss = 0.01330054271966219\n",
            "I0420 22:26:07.434436 139904526645120 model_training_utils.py:450] Train Step: 19932/21936  / loss = 0.4304048418998718\n",
            "I0420 22:26:07.972060 139904526645120 model_training_utils.py:450] Train Step: 19933/21936  / loss = 0.4107755124568939\n",
            "I0420 22:26:08.508184 139904526645120 model_training_utils.py:450] Train Step: 19934/21936  / loss = 0.4434378743171692\n",
            "I0420 22:26:09.046076 139904526645120 model_training_utils.py:450] Train Step: 19935/21936  / loss = 0.1174476146697998\n",
            "I0420 22:26:09.583472 139904526645120 model_training_utils.py:450] Train Step: 19936/21936  / loss = 0.13868725299835205\n",
            "I0420 22:26:10.121447 139904526645120 model_training_utils.py:450] Train Step: 19937/21936  / loss = 0.9395990371704102\n",
            "I0420 22:26:10.658675 139904526645120 model_training_utils.py:450] Train Step: 19938/21936  / loss = 0.1967441439628601\n",
            "I0420 22:26:11.207066 139904526645120 model_training_utils.py:450] Train Step: 19939/21936  / loss = 0.38822293281555176\n",
            "I0420 22:26:11.743844 139904526645120 model_training_utils.py:450] Train Step: 19940/21936  / loss = 0.4204975962638855\n",
            "I0420 22:26:12.286235 139904526645120 model_training_utils.py:450] Train Step: 19941/21936  / loss = 0.28908053040504456\n",
            "I0420 22:26:12.823640 139904526645120 model_training_utils.py:450] Train Step: 19942/21936  / loss = 0.8509372472763062\n",
            "I0420 22:26:13.360954 139904526645120 model_training_utils.py:450] Train Step: 19943/21936  / loss = 1.0525016784667969\n",
            "I0420 22:26:13.897275 139904526645120 model_training_utils.py:450] Train Step: 19944/21936  / loss = 0.37488842010498047\n",
            "I0420 22:26:14.433757 139904526645120 model_training_utils.py:450] Train Step: 19945/21936  / loss = 0.40300363302230835\n",
            "I0420 22:26:14.971357 139904526645120 model_training_utils.py:450] Train Step: 19946/21936  / loss = 0.45669668912887573\n",
            "I0420 22:26:15.509073 139904526645120 model_training_utils.py:450] Train Step: 19947/21936  / loss = 0.8061526417732239\n",
            "I0420 22:26:16.046117 139904526645120 model_training_utils.py:450] Train Step: 19948/21936  / loss = 0.7484618425369263\n",
            "I0420 22:26:16.585474 139904526645120 model_training_utils.py:450] Train Step: 19949/21936  / loss = 0.793660044670105\n",
            "I0420 22:26:17.119980 139904526645120 model_training_utils.py:450] Train Step: 19950/21936  / loss = 0.15347562730312347\n",
            "I0420 22:26:17.659175 139904526645120 model_training_utils.py:450] Train Step: 19951/21936  / loss = 0.8027820587158203\n",
            "I0420 22:26:18.197646 139904526645120 model_training_utils.py:450] Train Step: 19952/21936  / loss = 0.6008299589157104\n",
            "I0420 22:26:18.737262 139904526645120 model_training_utils.py:450] Train Step: 19953/21936  / loss = 0.8852258324623108\n",
            "I0420 22:26:19.274959 139904526645120 model_training_utils.py:450] Train Step: 19954/21936  / loss = 0.3207813501358032\n",
            "I0420 22:26:19.811832 139904526645120 model_training_utils.py:450] Train Step: 19955/21936  / loss = 1.256287932395935\n",
            "I0420 22:26:20.346906 139904526645120 model_training_utils.py:450] Train Step: 19956/21936  / loss = 0.43364590406417847\n",
            "I0420 22:26:20.887276 139904526645120 model_training_utils.py:450] Train Step: 19957/21936  / loss = 0.5050610899925232\n",
            "I0420 22:26:21.422157 139904526645120 model_training_utils.py:450] Train Step: 19958/21936  / loss = 0.453781396150589\n",
            "I0420 22:26:21.961871 139904526645120 model_training_utils.py:450] Train Step: 19959/21936  / loss = 0.4908473491668701\n",
            "I0420 22:26:22.497997 139904526645120 model_training_utils.py:450] Train Step: 19960/21936  / loss = 0.35998889803886414\n",
            "I0420 22:26:23.035427 139904526645120 model_training_utils.py:450] Train Step: 19961/21936  / loss = 0.4365851879119873\n",
            "I0420 22:26:23.572944 139904526645120 model_training_utils.py:450] Train Step: 19962/21936  / loss = 0.4511502981185913\n",
            "I0420 22:26:24.111247 139904526645120 model_training_utils.py:450] Train Step: 19963/21936  / loss = 0.31198811531066895\n",
            "I0420 22:26:24.649512 139904526645120 model_training_utils.py:450] Train Step: 19964/21936  / loss = 0.49574995040893555\n",
            "I0420 22:26:25.198195 139904526645120 model_training_utils.py:450] Train Step: 19965/21936  / loss = 0.35480087995529175\n",
            "I0420 22:26:25.735126 139904526645120 model_training_utils.py:450] Train Step: 19966/21936  / loss = 0.2811138927936554\n",
            "I0420 22:26:26.275187 139904526645120 model_training_utils.py:450] Train Step: 19967/21936  / loss = 0.8671278953552246\n",
            "I0420 22:26:26.813354 139904526645120 model_training_utils.py:450] Train Step: 19968/21936  / loss = 0.4610571265220642\n",
            "I0420 22:26:27.356707 139904526645120 keras_utils.py:122] TimeHistory: 26.91 seconds, 14.86 examples/second between steps 30887 and 30937\n",
            "I0420 22:26:27.359203 139904526645120 model_training_utils.py:450] Train Step: 19969/21936  / loss = 0.23036392033100128\n",
            "I0420 22:26:27.901143 139904526645120 model_training_utils.py:450] Train Step: 19970/21936  / loss = 0.014606068842113018\n",
            "I0420 22:26:28.438846 139904526645120 model_training_utils.py:450] Train Step: 19971/21936  / loss = 0.9224065542221069\n",
            "I0420 22:26:28.975583 139904526645120 model_training_utils.py:450] Train Step: 19972/21936  / loss = 0.26455339789390564\n",
            "I0420 22:26:29.510480 139904526645120 model_training_utils.py:450] Train Step: 19973/21936  / loss = 0.3649882674217224\n",
            "I0420 22:26:30.052919 139904526645120 model_training_utils.py:450] Train Step: 19974/21936  / loss = 0.2592960000038147\n",
            "I0420 22:26:30.590128 139904526645120 model_training_utils.py:450] Train Step: 19975/21936  / loss = 0.4646957516670227\n",
            "I0420 22:26:31.125998 139904526645120 model_training_utils.py:450] Train Step: 19976/21936  / loss = 0.6222996115684509\n",
            "I0420 22:26:31.662303 139904526645120 model_training_utils.py:450] Train Step: 19977/21936  / loss = 0.5923275947570801\n",
            "I0420 22:26:32.201276 139904526645120 model_training_utils.py:450] Train Step: 19978/21936  / loss = 0.4409990608692169\n",
            "I0420 22:26:32.739050 139904526645120 model_training_utils.py:450] Train Step: 19979/21936  / loss = 0.2004852294921875\n",
            "I0420 22:26:33.276079 139904526645120 model_training_utils.py:450] Train Step: 19980/21936  / loss = 0.6694970726966858\n",
            "I0420 22:26:33.814781 139904526645120 model_training_utils.py:450] Train Step: 19981/21936  / loss = 0.16341538727283478\n",
            "I0420 22:26:34.352490 139904526645120 model_training_utils.py:450] Train Step: 19982/21936  / loss = 0.582177996635437\n",
            "I0420 22:26:34.891256 139904526645120 model_training_utils.py:450] Train Step: 19983/21936  / loss = 0.7929344177246094\n",
            "I0420 22:26:35.429044 139904526645120 model_training_utils.py:450] Train Step: 19984/21936  / loss = 0.6405399441719055\n",
            "I0420 22:26:35.966498 139904526645120 model_training_utils.py:450] Train Step: 19985/21936  / loss = 0.9830007553100586\n",
            "I0420 22:26:36.505481 139904526645120 model_training_utils.py:450] Train Step: 19986/21936  / loss = 0.5764080882072449\n",
            "I0420 22:26:37.041299 139904526645120 model_training_utils.py:450] Train Step: 19987/21936  / loss = 1.4053819179534912\n",
            "I0420 22:26:37.586515 139904526645120 model_training_utils.py:450] Train Step: 19988/21936  / loss = 0.5263097882270813\n",
            "I0420 22:26:38.124094 139904526645120 model_training_utils.py:450] Train Step: 19989/21936  / loss = 0.5715781450271606\n",
            "I0420 22:26:38.660729 139904526645120 model_training_utils.py:450] Train Step: 19990/21936  / loss = 1.8122637271881104\n",
            "I0420 22:26:39.199274 139904526645120 model_training_utils.py:450] Train Step: 19991/21936  / loss = 0.21805256605148315\n",
            "I0420 22:26:39.743262 139904526645120 model_training_utils.py:450] Train Step: 19992/21936  / loss = 1.9789111614227295\n",
            "I0420 22:26:40.280184 139904526645120 model_training_utils.py:450] Train Step: 19993/21936  / loss = 0.3929237425327301\n",
            "I0420 22:26:40.815002 139904526645120 model_training_utils.py:450] Train Step: 19994/21936  / loss = 0.5506073236465454\n",
            "I0420 22:26:41.365628 139904526645120 model_training_utils.py:450] Train Step: 19995/21936  / loss = 0.8702728152275085\n",
            "I0420 22:26:41.903977 139904526645120 model_training_utils.py:450] Train Step: 19996/21936  / loss = 0.626887321472168\n",
            "I0420 22:26:42.442013 139904526645120 model_training_utils.py:450] Train Step: 19997/21936  / loss = 0.6290014982223511\n",
            "I0420 22:26:42.979021 139904526645120 model_training_utils.py:450] Train Step: 19998/21936  / loss = 0.7020172476768494\n",
            "I0420 22:26:43.518664 139904526645120 model_training_utils.py:450] Train Step: 19999/21936  / loss = 0.9670661687850952\n",
            "I0420 22:26:44.056385 139904526645120 model_training_utils.py:450] Train Step: 20000/21936  / loss = 1.124248743057251\n",
            "I0420 22:26:44.594627 139904526645120 model_training_utils.py:450] Train Step: 20001/21936  / loss = 1.0053715705871582\n",
            "I0420 22:26:45.135772 139904526645120 model_training_utils.py:450] Train Step: 20002/21936  / loss = 1.701993703842163\n",
            "I0420 22:26:45.675101 139904526645120 model_training_utils.py:450] Train Step: 20003/21936  / loss = 2.5078580379486084\n",
            "I0420 22:26:46.212474 139904526645120 model_training_utils.py:450] Train Step: 20004/21936  / loss = 1.9525517225265503\n",
            "I0420 22:26:46.750053 139904526645120 model_training_utils.py:450] Train Step: 20005/21936  / loss = 2.3459787368774414\n",
            "I0420 22:26:47.287051 139904526645120 model_training_utils.py:450] Train Step: 20006/21936  / loss = 1.9469354152679443\n",
            "I0420 22:26:47.831452 139904526645120 model_training_utils.py:450] Train Step: 20007/21936  / loss = 1.2827763557434082\n",
            "I0420 22:26:48.368870 139904526645120 model_training_utils.py:450] Train Step: 20008/21936  / loss = 1.4857594966888428\n",
            "I0420 22:26:48.911449 139904526645120 model_training_utils.py:450] Train Step: 20009/21936  / loss = 2.6677913665771484\n",
            "I0420 22:26:49.447148 139904526645120 model_training_utils.py:450] Train Step: 20010/21936  / loss = 1.6276814937591553\n",
            "I0420 22:26:49.985658 139904526645120 model_training_utils.py:450] Train Step: 20011/21936  / loss = 1.3437919616699219\n",
            "I0420 22:26:50.522728 139904526645120 model_training_utils.py:450] Train Step: 20012/21936  / loss = 2.0867488384246826\n",
            "I0420 22:26:51.059516 139904526645120 model_training_utils.py:450] Train Step: 20013/21936  / loss = 2.642834186553955\n",
            "I0420 22:26:51.599650 139904526645120 model_training_utils.py:450] Train Step: 20014/21936  / loss = 3.735717296600342\n",
            "I0420 22:26:52.151610 139904526645120 model_training_utils.py:450] Train Step: 20015/21936  / loss = 1.3462812900543213\n",
            "I0420 22:26:52.689772 139904526645120 model_training_utils.py:450] Train Step: 20016/21936  / loss = 1.7496421337127686\n",
            "I0420 22:26:53.225375 139904526645120 model_training_utils.py:450] Train Step: 20017/21936  / loss = 0.8756216764450073\n",
            "I0420 22:26:53.761903 139904526645120 model_training_utils.py:450] Train Step: 20018/21936  / loss = 0.8827866911888123\n",
            "I0420 22:26:54.298890 139904526645120 keras_utils.py:122] TimeHistory: 26.94 seconds, 14.85 examples/second between steps 30937 and 30987\n",
            "I0420 22:26:54.301554 139904526645120 model_training_utils.py:450] Train Step: 20019/21936  / loss = 0.5269486904144287\n",
            "I0420 22:26:54.839544 139904526645120 model_training_utils.py:450] Train Step: 20020/21936  / loss = 0.6124712824821472\n",
            "I0420 22:26:55.378827 139904526645120 model_training_utils.py:450] Train Step: 20021/21936  / loss = 0.17287257313728333\n",
            "I0420 22:26:55.917157 139904526645120 model_training_utils.py:450] Train Step: 20022/21936  / loss = 0.9878653287887573\n",
            "I0420 22:26:56.457076 139904526645120 model_training_utils.py:450] Train Step: 20023/21936  / loss = 2.2046468257904053\n",
            "I0420 22:26:56.992716 139904526645120 model_training_utils.py:450] Train Step: 20024/21936  / loss = 0.13736101984977722\n",
            "I0420 22:26:57.528833 139904526645120 model_training_utils.py:450] Train Step: 20025/21936  / loss = 0.9937472343444824\n",
            "I0420 22:26:58.075965 139904526645120 model_training_utils.py:450] Train Step: 20026/21936  / loss = 0.6938652992248535\n",
            "I0420 22:26:58.610534 139904526645120 model_training_utils.py:450] Train Step: 20027/21936  / loss = 0.86359703540802\n",
            "I0420 22:26:59.148071 139904526645120 model_training_utils.py:450] Train Step: 20028/21936  / loss = 1.0284607410430908\n",
            "I0420 22:26:59.689139 139904526645120 model_training_utils.py:450] Train Step: 20029/21936  / loss = 0.4159783124923706\n",
            "I0420 22:27:00.227850 139904526645120 model_training_utils.py:450] Train Step: 20030/21936  / loss = 0.5151435136795044\n",
            "I0420 22:27:00.764858 139904526645120 model_training_utils.py:450] Train Step: 20031/21936  / loss = 0.9447534680366516\n",
            "I0420 22:27:01.303874 139904526645120 model_training_utils.py:450] Train Step: 20032/21936  / loss = 0.12854406237602234\n",
            "I0420 22:27:01.846161 139904526645120 model_training_utils.py:450] Train Step: 20033/21936  / loss = 0.5854225158691406\n",
            "I0420 22:27:02.392599 139904526645120 model_training_utils.py:450] Train Step: 20034/21936  / loss = 0.24393784999847412\n",
            "I0420 22:27:02.930151 139904526645120 model_training_utils.py:450] Train Step: 20035/21936  / loss = 0.3070642650127411\n",
            "I0420 22:27:03.471006 139904526645120 model_training_utils.py:450] Train Step: 20036/21936  / loss = 0.43744826316833496\n",
            "I0420 22:27:04.014635 139904526645120 model_training_utils.py:450] Train Step: 20037/21936  / loss = 0.5892361998558044\n",
            "I0420 22:27:04.557800 139904526645120 model_training_utils.py:450] Train Step: 20038/21936  / loss = 0.47968360781669617\n",
            "I0420 22:27:05.096854 139904526645120 model_training_utils.py:450] Train Step: 20039/21936  / loss = 0.22336435317993164\n",
            "I0420 22:27:05.635753 139904526645120 model_training_utils.py:450] Train Step: 20040/21936  / loss = 0.8109517693519592\n",
            "I0420 22:27:06.173980 139904526645120 model_training_utils.py:450] Train Step: 20041/21936  / loss = 0.4018216133117676\n",
            "I0420 22:27:06.719368 139904526645120 model_training_utils.py:450] Train Step: 20042/21936  / loss = 0.3405473232269287\n",
            "I0420 22:27:07.257181 139904526645120 model_training_utils.py:450] Train Step: 20043/21936  / loss = 0.22948342561721802\n",
            "I0420 22:27:07.796583 139904526645120 model_training_utils.py:450] Train Step: 20044/21936  / loss = 1.1154553890228271\n",
            "I0420 22:27:08.335097 139904526645120 model_training_utils.py:450] Train Step: 20045/21936  / loss = 0.31620725989341736\n",
            "I0420 22:27:08.871138 139904526645120 model_training_utils.py:450] Train Step: 20046/21936  / loss = 0.6298900842666626\n",
            "I0420 22:27:09.409243 139904526645120 model_training_utils.py:450] Train Step: 20047/21936  / loss = 0.3681444227695465\n",
            "I0420 22:27:09.946847 139904526645120 model_training_utils.py:450] Train Step: 20048/21936  / loss = 0.26781409978866577\n",
            "I0420 22:27:10.483263 139904526645120 model_training_utils.py:450] Train Step: 20049/21936  / loss = 0.014331933110952377\n",
            "I0420 22:27:11.020291 139904526645120 model_training_utils.py:450] Train Step: 20050/21936  / loss = 0.7825572490692139\n",
            "I0420 22:27:11.565542 139904526645120 model_training_utils.py:450] Train Step: 20051/21936  / loss = 0.329054594039917\n",
            "I0420 22:27:12.103894 139904526645120 model_training_utils.py:450] Train Step: 20052/21936  / loss = 0.0912945494055748\n",
            "I0420 22:27:12.647377 139904526645120 model_training_utils.py:450] Train Step: 20053/21936  / loss = 0.4244016408920288\n",
            "I0420 22:27:13.183147 139904526645120 model_training_utils.py:450] Train Step: 20054/21936  / loss = 0.7859518527984619\n",
            "I0420 22:27:13.720990 139904526645120 model_training_utils.py:450] Train Step: 20055/21936  / loss = 0.2662191689014435\n",
            "I0420 22:27:14.257625 139904526645120 model_training_utils.py:450] Train Step: 20056/21936  / loss = 1.0953792333602905\n",
            "I0420 22:27:14.795688 139904526645120 model_training_utils.py:450] Train Step: 20057/21936  / loss = 0.6117262840270996\n",
            "I0420 22:27:15.338207 139904526645120 model_training_utils.py:450] Train Step: 20058/21936  / loss = 0.10179515928030014\n",
            "I0420 22:27:15.875280 139904526645120 model_training_utils.py:450] Train Step: 20059/21936  / loss = 0.0542609840631485\n",
            "I0420 22:27:16.416292 139904526645120 model_training_utils.py:450] Train Step: 20060/21936  / loss = 0.6185337901115417\n",
            "I0420 22:27:16.953660 139904526645120 model_training_utils.py:450] Train Step: 20061/21936  / loss = 0.10726052522659302\n",
            "I0420 22:27:17.490239 139904526645120 model_training_utils.py:450] Train Step: 20062/21936  / loss = 0.26128101348876953\n",
            "I0420 22:27:18.028492 139904526645120 model_training_utils.py:450] Train Step: 20063/21936  / loss = 0.36291778087615967\n",
            "I0420 22:27:18.563220 139904526645120 model_training_utils.py:450] Train Step: 20064/21936  / loss = 0.1606171876192093\n",
            "I0420 22:27:19.098179 139904526645120 model_training_utils.py:450] Train Step: 20065/21936  / loss = 0.6441536545753479\n",
            "I0420 22:27:19.641634 139904526645120 model_training_utils.py:450] Train Step: 20066/21936  / loss = 1.0037785768508911\n",
            "I0420 22:27:20.178101 139904526645120 model_training_utils.py:450] Train Step: 20067/21936  / loss = 0.2060340791940689\n",
            "I0420 22:27:20.716121 139904526645120 model_training_utils.py:450] Train Step: 20068/21936  / loss = 0.5562357902526855\n",
            "I0420 22:27:21.252427 139904526645120 keras_utils.py:122] TimeHistory: 26.95 seconds, 14.84 examples/second between steps 30987 and 31037\n",
            "I0420 22:27:21.255509 139904526645120 model_training_utils.py:450] Train Step: 20069/21936  / loss = 0.12249653786420822\n",
            "I0420 22:27:21.792435 139904526645120 model_training_utils.py:450] Train Step: 20070/21936  / loss = 0.32995861768722534\n",
            "I0420 22:27:22.329249 139904526645120 model_training_utils.py:450] Train Step: 20071/21936  / loss = 1.211849570274353\n",
            "I0420 22:27:22.867152 139904526645120 model_training_utils.py:450] Train Step: 20072/21936  / loss = 0.11541590094566345\n",
            "I0420 22:27:23.405062 139904526645120 model_training_utils.py:450] Train Step: 20073/21936  / loss = 0.16861245036125183\n",
            "I0420 22:27:23.940991 139904526645120 model_training_utils.py:450] Train Step: 20074/21936  / loss = 0.3588141202926636\n",
            "I0420 22:27:24.476172 139904526645120 model_training_utils.py:450] Train Step: 20075/21936  / loss = 0.44608640670776367\n",
            "I0420 22:27:25.013820 139904526645120 model_training_utils.py:450] Train Step: 20076/21936  / loss = 0.1674940288066864\n",
            "I0420 22:27:25.552382 139904526645120 model_training_utils.py:450] Train Step: 20077/21936  / loss = 0.12008252739906311\n",
            "I0420 22:27:26.088369 139904526645120 model_training_utils.py:450] Train Step: 20078/21936  / loss = 0.3755931854248047\n",
            "I0420 22:27:26.626888 139904526645120 model_training_utils.py:450] Train Step: 20079/21936  / loss = 0.37435925006866455\n",
            "I0420 22:27:27.164051 139904526645120 model_training_utils.py:450] Train Step: 20080/21936  / loss = 0.5725579261779785\n",
            "I0420 22:27:27.701023 139904526645120 model_training_utils.py:450] Train Step: 20081/21936  / loss = 0.6256675124168396\n",
            "I0420 22:27:28.241257 139904526645120 model_training_utils.py:450] Train Step: 20082/21936  / loss = 0.7042824625968933\n",
            "I0420 22:27:28.775626 139904526645120 model_training_utils.py:450] Train Step: 20083/21936  / loss = 0.10083204507827759\n",
            "I0420 22:27:29.312505 139904526645120 model_training_utils.py:450] Train Step: 20084/21936  / loss = 0.5265403985977173\n",
            "I0420 22:27:29.849208 139904526645120 model_training_utils.py:450] Train Step: 20085/21936  / loss = 0.15993760526180267\n",
            "I0420 22:27:30.386042 139904526645120 model_training_utils.py:450] Train Step: 20086/21936  / loss = 1.1473716497421265\n",
            "I0420 22:27:30.921660 139904526645120 model_training_utils.py:450] Train Step: 20087/21936  / loss = 0.47670966386795044\n",
            "I0420 22:27:31.456764 139904526645120 model_training_utils.py:450] Train Step: 20088/21936  / loss = 0.6361169219017029\n",
            "I0420 22:27:31.993286 139904526645120 model_training_utils.py:450] Train Step: 20089/21936  / loss = 0.14260819554328918\n",
            "I0420 22:27:32.530790 139904526645120 model_training_utils.py:450] Train Step: 20090/21936  / loss = 0.384844034910202\n",
            "I0420 22:27:33.068925 139904526645120 model_training_utils.py:450] Train Step: 20091/21936  / loss = 0.18625402450561523\n",
            "I0420 22:27:33.606037 139904526645120 model_training_utils.py:450] Train Step: 20092/21936  / loss = 1.2326772212982178\n",
            "I0420 22:27:34.142286 139904526645120 model_training_utils.py:450] Train Step: 20093/21936  / loss = 0.7992990016937256\n",
            "I0420 22:27:34.678996 139904526645120 model_training_utils.py:450] Train Step: 20094/21936  / loss = 0.3410700261592865\n",
            "I0420 22:27:35.215663 139904526645120 model_training_utils.py:450] Train Step: 20095/21936  / loss = 0.18100979924201965\n",
            "I0420 22:27:35.755467 139904526645120 model_training_utils.py:450] Train Step: 20096/21936  / loss = 0.5365362167358398\n",
            "I0420 22:27:36.294258 139904526645120 model_training_utils.py:450] Train Step: 20097/21936  / loss = 0.3159005641937256\n",
            "I0420 22:27:36.832374 139904526645120 model_training_utils.py:450] Train Step: 20098/21936  / loss = 0.5011163353919983\n",
            "I0420 22:27:37.369834 139904526645120 model_training_utils.py:450] Train Step: 20099/21936  / loss = 0.24460594356060028\n",
            "I0420 22:27:37.907902 139904526645120 model_training_utils.py:450] Train Step: 20100/21936  / loss = 0.3974827527999878\n",
            "I0420 22:27:38.444080 139904526645120 model_training_utils.py:450] Train Step: 20101/21936  / loss = 0.30413317680358887\n",
            "I0420 22:27:38.978544 139904526645120 model_training_utils.py:450] Train Step: 20102/21936  / loss = 0.1618260145187378\n",
            "I0420 22:27:39.514916 139904526645120 model_training_utils.py:450] Train Step: 20103/21936  / loss = 0.3243238925933838\n",
            "I0420 22:27:40.063220 139904526645120 model_training_utils.py:450] Train Step: 20104/21936  / loss = 0.2202000617980957\n",
            "I0420 22:27:40.599794 139904526645120 model_training_utils.py:450] Train Step: 20105/21936  / loss = 0.5395194888114929\n",
            "I0420 22:27:41.140028 139904526645120 model_training_utils.py:450] Train Step: 20106/21936  / loss = 0.2189224809408188\n",
            "I0420 22:27:41.682595 139904526645120 model_training_utils.py:450] Train Step: 20107/21936  / loss = 0.4671928882598877\n",
            "I0420 22:27:42.220351 139904526645120 model_training_utils.py:450] Train Step: 20108/21936  / loss = 0.5654231309890747\n",
            "I0420 22:27:42.758987 139904526645120 model_training_utils.py:450] Train Step: 20109/21936  / loss = 0.43491992354393005\n",
            "I0420 22:27:43.296972 139904526645120 model_training_utils.py:450] Train Step: 20110/21936  / loss = 0.08543233573436737\n",
            "I0420 22:27:43.846693 139904526645120 model_training_utils.py:450] Train Step: 20111/21936  / loss = 0.426540732383728\n",
            "I0420 22:27:44.385827 139904526645120 model_training_utils.py:450] Train Step: 20112/21936  / loss = 0.1893099844455719\n",
            "I0420 22:27:44.923475 139904526645120 model_training_utils.py:450] Train Step: 20113/21936  / loss = 0.6747114658355713\n",
            "I0420 22:27:45.460790 139904526645120 model_training_utils.py:450] Train Step: 20114/21936  / loss = 0.5064423680305481\n",
            "I0420 22:27:45.998697 139904526645120 model_training_utils.py:450] Train Step: 20115/21936  / loss = 0.7327119708061218\n",
            "I0420 22:27:46.534583 139904526645120 model_training_utils.py:450] Train Step: 20116/21936  / loss = 0.38743460178375244\n",
            "I0420 22:27:47.072751 139904526645120 model_training_utils.py:450] Train Step: 20117/21936  / loss = 0.5449823141098022\n",
            "I0420 22:27:47.607638 139904526645120 model_training_utils.py:450] Train Step: 20118/21936  / loss = 0.8669973611831665\n",
            "I0420 22:27:48.144794 139904526645120 keras_utils.py:122] TimeHistory: 26.89 seconds, 14.88 examples/second between steps 31037 and 31087\n",
            "I0420 22:27:48.147717 139904526645120 model_training_utils.py:450] Train Step: 20119/21936  / loss = 1.0752496719360352\n",
            "I0420 22:27:48.685077 139904526645120 model_training_utils.py:450] Train Step: 20120/21936  / loss = 0.20945590734481812\n",
            "I0420 22:27:49.225684 139904526645120 model_training_utils.py:450] Train Step: 20121/21936  / loss = 1.0818604230880737\n",
            "I0420 22:27:49.761247 139904526645120 model_training_utils.py:450] Train Step: 20122/21936  / loss = 0.8992325067520142\n",
            "I0420 22:27:50.297656 139904526645120 model_training_utils.py:450] Train Step: 20123/21936  / loss = 1.1090075969696045\n",
            "I0420 22:27:50.834850 139904526645120 model_training_utils.py:450] Train Step: 20124/21936  / loss = 1.228833556175232\n",
            "I0420 22:27:51.372717 139904526645120 model_training_utils.py:450] Train Step: 20125/21936  / loss = 0.7639751434326172\n",
            "I0420 22:27:51.910060 139904526645120 model_training_utils.py:450] Train Step: 20126/21936  / loss = 1.5115447044372559\n",
            "I0420 22:27:52.447488 139904526645120 model_training_utils.py:450] Train Step: 20127/21936  / loss = 1.032151460647583\n",
            "I0420 22:27:52.985414 139904526645120 model_training_utils.py:450] Train Step: 20128/21936  / loss = 0.6462436318397522\n",
            "I0420 22:27:53.526680 139904526645120 model_training_utils.py:450] Train Step: 20129/21936  / loss = 0.5280802249908447\n",
            "I0420 22:27:54.060633 139904526645120 model_training_utils.py:450] Train Step: 20130/21936  / loss = 1.0094128847122192\n",
            "I0420 22:27:54.596965 139904526645120 model_training_utils.py:450] Train Step: 20131/21936  / loss = 1.3068292140960693\n",
            "I0420 22:27:55.135890 139904526645120 model_training_utils.py:450] Train Step: 20132/21936  / loss = 1.8081799745559692\n",
            "I0420 22:27:55.675310 139904526645120 model_training_utils.py:450] Train Step: 20133/21936  / loss = 0.43408605456352234\n",
            "I0420 22:27:56.214731 139904526645120 model_training_utils.py:450] Train Step: 20134/21936  / loss = 0.29719656705856323\n",
            "I0420 22:27:56.751557 139904526645120 model_training_utils.py:450] Train Step: 20135/21936  / loss = 1.174294352531433\n",
            "I0420 22:27:57.288849 139904526645120 model_training_utils.py:450] Train Step: 20136/21936  / loss = 0.48406946659088135\n",
            "I0420 22:27:57.830271 139904526645120 model_training_utils.py:450] Train Step: 20137/21936  / loss = 0.8887975215911865\n",
            "I0420 22:27:58.368382 139904526645120 model_training_utils.py:450] Train Step: 20138/21936  / loss = 1.0256140232086182\n",
            "I0420 22:27:58.906025 139904526645120 model_training_utils.py:450] Train Step: 20139/21936  / loss = 1.3288700580596924\n",
            "I0420 22:27:59.446437 139904526645120 model_training_utils.py:450] Train Step: 20140/21936  / loss = 0.6421161890029907\n",
            "I0420 22:27:59.984069 139904526645120 model_training_utils.py:450] Train Step: 20141/21936  / loss = 0.8889837265014648\n",
            "I0420 22:28:00.523205 139904526645120 model_training_utils.py:450] Train Step: 20142/21936  / loss = 0.6550431847572327\n",
            "I0420 22:28:01.060109 139904526645120 model_training_utils.py:450] Train Step: 20143/21936  / loss = 0.2840498685836792\n",
            "I0420 22:28:01.598880 139904526645120 model_training_utils.py:450] Train Step: 20144/21936  / loss = 0.48400741815567017\n",
            "I0420 22:28:02.136195 139904526645120 model_training_utils.py:450] Train Step: 20145/21936  / loss = 0.3961787223815918\n",
            "I0420 22:28:02.672385 139904526645120 model_training_utils.py:450] Train Step: 20146/21936  / loss = 0.29651200771331787\n",
            "I0420 22:28:03.209729 139904526645120 model_training_utils.py:450] Train Step: 20147/21936  / loss = 0.25488364696502686\n",
            "I0420 22:28:03.747473 139904526645120 model_training_utils.py:450] Train Step: 20148/21936  / loss = 1.0065414905548096\n",
            "I0420 22:28:04.283021 139904526645120 model_training_utils.py:450] Train Step: 20149/21936  / loss = 2.1916565895080566\n",
            "I0420 22:28:04.819998 139904526645120 model_training_utils.py:450] Train Step: 20150/21936  / loss = 0.5415329933166504\n",
            "I0420 22:28:05.357614 139904526645120 model_training_utils.py:450] Train Step: 20151/21936  / loss = 0.9330506324768066\n",
            "I0420 22:28:05.895361 139904526645120 model_training_utils.py:450] Train Step: 20152/21936  / loss = 1.1012721061706543\n",
            "I0420 22:28:06.432057 139904526645120 model_training_utils.py:450] Train Step: 20153/21936  / loss = 0.5319262146949768\n",
            "I0420 22:28:06.968477 139904526645120 model_training_utils.py:450] Train Step: 20154/21936  / loss = 0.8386332988739014\n",
            "I0420 22:28:07.505598 139904526645120 model_training_utils.py:450] Train Step: 20155/21936  / loss = 1.2970197200775146\n",
            "I0420 22:28:08.042877 139904526645120 model_training_utils.py:450] Train Step: 20156/21936  / loss = 0.633636474609375\n",
            "I0420 22:28:08.580377 139904526645120 model_training_utils.py:450] Train Step: 20157/21936  / loss = 0.5897625684738159\n",
            "I0420 22:28:09.118145 139904526645120 model_training_utils.py:450] Train Step: 20158/21936  / loss = 0.48013317584991455\n",
            "I0420 22:28:09.657844 139904526645120 model_training_utils.py:450] Train Step: 20159/21936  / loss = 1.070157766342163\n",
            "I0420 22:28:10.200181 139904526645120 model_training_utils.py:450] Train Step: 20160/21936  / loss = 0.4262275695800781\n",
            "I0420 22:28:10.738723 139904526645120 model_training_utils.py:450] Train Step: 20161/21936  / loss = 0.20484203100204468\n",
            "I0420 22:28:11.274761 139904526645120 model_training_utils.py:450] Train Step: 20162/21936  / loss = 0.23953993618488312\n",
            "I0420 22:28:11.813674 139904526645120 model_training_utils.py:450] Train Step: 20163/21936  / loss = 0.9906253814697266\n",
            "I0420 22:28:12.351016 139904526645120 model_training_utils.py:450] Train Step: 20164/21936  / loss = 0.6962036490440369\n",
            "I0420 22:28:12.889374 139904526645120 model_training_utils.py:450] Train Step: 20165/21936  / loss = 0.21217606961727142\n",
            "I0420 22:28:13.427242 139904526645120 model_training_utils.py:450] Train Step: 20166/21936  / loss = 0.6141455173492432\n",
            "I0420 22:28:13.964472 139904526645120 model_training_utils.py:450] Train Step: 20167/21936  / loss = 0.5321775078773499\n",
            "I0420 22:28:14.500716 139904526645120 model_training_utils.py:450] Train Step: 20168/21936  / loss = 1.0544065237045288\n",
            "I0420 22:28:15.037303 139904526645120 keras_utils.py:122] TimeHistory: 26.89 seconds, 14.88 examples/second between steps 31087 and 31137\n",
            "I0420 22:28:15.040028 139904526645120 model_training_utils.py:450] Train Step: 20169/21936  / loss = 0.5523645877838135\n",
            "I0420 22:28:15.577250 139904526645120 model_training_utils.py:450] Train Step: 20170/21936  / loss = 0.32950130105018616\n",
            "I0420 22:28:16.114630 139904526645120 model_training_utils.py:450] Train Step: 20171/21936  / loss = 0.48350659012794495\n",
            "I0420 22:28:16.652860 139904526645120 model_training_utils.py:450] Train Step: 20172/21936  / loss = 0.26500576734542847\n",
            "I0420 22:28:17.191446 139904526645120 model_training_utils.py:450] Train Step: 20173/21936  / loss = 0.14942771196365356\n",
            "I0420 22:28:17.728609 139904526645120 model_training_utils.py:450] Train Step: 20174/21936  / loss = 0.3576410412788391\n",
            "I0420 22:28:18.269271 139904526645120 model_training_utils.py:450] Train Step: 20175/21936  / loss = 0.6661577224731445\n",
            "I0420 22:28:18.806684 139904526645120 model_training_utils.py:450] Train Step: 20176/21936  / loss = 0.34792256355285645\n",
            "I0420 22:28:19.347677 139904526645120 model_training_utils.py:450] Train Step: 20177/21936  / loss = 0.30757299065589905\n",
            "I0420 22:28:19.885207 139904526645120 model_training_utils.py:450] Train Step: 20178/21936  / loss = 0.167804554104805\n",
            "I0420 22:28:20.424399 139904526645120 model_training_utils.py:450] Train Step: 20179/21936  / loss = 0.7380982041358948\n",
            "I0420 22:28:20.960847 139904526645120 model_training_utils.py:450] Train Step: 20180/21936  / loss = 1.3461977243423462\n",
            "I0420 22:28:21.499869 139904526645120 model_training_utils.py:450] Train Step: 20181/21936  / loss = 0.3724156618118286\n",
            "I0420 22:28:22.037235 139904526645120 model_training_utils.py:450] Train Step: 20182/21936  / loss = 0.9707512855529785\n",
            "I0420 22:28:22.574917 139904526645120 model_training_utils.py:450] Train Step: 20183/21936  / loss = 0.1296565979719162\n",
            "I0420 22:28:23.109334 139904526645120 model_training_utils.py:450] Train Step: 20184/21936  / loss = 0.4014761447906494\n",
            "I0420 22:28:23.648262 139904526645120 model_training_utils.py:450] Train Step: 20185/21936  / loss = 0.30961716175079346\n",
            "I0420 22:28:24.195239 139904526645120 model_training_utils.py:450] Train Step: 20186/21936  / loss = 0.48803219199180603\n",
            "I0420 22:28:24.733632 139904526645120 model_training_utils.py:450] Train Step: 20187/21936  / loss = 0.2860294580459595\n",
            "I0420 22:28:25.272216 139904526645120 model_training_utils.py:450] Train Step: 20188/21936  / loss = 0.47651711106300354\n",
            "I0420 22:28:25.810069 139904526645120 model_training_utils.py:450] Train Step: 20189/21936  / loss = 0.2053098976612091\n",
            "I0420 22:28:26.349439 139904526645120 model_training_utils.py:450] Train Step: 20190/21936  / loss = 0.18211182951927185\n",
            "I0420 22:28:26.887174 139904526645120 model_training_utils.py:450] Train Step: 20191/21936  / loss = 0.3218924403190613\n",
            "I0420 22:28:27.423429 139904526645120 model_training_utils.py:450] Train Step: 20192/21936  / loss = 0.5098192095756531\n",
            "I0420 22:28:27.969691 139904526645120 model_training_utils.py:450] Train Step: 20193/21936  / loss = 0.2567302882671356\n",
            "I0420 22:28:28.508385 139904526645120 model_training_utils.py:450] Train Step: 20194/21936  / loss = 0.2791193127632141\n",
            "I0420 22:28:29.044917 139904526645120 model_training_utils.py:450] Train Step: 20195/21936  / loss = 0.045941852033138275\n",
            "I0420 22:28:29.584361 139904526645120 model_training_utils.py:450] Train Step: 20196/21936  / loss = 0.28839394450187683\n",
            "I0420 22:28:30.123273 139904526645120 model_training_utils.py:450] Train Step: 20197/21936  / loss = 0.14309675991535187\n",
            "I0420 22:28:30.660928 139904526645120 model_training_utils.py:450] Train Step: 20198/21936  / loss = 1.3400917053222656\n",
            "I0420 22:28:31.199288 139904526645120 model_training_utils.py:450] Train Step: 20199/21936  / loss = 0.16164028644561768\n",
            "I0420 22:28:31.737939 139904526645120 model_training_utils.py:450] Train Step: 20200/21936  / loss = 0.6184603571891785\n",
            "I0420 22:28:32.280172 139904526645120 model_training_utils.py:450] Train Step: 20201/21936  / loss = 1.381734848022461\n",
            "I0420 22:28:32.818758 139904526645120 model_training_utils.py:450] Train Step: 20202/21936  / loss = 2.1761069297790527\n",
            "I0420 22:28:33.362685 139904526645120 model_training_utils.py:450] Train Step: 20203/21936  / loss = 1.4011075496673584\n",
            "I0420 22:28:33.902309 139904526645120 model_training_utils.py:450] Train Step: 20204/21936  / loss = 0.8297727108001709\n",
            "I0420 22:28:34.441855 139904526645120 model_training_utils.py:450] Train Step: 20205/21936  / loss = 0.4760066270828247\n",
            "I0420 22:28:34.981302 139904526645120 model_training_utils.py:450] Train Step: 20206/21936  / loss = 2.836014986038208\n",
            "I0420 22:28:35.523195 139904526645120 model_training_utils.py:450] Train Step: 20207/21936  / loss = 1.9931682348251343\n",
            "I0420 22:28:36.061742 139904526645120 model_training_utils.py:450] Train Step: 20208/21936  / loss = 2.734931230545044\n",
            "I0420 22:28:36.599944 139904526645120 model_training_utils.py:450] Train Step: 20209/21936  / loss = 1.47954523563385\n",
            "I0420 22:28:37.141074 139904526645120 model_training_utils.py:450] Train Step: 20210/21936  / loss = 1.8685336112976074\n",
            "I0420 22:28:37.680181 139904526645120 model_training_utils.py:450] Train Step: 20211/21936  / loss = 2.558377981185913\n",
            "I0420 22:28:38.218922 139904526645120 model_training_utils.py:450] Train Step: 20212/21936  / loss = 1.7335675954818726\n",
            "I0420 22:28:38.756064 139904526645120 model_training_utils.py:450] Train Step: 20213/21936  / loss = 2.3732829093933105\n",
            "I0420 22:28:39.294647 139904526645120 model_training_utils.py:450] Train Step: 20214/21936  / loss = 2.3424038887023926\n",
            "I0420 22:28:39.833899 139904526645120 model_training_utils.py:450] Train Step: 20215/21936  / loss = 2.4674806594848633\n",
            "I0420 22:28:40.374149 139904526645120 model_training_utils.py:450] Train Step: 20216/21936  / loss = 1.8702003955841064\n",
            "I0420 22:28:40.912283 139904526645120 model_training_utils.py:450] Train Step: 20217/21936  / loss = 2.1209867000579834\n",
            "I0420 22:28:41.448288 139904526645120 model_training_utils.py:450] Train Step: 20218/21936  / loss = 2.521367311477661\n",
            "I0420 22:28:41.988327 139904526645120 keras_utils.py:122] TimeHistory: 26.95 seconds, 14.84 examples/second between steps 31137 and 31187\n",
            "I0420 22:28:41.991228 139904526645120 model_training_utils.py:450] Train Step: 20219/21936  / loss = 1.1129040718078613\n",
            "I0420 22:28:42.537231 139904526645120 model_training_utils.py:450] Train Step: 20220/21936  / loss = 1.825804591178894\n",
            "I0420 22:28:43.077636 139904526645120 model_training_utils.py:450] Train Step: 20221/21936  / loss = 2.5555663108825684\n",
            "I0420 22:28:43.615683 139904526645120 model_training_utils.py:450] Train Step: 20222/21936  / loss = 2.4030184745788574\n",
            "I0420 22:28:44.155520 139904526645120 model_training_utils.py:450] Train Step: 20223/21936  / loss = 2.4542312622070312\n",
            "I0420 22:28:44.691939 139904526645120 model_training_utils.py:450] Train Step: 20224/21936  / loss = 0.6395498514175415\n",
            "I0420 22:28:45.231225 139904526645120 model_training_utils.py:450] Train Step: 20225/21936  / loss = 1.9917088747024536\n",
            "I0420 22:28:45.769860 139904526645120 model_training_utils.py:450] Train Step: 20226/21936  / loss = 1.0072932243347168\n",
            "I0420 22:28:46.308247 139904526645120 model_training_utils.py:450] Train Step: 20227/21936  / loss = 2.271479606628418\n",
            "I0420 22:28:46.846370 139904526645120 model_training_utils.py:450] Train Step: 20228/21936  / loss = 1.4462904930114746\n",
            "I0420 22:28:47.385181 139904526645120 model_training_utils.py:450] Train Step: 20229/21936  / loss = 1.3958277702331543\n",
            "I0420 22:28:47.922199 139904526645120 model_training_utils.py:450] Train Step: 20230/21936  / loss = 1.1056604385375977\n",
            "I0420 22:28:48.460268 139904526645120 model_training_utils.py:450] Train Step: 20231/21936  / loss = 1.2749834060668945\n",
            "I0420 22:28:48.997107 139904526645120 model_training_utils.py:450] Train Step: 20232/21936  / loss = 1.6170172691345215\n",
            "I0420 22:28:49.535226 139904526645120 model_training_utils.py:450] Train Step: 20233/21936  / loss = 0.4338000416755676\n",
            "I0420 22:28:50.073928 139904526645120 model_training_utils.py:450] Train Step: 20234/21936  / loss = 0.9659096002578735\n",
            "I0420 22:28:50.612109 139904526645120 model_training_utils.py:450] Train Step: 20235/21936  / loss = 1.1158949136734009\n",
            "I0420 22:28:51.149476 139904526645120 model_training_utils.py:450] Train Step: 20236/21936  / loss = 1.287636399269104\n",
            "I0420 22:28:51.688792 139904526645120 model_training_utils.py:450] Train Step: 20237/21936  / loss = 0.37330836057662964\n",
            "I0420 22:28:52.227432 139904526645120 model_training_utils.py:450] Train Step: 20238/21936  / loss = 0.6825371384620667\n",
            "I0420 22:28:52.763664 139904526645120 model_training_utils.py:450] Train Step: 20239/21936  / loss = 1.0659831762313843\n",
            "I0420 22:28:53.300997 139904526645120 model_training_utils.py:450] Train Step: 20240/21936  / loss = 0.11987312883138657\n",
            "I0420 22:28:53.841636 139904526645120 model_training_utils.py:450] Train Step: 20241/21936  / loss = 0.6887544393539429\n",
            "I0420 22:28:54.381702 139904526645120 model_training_utils.py:450] Train Step: 20242/21936  / loss = 0.19906115531921387\n",
            "I0420 22:28:54.922720 139904526645120 model_training_utils.py:450] Train Step: 20243/21936  / loss = 0.4395827054977417\n",
            "I0420 22:28:55.457814 139904526645120 model_training_utils.py:450] Train Step: 20244/21936  / loss = 1.6057268381118774\n",
            "I0420 22:28:55.998631 139904526645120 model_training_utils.py:450] Train Step: 20245/21936  / loss = 1.1117457151412964\n",
            "I0420 22:28:56.534811 139904526645120 model_training_utils.py:450] Train Step: 20246/21936  / loss = 0.6600053310394287\n",
            "I0420 22:28:57.071047 139904526645120 model_training_utils.py:450] Train Step: 20247/21936  / loss = 0.6589028835296631\n",
            "I0420 22:28:57.608541 139904526645120 model_training_utils.py:450] Train Step: 20248/21936  / loss = 0.32293233275413513\n",
            "I0420 22:28:58.144521 139904526645120 model_training_utils.py:450] Train Step: 20249/21936  / loss = 0.8833115100860596\n",
            "I0420 22:28:58.681914 139904526645120 model_training_utils.py:450] Train Step: 20250/21936  / loss = 1.1385467052459717\n",
            "I0420 22:28:59.219404 139904526645120 model_training_utils.py:450] Train Step: 20251/21936  / loss = 1.2475802898406982\n",
            "I0420 22:28:59.754854 139904526645120 model_training_utils.py:450] Train Step: 20252/21936  / loss = 0.6298134326934814\n",
            "I0420 22:29:00.302216 139904526645120 model_training_utils.py:450] Train Step: 20253/21936  / loss = 0.15528807044029236\n",
            "I0420 22:29:00.845501 139904526645120 model_training_utils.py:450] Train Step: 20254/21936  / loss = 0.43713974952697754\n",
            "I0420 22:29:01.385090 139904526645120 model_training_utils.py:450] Train Step: 20255/21936  / loss = 0.11660617589950562\n",
            "I0420 22:29:01.924796 139904526645120 model_training_utils.py:450] Train Step: 20256/21936  / loss = 0.5053025484085083\n",
            "I0420 22:29:02.461745 139904526645120 model_training_utils.py:450] Train Step: 20257/21936  / loss = 0.6082364916801453\n",
            "I0420 22:29:03.000489 139904526645120 model_training_utils.py:450] Train Step: 20258/21936  / loss = 0.9958406090736389\n",
            "I0420 22:29:03.535520 139904526645120 model_training_utils.py:450] Train Step: 20259/21936  / loss = 0.8161731958389282\n",
            "I0420 22:29:04.075359 139904526645120 model_training_utils.py:450] Train Step: 20260/21936  / loss = 0.8290925025939941\n",
            "I0420 22:29:04.611431 139904526645120 model_training_utils.py:450] Train Step: 20261/21936  / loss = 0.8057381510734558\n",
            "I0420 22:29:05.149827 139904526645120 model_training_utils.py:450] Train Step: 20262/21936  / loss = 0.4115680456161499\n",
            "I0420 22:29:05.688421 139904526645120 model_training_utils.py:450] Train Step: 20263/21936  / loss = 0.6643862724304199\n",
            "I0420 22:29:06.227716 139904526645120 model_training_utils.py:450] Train Step: 20264/21936  / loss = 0.5030041337013245\n",
            "I0420 22:29:06.764702 139904526645120 model_training_utils.py:450] Train Step: 20265/21936  / loss = 0.492727667093277\n",
            "I0420 22:29:07.302196 139904526645120 model_training_utils.py:450] Train Step: 20266/21936  / loss = 1.5941410064697266\n",
            "I0420 22:29:07.841556 139904526645120 model_training_utils.py:450] Train Step: 20267/21936  / loss = 0.2030218094587326\n",
            "I0420 22:29:08.377800 139904526645120 model_training_utils.py:450] Train Step: 20268/21936  / loss = 0.29800257086753845\n",
            "I0420 22:29:08.917264 139904526645120 keras_utils.py:122] TimeHistory: 26.93 seconds, 14.86 examples/second between steps 31187 and 31237\n",
            "I0420 22:29:08.919965 139904526645120 model_training_utils.py:450] Train Step: 20269/21936  / loss = 0.44551771879196167\n",
            "I0420 22:29:09.457207 139904526645120 model_training_utils.py:450] Train Step: 20270/21936  / loss = 0.1412927210330963\n",
            "I0420 22:29:09.994237 139904526645120 model_training_utils.py:450] Train Step: 20271/21936  / loss = 0.1972040832042694\n",
            "I0420 22:29:10.529451 139904526645120 model_training_utils.py:450] Train Step: 20272/21936  / loss = 0.26909762620925903\n",
            "I0420 22:29:11.069149 139904526645120 model_training_utils.py:450] Train Step: 20273/21936  / loss = 0.1030731052160263\n",
            "I0420 22:29:11.608110 139904526645120 model_training_utils.py:450] Train Step: 20274/21936  / loss = 0.3852366805076599\n",
            "I0420 22:29:12.145693 139904526645120 model_training_utils.py:450] Train Step: 20275/21936  / loss = 0.27640703320503235\n",
            "I0420 22:29:12.682473 139904526645120 model_training_utils.py:450] Train Step: 20276/21936  / loss = 0.9625757932662964\n",
            "I0420 22:29:13.220741 139904526645120 model_training_utils.py:450] Train Step: 20277/21936  / loss = 0.8166982531547546\n",
            "I0420 22:29:13.756199 139904526645120 model_training_utils.py:450] Train Step: 20278/21936  / loss = 0.16691891849040985\n",
            "I0420 22:29:14.294405 139904526645120 model_training_utils.py:450] Train Step: 20279/21936  / loss = 0.3747263550758362\n",
            "I0420 22:29:14.832907 139904526645120 model_training_utils.py:450] Train Step: 20280/21936  / loss = 0.8285354375839233\n",
            "I0420 22:29:15.376150 139904526645120 model_training_utils.py:450] Train Step: 20281/21936  / loss = 0.50590980052948\n",
            "I0420 22:29:15.913038 139904526645120 model_training_utils.py:450] Train Step: 20282/21936  / loss = 1.0775974988937378\n",
            "I0420 22:29:16.450126 139904526645120 model_training_utils.py:450] Train Step: 20283/21936  / loss = 0.22225072979927063\n",
            "I0420 22:29:16.996170 139904526645120 model_training_utils.py:450] Train Step: 20284/21936  / loss = 0.7386839389801025\n",
            "I0420 22:29:17.533429 139904526645120 model_training_utils.py:450] Train Step: 20285/21936  / loss = 0.7907060384750366\n",
            "I0420 22:29:18.071501 139904526645120 model_training_utils.py:450] Train Step: 20286/21936  / loss = 0.39991915225982666\n",
            "I0420 22:29:18.612364 139904526645120 model_training_utils.py:450] Train Step: 20287/21936  / loss = 0.08581512421369553\n",
            "I0420 22:29:19.150967 139904526645120 model_training_utils.py:450] Train Step: 20288/21936  / loss = 0.8307475447654724\n",
            "I0420 22:29:19.688754 139904526645120 model_training_utils.py:450] Train Step: 20289/21936  / loss = 0.4013289213180542\n",
            "I0420 22:29:20.228892 139904526645120 model_training_utils.py:450] Train Step: 20290/21936  / loss = 0.4698202311992645\n",
            "I0420 22:29:20.767691 139904526645120 model_training_utils.py:450] Train Step: 20291/21936  / loss = 0.18309906125068665\n",
            "I0420 22:29:21.303834 139904526645120 model_training_utils.py:450] Train Step: 20292/21936  / loss = 0.6536655426025391\n",
            "I0420 22:29:21.842064 139904526645120 model_training_utils.py:450] Train Step: 20293/21936  / loss = 0.6230744123458862\n",
            "I0420 22:29:22.378474 139904526645120 model_training_utils.py:450] Train Step: 20294/21936  / loss = 0.3609578013420105\n",
            "I0420 22:29:22.917279 139904526645120 model_training_utils.py:450] Train Step: 20295/21936  / loss = 0.0951065942645073\n",
            "I0420 22:29:23.453963 139904526645120 model_training_utils.py:450] Train Step: 20296/21936  / loss = 0.5384235382080078\n",
            "I0420 22:29:23.991833 139904526645120 model_training_utils.py:450] Train Step: 20297/21936  / loss = 0.5164734125137329\n",
            "I0420 22:29:24.528345 139904526645120 model_training_utils.py:450] Train Step: 20298/21936  / loss = 0.3186761140823364\n",
            "I0420 22:29:25.062286 139904526645120 model_training_utils.py:450] Train Step: 20299/21936  / loss = 0.3733293116092682\n",
            "I0420 22:29:25.599794 139904526645120 model_training_utils.py:450] Train Step: 20300/21936  / loss = 0.7047650814056396\n",
            "I0420 22:29:26.136077 139904526645120 model_training_utils.py:450] Train Step: 20301/21936  / loss = 0.27488499879837036\n",
            "I0420 22:29:26.672632 139904526645120 model_training_utils.py:450] Train Step: 20302/21936  / loss = 0.4432315230369568\n",
            "I0420 22:29:27.209791 139904526645120 model_training_utils.py:450] Train Step: 20303/21936  / loss = 0.6059763431549072\n",
            "I0420 22:29:27.745830 139904526645120 model_training_utils.py:450] Train Step: 20304/21936  / loss = 0.20589450001716614\n",
            "I0420 22:29:28.280846 139904526645120 model_training_utils.py:450] Train Step: 20305/21936  / loss = 1.1426067352294922\n",
            "I0420 22:29:28.819471 139904526645120 model_training_utils.py:450] Train Step: 20306/21936  / loss = 0.3330250382423401\n",
            "I0420 22:29:29.355701 139904526645120 model_training_utils.py:450] Train Step: 20307/21936  / loss = 0.6151763200759888\n",
            "I0420 22:29:29.893591 139904526645120 model_training_utils.py:450] Train Step: 20308/21936  / loss = 0.4336368441581726\n",
            "I0420 22:29:30.432481 139904526645120 model_training_utils.py:450] Train Step: 20309/21936  / loss = 0.7508584260940552\n",
            "I0420 22:29:30.970186 139904526645120 model_training_utils.py:450] Train Step: 20310/21936  / loss = 0.7683382034301758\n",
            "I0420 22:29:31.509078 139904526645120 model_training_utils.py:450] Train Step: 20311/21936  / loss = 0.49293357133865356\n",
            "I0420 22:29:32.050980 139904526645120 model_training_utils.py:450] Train Step: 20312/21936  / loss = 0.1975909024477005\n",
            "I0420 22:29:32.590280 139904526645120 model_training_utils.py:450] Train Step: 20313/21936  / loss = 0.5582668781280518\n",
            "I0420 22:29:33.131137 139904526645120 model_training_utils.py:450] Train Step: 20314/21936  / loss = 0.23393237590789795\n",
            "I0420 22:29:33.667895 139904526645120 model_training_utils.py:450] Train Step: 20315/21936  / loss = 0.37855392694473267\n",
            "I0420 22:29:34.204631 139904526645120 model_training_utils.py:450] Train Step: 20316/21936  / loss = 0.6647369861602783\n",
            "I0420 22:29:34.738947 139904526645120 model_training_utils.py:450] Train Step: 20317/21936  / loss = 0.8156061768531799\n",
            "I0420 22:29:35.277474 139904526645120 model_training_utils.py:450] Train Step: 20318/21936  / loss = 0.3834686279296875\n",
            "I0420 22:29:35.815505 139904526645120 keras_utils.py:122] TimeHistory: 26.89 seconds, 14.87 examples/second between steps 31237 and 31287\n",
            "I0420 22:29:35.819157 139904526645120 model_training_utils.py:450] Train Step: 20319/21936  / loss = 0.38998618721961975\n",
            "I0420 22:29:36.354930 139904526645120 model_training_utils.py:450] Train Step: 20320/21936  / loss = 1.4977507591247559\n",
            "I0420 22:29:36.892504 139904526645120 model_training_utils.py:450] Train Step: 20321/21936  / loss = 0.35548466444015503\n",
            "I0420 22:29:37.429979 139904526645120 model_training_utils.py:450] Train Step: 20322/21936  / loss = 0.5719951391220093\n",
            "I0420 22:29:37.966087 139904526645120 model_training_utils.py:450] Train Step: 20323/21936  / loss = 0.8913156986236572\n",
            "I0420 22:29:38.504673 139904526645120 model_training_utils.py:450] Train Step: 20324/21936  / loss = 0.7068628072738647\n",
            "I0420 22:29:39.040117 139904526645120 model_training_utils.py:450] Train Step: 20325/21936  / loss = 1.4838669300079346\n",
            "I0420 22:29:39.579591 139904526645120 model_training_utils.py:450] Train Step: 20326/21936  / loss = 1.2624715566635132\n",
            "I0420 22:29:40.117933 139904526645120 model_training_utils.py:450] Train Step: 20327/21936  / loss = 1.0117535591125488\n",
            "I0420 22:29:40.654100 139904526645120 model_training_utils.py:450] Train Step: 20328/21936  / loss = 0.6441587805747986\n",
            "I0420 22:29:41.191926 139904526645120 model_training_utils.py:450] Train Step: 20329/21936  / loss = 0.6363376379013062\n",
            "I0420 22:29:41.727651 139904526645120 model_training_utils.py:450] Train Step: 20330/21936  / loss = 2.342560291290283\n",
            "I0420 22:29:42.264461 139904526645120 model_training_utils.py:450] Train Step: 20331/21936  / loss = 0.4507425129413605\n",
            "I0420 22:29:42.803159 139904526645120 model_training_utils.py:450] Train Step: 20332/21936  / loss = 0.5458232760429382\n",
            "I0420 22:29:43.338506 139904526645120 model_training_utils.py:450] Train Step: 20333/21936  / loss = 0.7964685559272766\n",
            "I0420 22:29:43.884139 139904526645120 model_training_utils.py:450] Train Step: 20334/21936  / loss = 1.0068202018737793\n",
            "I0420 22:29:44.421588 139904526645120 model_training_utils.py:450] Train Step: 20335/21936  / loss = 0.6983327865600586\n",
            "I0420 22:29:44.959859 139904526645120 model_training_utils.py:450] Train Step: 20336/21936  / loss = 0.5409523248672485\n",
            "I0420 22:29:45.500812 139904526645120 model_training_utils.py:450] Train Step: 20337/21936  / loss = 0.8497998714447021\n",
            "I0420 22:29:46.038401 139904526645120 model_training_utils.py:450] Train Step: 20338/21936  / loss = 0.8067153692245483\n",
            "I0420 22:29:46.575685 139904526645120 model_training_utils.py:450] Train Step: 20339/21936  / loss = 0.8629637360572815\n",
            "I0420 22:29:47.113403 139904526645120 model_training_utils.py:450] Train Step: 20340/21936  / loss = 0.6874993443489075\n",
            "I0420 22:29:47.651190 139904526645120 model_training_utils.py:450] Train Step: 20341/21936  / loss = 0.48404818773269653\n",
            "I0420 22:29:48.188601 139904526645120 model_training_utils.py:450] Train Step: 20342/21936  / loss = 0.5707555413246155\n",
            "I0420 22:29:48.728769 139904526645120 model_training_utils.py:450] Train Step: 20343/21936  / loss = 0.4306187927722931\n",
            "I0420 22:29:49.266297 139904526645120 model_training_utils.py:450] Train Step: 20344/21936  / loss = 0.4430676996707916\n",
            "I0420 22:29:49.800274 139904526645120 model_training_utils.py:450] Train Step: 20345/21936  / loss = 0.6804419755935669\n",
            "I0420 22:29:50.336397 139904526645120 model_training_utils.py:450] Train Step: 20346/21936  / loss = 0.14768347144126892\n",
            "I0420 22:29:50.874162 139904526645120 model_training_utils.py:450] Train Step: 20347/21936  / loss = 0.31634700298309326\n",
            "I0420 22:29:51.411145 139904526645120 model_training_utils.py:450] Train Step: 20348/21936  / loss = 0.5455564260482788\n",
            "I0420 22:29:51.949221 139904526645120 model_training_utils.py:450] Train Step: 20349/21936  / loss = 0.3039476275444031\n",
            "I0420 22:29:52.486163 139904526645120 model_training_utils.py:450] Train Step: 20350/21936  / loss = 0.6415939927101135\n",
            "I0420 22:29:53.023515 139904526645120 model_training_utils.py:450] Train Step: 20351/21936  / loss = 0.4341064393520355\n",
            "I0420 22:29:53.569537 139904526645120 model_training_utils.py:450] Train Step: 20352/21936  / loss = 0.9483982920646667\n",
            "I0420 22:29:54.107218 139904526645120 model_training_utils.py:450] Train Step: 20353/21936  / loss = 0.6972174644470215\n",
            "I0420 22:29:54.645458 139904526645120 model_training_utils.py:450] Train Step: 20354/21936  / loss = 0.9079543352127075\n",
            "I0420 22:29:55.182905 139904526645120 model_training_utils.py:450] Train Step: 20355/21936  / loss = 0.70246422290802\n",
            "I0420 22:29:55.719243 139904526645120 model_training_utils.py:450] Train Step: 20356/21936  / loss = 0.5670121312141418\n",
            "I0420 22:29:56.266512 139904526645120 model_training_utils.py:450] Train Step: 20357/21936  / loss = 0.611571192741394\n",
            "I0420 22:29:56.803644 139904526645120 model_training_utils.py:450] Train Step: 20358/21936  / loss = 0.6678115129470825\n",
            "I0420 22:29:57.341908 139904526645120 model_training_utils.py:450] Train Step: 20359/21936  / loss = 0.5725667476654053\n",
            "I0420 22:29:57.886671 139904526645120 model_training_utils.py:450] Train Step: 20360/21936  / loss = 1.714965581893921\n",
            "I0420 22:29:58.425896 139904526645120 model_training_utils.py:450] Train Step: 20361/21936  / loss = 0.5506952404975891\n",
            "I0420 22:29:58.964151 139904526645120 model_training_utils.py:450] Train Step: 20362/21936  / loss = 1.0384043455123901\n",
            "I0420 22:29:59.500930 139904526645120 model_training_utils.py:450] Train Step: 20363/21936  / loss = 0.8378280401229858\n",
            "I0420 22:30:00.038764 139904526645120 model_training_utils.py:450] Train Step: 20364/21936  / loss = 0.7067912817001343\n",
            "I0420 22:30:00.575137 139904526645120 model_training_utils.py:450] Train Step: 20365/21936  / loss = 0.7080850601196289\n",
            "I0420 22:30:01.112256 139904526645120 model_training_utils.py:450] Train Step: 20366/21936  / loss = 0.36770737171173096\n",
            "I0420 22:30:01.650681 139904526645120 model_training_utils.py:450] Train Step: 20367/21936  / loss = 1.2201690673828125\n",
            "I0420 22:30:02.187434 139904526645120 model_training_utils.py:450] Train Step: 20368/21936  / loss = 0.5280892848968506\n",
            "I0420 22:30:02.723358 139904526645120 keras_utils.py:122] TimeHistory: 26.90 seconds, 14.87 examples/second between steps 31287 and 31337\n",
            "I0420 22:30:02.726288 139904526645120 model_training_utils.py:450] Train Step: 20369/21936  / loss = 0.765427827835083\n",
            "I0420 22:30:03.265114 139904526645120 model_training_utils.py:450] Train Step: 20370/21936  / loss = 1.281723976135254\n",
            "I0420 22:30:03.803599 139904526645120 model_training_utils.py:450] Train Step: 20371/21936  / loss = 0.34698230028152466\n",
            "I0420 22:30:04.342721 139904526645120 model_training_utils.py:450] Train Step: 20372/21936  / loss = 0.25480031967163086\n",
            "I0420 22:30:04.880399 139904526645120 model_training_utils.py:450] Train Step: 20373/21936  / loss = 0.16637460887432098\n",
            "I0420 22:30:05.421685 139904526645120 model_training_utils.py:450] Train Step: 20374/21936  / loss = 1.3360408544540405\n",
            "I0420 22:30:05.959134 139904526645120 model_training_utils.py:450] Train Step: 20375/21936  / loss = 1.2045323848724365\n",
            "I0420 22:30:06.497593 139904526645120 model_training_utils.py:450] Train Step: 20376/21936  / loss = 0.5953418016433716\n",
            "I0420 22:30:07.036599 139904526645120 model_training_utils.py:450] Train Step: 20377/21936  / loss = 0.9006169438362122\n",
            "I0420 22:30:07.572723 139904526645120 model_training_utils.py:450] Train Step: 20378/21936  / loss = 0.7532695531845093\n",
            "I0420 22:30:08.108598 139904526645120 model_training_utils.py:450] Train Step: 20379/21936  / loss = 1.238498568534851\n",
            "I0420 22:30:08.646251 139904526645120 model_training_utils.py:450] Train Step: 20380/21936  / loss = 0.7885436415672302\n",
            "I0420 22:30:09.180220 139904526645120 model_training_utils.py:450] Train Step: 20381/21936  / loss = 0.5060541033744812\n",
            "I0420 22:30:09.715772 139904526645120 model_training_utils.py:450] Train Step: 20382/21936  / loss = 0.6501113176345825\n",
            "I0420 22:30:10.252217 139904526645120 model_training_utils.py:450] Train Step: 20383/21936  / loss = 0.5360286831855774\n",
            "I0420 22:30:10.789845 139904526645120 model_training_utils.py:450] Train Step: 20384/21936  / loss = 1.0768263339996338\n",
            "I0420 22:30:11.328172 139904526645120 model_training_utils.py:450] Train Step: 20385/21936  / loss = 0.38141682744026184\n",
            "I0420 22:30:11.864874 139904526645120 model_training_utils.py:450] Train Step: 20386/21936  / loss = 1.5112457275390625\n",
            "I0420 22:30:12.406309 139904526645120 model_training_utils.py:450] Train Step: 20387/21936  / loss = 0.5573859214782715\n",
            "I0420 22:30:12.949865 139904526645120 model_training_utils.py:450] Train Step: 20388/21936  / loss = 0.7082034349441528\n",
            "I0420 22:30:13.486833 139904526645120 model_training_utils.py:450] Train Step: 20389/21936  / loss = 1.6151034832000732\n",
            "I0420 22:30:14.024906 139904526645120 model_training_utils.py:450] Train Step: 20390/21936  / loss = 0.522699236869812\n",
            "I0420 22:30:14.565144 139904526645120 model_training_utils.py:450] Train Step: 20391/21936  / loss = 1.1580628156661987\n",
            "I0420 22:30:15.102313 139904526645120 model_training_utils.py:450] Train Step: 20392/21936  / loss = 0.7665697336196899\n",
            "I0420 22:30:15.639203 139904526645120 model_training_utils.py:450] Train Step: 20393/21936  / loss = 0.671356201171875\n",
            "I0420 22:30:16.177616 139904526645120 model_training_utils.py:450] Train Step: 20394/21936  / loss = 1.2007673978805542\n",
            "I0420 22:30:16.713995 139904526645120 model_training_utils.py:450] Train Step: 20395/21936  / loss = 1.5559170246124268\n",
            "I0420 22:30:17.250203 139904526645120 model_training_utils.py:450] Train Step: 20396/21936  / loss = 1.599968433380127\n",
            "I0420 22:30:17.788022 139904526645120 model_training_utils.py:450] Train Step: 20397/21936  / loss = 0.6502387523651123\n",
            "I0420 22:30:18.326122 139904526645120 model_training_utils.py:450] Train Step: 20398/21936  / loss = 0.7224688529968262\n",
            "I0420 22:30:18.865042 139904526645120 model_training_utils.py:450] Train Step: 20399/21936  / loss = 0.6133694648742676\n",
            "I0420 22:30:19.400790 139904526645120 model_training_utils.py:450] Train Step: 20400/21936  / loss = 0.7305178642272949\n",
            "I0420 22:30:19.937690 139904526645120 model_training_utils.py:450] Train Step: 20401/21936  / loss = 0.35118526220321655\n",
            "I0420 22:30:20.475255 139904526645120 model_training_utils.py:450] Train Step: 20402/21936  / loss = 1.952413558959961\n",
            "I0420 22:30:21.013183 139904526645120 model_training_utils.py:450] Train Step: 20403/21936  / loss = 0.5760093331336975\n",
            "I0420 22:30:21.551422 139904526645120 model_training_utils.py:450] Train Step: 20404/21936  / loss = 0.7297859191894531\n",
            "I0420 22:30:22.090518 139904526645120 model_training_utils.py:450] Train Step: 20405/21936  / loss = 0.6328455209732056\n",
            "I0420 22:30:22.627709 139904526645120 model_training_utils.py:450] Train Step: 20406/21936  / loss = 0.40385180711746216\n",
            "I0420 22:30:23.167927 139904526645120 model_training_utils.py:450] Train Step: 20407/21936  / loss = 0.5899070501327515\n",
            "I0420 22:30:23.704815 139904526645120 model_training_utils.py:450] Train Step: 20408/21936  / loss = 0.7078078389167786\n",
            "I0420 22:30:24.243856 139904526645120 model_training_utils.py:450] Train Step: 20409/21936  / loss = 1.0712051391601562\n",
            "I0420 22:30:24.779913 139904526645120 model_training_utils.py:450] Train Step: 20410/21936  / loss = 0.8004976511001587\n",
            "I0420 22:30:25.321130 139904526645120 model_training_utils.py:450] Train Step: 20411/21936  / loss = 0.5922482013702393\n",
            "I0420 22:30:25.859176 139904526645120 model_training_utils.py:450] Train Step: 20412/21936  / loss = 1.068687915802002\n",
            "I0420 22:30:26.397616 139904526645120 model_training_utils.py:450] Train Step: 20413/21936  / loss = 1.6134836673736572\n",
            "I0420 22:30:26.935332 139904526645120 model_training_utils.py:450] Train Step: 20414/21936  / loss = 0.421846866607666\n",
            "I0420 22:30:27.474702 139904526645120 model_training_utils.py:450] Train Step: 20415/21936  / loss = 0.2199886590242386\n",
            "I0420 22:30:28.012402 139904526645120 model_training_utils.py:450] Train Step: 20416/21936  / loss = 0.7564632892608643\n",
            "I0420 22:30:28.549090 139904526645120 model_training_utils.py:450] Train Step: 20417/21936  / loss = 0.5838261246681213\n",
            "I0420 22:30:29.083309 139904526645120 model_training_utils.py:450] Train Step: 20418/21936  / loss = 0.7406622171401978\n",
            "I0420 22:30:29.618477 139904526645120 keras_utils.py:122] TimeHistory: 26.89 seconds, 14.87 examples/second between steps 31337 and 31387\n",
            "I0420 22:30:29.621145 139904526645120 model_training_utils.py:450] Train Step: 20419/21936  / loss = 1.1164615154266357\n",
            "I0420 22:30:30.157009 139904526645120 model_training_utils.py:450] Train Step: 20420/21936  / loss = 0.17235040664672852\n",
            "I0420 22:30:30.690935 139904526645120 model_training_utils.py:450] Train Step: 20421/21936  / loss = 0.9552955627441406\n",
            "I0420 22:30:31.226736 139904526645120 model_training_utils.py:450] Train Step: 20422/21936  / loss = 0.2445770502090454\n",
            "I0420 22:30:31.762401 139904526645120 model_training_utils.py:450] Train Step: 20423/21936  / loss = 0.29487335681915283\n",
            "I0420 22:30:32.299113 139904526645120 model_training_utils.py:450] Train Step: 20424/21936  / loss = 0.43041688203811646\n",
            "I0420 22:30:32.835041 139904526645120 model_training_utils.py:450] Train Step: 20425/21936  / loss = 0.22698774933815002\n",
            "I0420 22:30:33.373137 139904526645120 model_training_utils.py:450] Train Step: 20426/21936  / loss = 1.3202555179595947\n",
            "I0420 22:30:33.909698 139904526645120 model_training_utils.py:450] Train Step: 20427/21936  / loss = 1.164738416671753\n",
            "I0420 22:30:34.447047 139904526645120 model_training_utils.py:450] Train Step: 20428/21936  / loss = 0.7043566703796387\n",
            "I0420 22:30:34.984940 139904526645120 model_training_utils.py:450] Train Step: 20429/21936  / loss = 0.8018331527709961\n",
            "I0420 22:30:35.522054 139904526645120 model_training_utils.py:450] Train Step: 20430/21936  / loss = 0.7289180159568787\n",
            "I0420 22:30:36.058215 139904526645120 model_training_utils.py:450] Train Step: 20431/21936  / loss = 0.36827534437179565\n",
            "I0420 22:30:36.595516 139904526645120 model_training_utils.py:450] Train Step: 20432/21936  / loss = 0.3598636984825134\n",
            "I0420 22:30:37.132126 139904526645120 model_training_utils.py:450] Train Step: 20433/21936  / loss = 0.49655038118362427\n",
            "I0420 22:30:37.669167 139904526645120 model_training_utils.py:450] Train Step: 20434/21936  / loss = 0.3314243257045746\n",
            "I0420 22:30:38.205404 139904526645120 model_training_utils.py:450] Train Step: 20435/21936  / loss = 0.06257125735282898\n",
            "I0420 22:30:38.741186 139904526645120 model_training_utils.py:450] Train Step: 20436/21936  / loss = 0.40426892042160034\n",
            "I0420 22:30:39.278471 139904526645120 model_training_utils.py:450] Train Step: 20437/21936  / loss = 0.8213081955909729\n",
            "I0420 22:30:39.815683 139904526645120 model_training_utils.py:450] Train Step: 20438/21936  / loss = 0.08670151233673096\n",
            "I0420 22:30:40.355037 139904526645120 model_training_utils.py:450] Train Step: 20439/21936  / loss = 0.6951569318771362\n",
            "I0420 22:30:40.892203 139904526645120 model_training_utils.py:450] Train Step: 20440/21936  / loss = 0.6762486100196838\n",
            "I0420 22:30:41.430956 139904526645120 model_training_utils.py:450] Train Step: 20441/21936  / loss = 0.9472739696502686\n",
            "I0420 22:30:41.966732 139904526645120 model_training_utils.py:450] Train Step: 20442/21936  / loss = 0.16675317287445068\n",
            "I0420 22:30:42.502832 139904526645120 model_training_utils.py:450] Train Step: 20443/21936  / loss = 0.6148947477340698\n",
            "I0420 22:30:43.038767 139904526645120 model_training_utils.py:450] Train Step: 20444/21936  / loss = 0.4134771227836609\n",
            "I0420 22:30:43.573976 139904526645120 model_training_utils.py:450] Train Step: 20445/21936  / loss = 0.4185760021209717\n",
            "I0420 22:30:44.111163 139904526645120 model_training_utils.py:450] Train Step: 20446/21936  / loss = 1.0774060487747192\n",
            "I0420 22:30:44.648010 139904526645120 model_training_utils.py:450] Train Step: 20447/21936  / loss = 0.857369065284729\n",
            "I0420 22:30:45.184428 139904526645120 model_training_utils.py:450] Train Step: 20448/21936  / loss = 0.9070910215377808\n",
            "I0420 22:30:45.718831 139904526645120 model_training_utils.py:450] Train Step: 20449/21936  / loss = 1.0288726091384888\n",
            "I0420 22:30:46.253500 139904526645120 model_training_utils.py:450] Train Step: 20450/21936  / loss = 0.4403686225414276\n",
            "I0420 22:30:46.789999 139904526645120 model_training_utils.py:450] Train Step: 20451/21936  / loss = 0.3261450529098511\n",
            "I0420 22:30:47.327383 139904526645120 model_training_utils.py:450] Train Step: 20452/21936  / loss = 0.29788312315940857\n",
            "I0420 22:30:47.864609 139904526645120 model_training_utils.py:450] Train Step: 20453/21936  / loss = 0.21516555547714233\n",
            "I0420 22:30:48.400866 139904526645120 model_training_utils.py:450] Train Step: 20454/21936  / loss = 1.0401675701141357\n",
            "I0420 22:30:48.935833 139904526645120 model_training_utils.py:450] Train Step: 20455/21936  / loss = 0.5664041638374329\n",
            "I0420 22:30:49.470366 139904526645120 model_training_utils.py:450] Train Step: 20456/21936  / loss = 0.5314605832099915\n",
            "I0420 22:30:50.005864 139904526645120 model_training_utils.py:450] Train Step: 20457/21936  / loss = 0.08842232823371887\n",
            "I0420 22:30:50.545495 139904526645120 model_training_utils.py:450] Train Step: 20458/21936  / loss = 0.39173683524131775\n",
            "I0420 22:30:51.083215 139904526645120 model_training_utils.py:450] Train Step: 20459/21936  / loss = 0.16892048716545105\n",
            "I0420 22:30:51.623676 139904526645120 model_training_utils.py:450] Train Step: 20460/21936  / loss = 0.27569496631622314\n",
            "I0420 22:30:52.160948 139904526645120 model_training_utils.py:450] Train Step: 20461/21936  / loss = 0.8650784492492676\n",
            "I0420 22:30:52.697556 139904526645120 model_training_utils.py:450] Train Step: 20462/21936  / loss = 0.2821160852909088\n",
            "I0420 22:30:53.235972 139904526645120 model_training_utils.py:450] Train Step: 20463/21936  / loss = 0.3713683485984802\n",
            "I0420 22:30:53.773586 139904526645120 model_training_utils.py:450] Train Step: 20464/21936  / loss = 0.2887270450592041\n",
            "I0420 22:30:54.308476 139904526645120 model_training_utils.py:450] Train Step: 20465/21936  / loss = 0.15711475908756256\n",
            "I0420 22:30:54.844736 139904526645120 model_training_utils.py:450] Train Step: 20466/21936  / loss = 0.5334285497665405\n",
            "I0420 22:30:55.382954 139904526645120 model_training_utils.py:450] Train Step: 20467/21936  / loss = 0.35511404275894165\n",
            "I0420 22:30:55.919234 139904526645120 model_training_utils.py:450] Train Step: 20468/21936  / loss = 0.6480384469032288\n",
            "I0420 22:30:56.456230 139904526645120 keras_utils.py:122] TimeHistory: 26.83 seconds, 14.91 examples/second between steps 31387 and 31437\n",
            "I0420 22:30:56.458968 139904526645120 model_training_utils.py:450] Train Step: 20469/21936  / loss = 0.8318588733673096\n",
            "I0420 22:30:56.992848 139904526645120 model_training_utils.py:450] Train Step: 20470/21936  / loss = 0.3210662007331848\n",
            "I0420 22:30:57.529985 139904526645120 model_training_utils.py:450] Train Step: 20471/21936  / loss = 0.6886679530143738\n",
            "I0420 22:30:58.065486 139904526645120 model_training_utils.py:450] Train Step: 20472/21936  / loss = 0.2965382933616638\n",
            "I0420 22:30:58.602106 139904526645120 model_training_utils.py:450] Train Step: 20473/21936  / loss = 0.8501374125480652\n",
            "I0420 22:30:59.140245 139904526645120 model_training_utils.py:450] Train Step: 20474/21936  / loss = 0.30269983410835266\n",
            "I0420 22:30:59.676530 139904526645120 model_training_utils.py:450] Train Step: 20475/21936  / loss = 0.06799851357936859\n",
            "I0420 22:31:00.213698 139904526645120 model_training_utils.py:450] Train Step: 20476/21936  / loss = 0.16874665021896362\n",
            "I0420 22:31:00.750914 139904526645120 model_training_utils.py:450] Train Step: 20477/21936  / loss = 0.2664409577846527\n",
            "I0420 22:31:01.290790 139904526645120 model_training_utils.py:450] Train Step: 20478/21936  / loss = 0.22554296255111694\n",
            "I0420 22:31:01.828515 139904526645120 model_training_utils.py:450] Train Step: 20479/21936  / loss = 0.11947166919708252\n",
            "I0420 22:31:02.364242 139904526645120 model_training_utils.py:450] Train Step: 20480/21936  / loss = 0.015632465481758118\n",
            "I0420 22:31:02.902248 139904526645120 model_training_utils.py:450] Train Step: 20481/21936  / loss = 0.7885861396789551\n",
            "I0420 22:31:03.440645 139904526645120 model_training_utils.py:450] Train Step: 20482/21936  / loss = 0.7132642269134521\n",
            "I0420 22:31:03.979283 139904526645120 model_training_utils.py:450] Train Step: 20483/21936  / loss = 0.6063257455825806\n",
            "I0420 22:31:04.515260 139904526645120 model_training_utils.py:450] Train Step: 20484/21936  / loss = 0.3941182494163513\n",
            "I0420 22:31:05.053289 139904526645120 model_training_utils.py:450] Train Step: 20485/21936  / loss = 0.9084750413894653\n",
            "I0420 22:31:05.592805 139904526645120 model_training_utils.py:450] Train Step: 20486/21936  / loss = 0.09398237615823746\n",
            "I0420 22:31:06.129556 139904526645120 model_training_utils.py:450] Train Step: 20487/21936  / loss = 0.9042956829071045\n",
            "I0420 22:31:06.670005 139904526645120 model_training_utils.py:450] Train Step: 20488/21936  / loss = 0.12341044843196869\n",
            "I0420 22:31:07.205223 139904526645120 model_training_utils.py:450] Train Step: 20489/21936  / loss = 0.94913250207901\n",
            "I0420 22:31:07.742388 139904526645120 model_training_utils.py:450] Train Step: 20490/21936  / loss = 0.04684339463710785\n",
            "I0420 22:31:08.277096 139904526645120 model_training_utils.py:450] Train Step: 20491/21936  / loss = 0.5140988826751709\n",
            "I0420 22:31:08.813878 139904526645120 model_training_utils.py:450] Train Step: 20492/21936  / loss = 1.0220710039138794\n",
            "I0420 22:31:09.349809 139904526645120 model_training_utils.py:450] Train Step: 20493/21936  / loss = 0.2306559830904007\n",
            "I0420 22:31:09.886698 139904526645120 model_training_utils.py:450] Train Step: 20494/21936  / loss = 0.5505232810974121\n",
            "I0420 22:31:10.422746 139904526645120 model_training_utils.py:450] Train Step: 20495/21936  / loss = 0.31533393263816833\n",
            "I0420 22:31:10.958881 139904526645120 model_training_utils.py:450] Train Step: 20496/21936  / loss = 0.37459707260131836\n",
            "I0420 22:31:11.495603 139904526645120 model_training_utils.py:450] Train Step: 20497/21936  / loss = 0.4987834095954895\n",
            "I0420 22:31:12.041391 139904526645120 model_training_utils.py:450] Train Step: 20498/21936  / loss = 0.5519689321517944\n",
            "I0420 22:31:12.579416 139904526645120 model_training_utils.py:450] Train Step: 20499/21936  / loss = 0.3429200053215027\n",
            "I0420 22:31:13.116213 139904526645120 model_training_utils.py:450] Train Step: 20500/21936  / loss = 0.5034045577049255\n",
            "I0420 22:31:13.654220 139904526645120 model_training_utils.py:450] Train Step: 20501/21936  / loss = 0.2605917155742645\n",
            "I0420 22:31:14.194005 139904526645120 model_training_utils.py:450] Train Step: 20502/21936  / loss = 0.4009474813938141\n",
            "I0420 22:31:14.731302 139904526645120 model_training_utils.py:450] Train Step: 20503/21936  / loss = 0.24104060232639313\n",
            "I0420 22:31:15.269940 139904526645120 model_training_utils.py:450] Train Step: 20504/21936  / loss = 0.10577188432216644\n",
            "I0420 22:31:15.806746 139904526645120 model_training_utils.py:450] Train Step: 20505/21936  / loss = 0.3470391035079956\n",
            "I0420 22:31:16.348583 139904526645120 model_training_utils.py:450] Train Step: 20506/21936  / loss = 0.07436720281839371\n",
            "I0420 22:31:16.887222 139904526645120 model_training_utils.py:450] Train Step: 20507/21936  / loss = 0.07275162637233734\n",
            "I0420 22:31:17.425427 139904526645120 model_training_utils.py:450] Train Step: 20508/21936  / loss = 0.17029699683189392\n",
            "I0420 22:31:17.961710 139904526645120 model_training_utils.py:450] Train Step: 20509/21936  / loss = 0.1990031599998474\n",
            "I0420 22:31:18.500374 139904526645120 model_training_utils.py:450] Train Step: 20510/21936  / loss = 0.23561573028564453\n",
            "I0420 22:31:19.039676 139904526645120 model_training_utils.py:450] Train Step: 20511/21936  / loss = 0.14805898070335388\n",
            "I0420 22:31:19.577559 139904526645120 model_training_utils.py:450] Train Step: 20512/21936  / loss = 0.2692224085330963\n",
            "I0420 22:31:20.113216 139904526645120 model_training_utils.py:450] Train Step: 20513/21936  / loss = 0.4572155177593231\n",
            "I0420 22:31:20.650492 139904526645120 model_training_utils.py:450] Train Step: 20514/21936  / loss = 0.2410530000925064\n",
            "I0420 22:31:21.189366 139904526645120 model_training_utils.py:450] Train Step: 20515/21936  / loss = 0.09405604004859924\n",
            "I0420 22:31:21.724361 139904526645120 model_training_utils.py:450] Train Step: 20516/21936  / loss = 0.4289492070674896\n",
            "I0420 22:31:22.262736 139904526645120 model_training_utils.py:450] Train Step: 20517/21936  / loss = 0.18818295001983643\n",
            "I0420 22:31:22.798659 139904526645120 model_training_utils.py:450] Train Step: 20518/21936  / loss = 0.08147278428077698\n",
            "I0420 22:31:23.337404 139904526645120 keras_utils.py:122] TimeHistory: 26.88 seconds, 14.88 examples/second between steps 31437 and 31487\n",
            "I0420 22:31:23.340178 139904526645120 model_training_utils.py:450] Train Step: 20519/21936  / loss = 0.3904007077217102\n",
            "I0420 22:31:23.878951 139904526645120 model_training_utils.py:450] Train Step: 20520/21936  / loss = 0.5874760150909424\n",
            "I0420 22:31:24.419077 139904526645120 model_training_utils.py:450] Train Step: 20521/21936  / loss = 0.31514859199523926\n",
            "I0420 22:31:24.956169 139904526645120 model_training_utils.py:450] Train Step: 20522/21936  / loss = 0.18533974885940552\n",
            "I0420 22:31:25.491175 139904526645120 model_training_utils.py:450] Train Step: 20523/21936  / loss = 0.13645388185977936\n",
            "I0420 22:31:26.029554 139904526645120 model_training_utils.py:450] Train Step: 20524/21936  / loss = 0.05735888332128525\n",
            "I0420 22:31:26.564216 139904526645120 model_training_utils.py:450] Train Step: 20525/21936  / loss = 0.25058478116989136\n",
            "I0420 22:31:27.101337 139904526645120 model_training_utils.py:450] Train Step: 20526/21936  / loss = 0.7733868360519409\n",
            "I0420 22:31:27.639939 139904526645120 model_training_utils.py:450] Train Step: 20527/21936  / loss = 0.26624685525894165\n",
            "I0420 22:31:28.177896 139904526645120 model_training_utils.py:450] Train Step: 20528/21936  / loss = 0.9821363687515259\n",
            "I0420 22:31:28.714921 139904526645120 model_training_utils.py:450] Train Step: 20529/21936  / loss = 1.7490147352218628\n",
            "I0420 22:31:29.255973 139904526645120 model_training_utils.py:450] Train Step: 20530/21936  / loss = 0.23049716651439667\n",
            "I0420 22:31:29.796281 139904526645120 model_training_utils.py:450] Train Step: 20531/21936  / loss = 0.30781665444374084\n",
            "I0420 22:31:30.334794 139904526645120 model_training_utils.py:450] Train Step: 20532/21936  / loss = 0.532785177230835\n",
            "I0420 22:31:30.877121 139904526645120 model_training_utils.py:450] Train Step: 20533/21936  / loss = 1.3220633268356323\n",
            "I0420 22:31:31.417741 139904526645120 model_training_utils.py:450] Train Step: 20534/21936  / loss = 0.8131328821182251\n",
            "I0420 22:31:31.956995 139904526645120 model_training_utils.py:450] Train Step: 20535/21936  / loss = 0.40407925844192505\n",
            "I0420 22:31:32.497122 139904526645120 model_training_utils.py:450] Train Step: 20536/21936  / loss = 0.2080569565296173\n",
            "I0420 22:31:33.034992 139904526645120 model_training_utils.py:450] Train Step: 20537/21936  / loss = 0.46609845757484436\n",
            "I0420 22:31:33.586382 139904526645120 model_training_utils.py:450] Train Step: 20538/21936  / loss = 0.4814777970314026\n",
            "I0420 22:31:34.126710 139904526645120 model_training_utils.py:450] Train Step: 20539/21936  / loss = 0.18054284155368805\n",
            "I0420 22:31:34.665032 139904526645120 model_training_utils.py:450] Train Step: 20540/21936  / loss = 0.227651447057724\n",
            "I0420 22:31:35.201188 139904526645120 model_training_utils.py:450] Train Step: 20541/21936  / loss = 0.24011759459972382\n",
            "I0420 22:31:35.738630 139904526645120 model_training_utils.py:450] Train Step: 20542/21936  / loss = 0.5150628685951233\n",
            "I0420 22:31:36.274814 139904526645120 model_training_utils.py:450] Train Step: 20543/21936  / loss = 0.10357921570539474\n",
            "I0420 22:31:36.812127 139904526645120 model_training_utils.py:450] Train Step: 20544/21936  / loss = 0.09752693772315979\n",
            "I0420 22:31:37.349347 139904526645120 model_training_utils.py:450] Train Step: 20545/21936  / loss = 0.12414059042930603\n",
            "I0420 22:31:37.887943 139904526645120 model_training_utils.py:450] Train Step: 20546/21936  / loss = 0.6458902359008789\n",
            "I0420 22:31:38.425786 139904526645120 model_training_utils.py:450] Train Step: 20547/21936  / loss = 0.24345403909683228\n",
            "I0420 22:31:38.965465 139904526645120 model_training_utils.py:450] Train Step: 20548/21936  / loss = 0.4043269753456116\n",
            "I0420 22:31:39.503365 139904526645120 model_training_utils.py:450] Train Step: 20549/21936  / loss = 1.1206262111663818\n",
            "I0420 22:31:40.043018 139904526645120 model_training_utils.py:450] Train Step: 20550/21936  / loss = 1.1111657619476318\n",
            "I0420 22:31:40.579842 139904526645120 model_training_utils.py:450] Train Step: 20551/21936  / loss = 0.1603246033191681\n",
            "I0420 22:31:41.120343 139904526645120 model_training_utils.py:450] Train Step: 20552/21936  / loss = 0.18861129879951477\n",
            "I0420 22:31:41.673024 139904526645120 model_training_utils.py:450] Train Step: 20553/21936  / loss = 0.10129787027835846\n",
            "I0420 22:31:42.218108 139904526645120 model_training_utils.py:450] Train Step: 20554/21936  / loss = 0.2880033552646637\n",
            "I0420 22:31:42.754806 139904526645120 model_training_utils.py:450] Train Step: 20555/21936  / loss = 0.5324627757072449\n",
            "I0420 22:31:43.292365 139904526645120 model_training_utils.py:450] Train Step: 20556/21936  / loss = 0.2134074866771698\n",
            "I0420 22:31:43.832216 139904526645120 model_training_utils.py:450] Train Step: 20557/21936  / loss = 1.1230121850967407\n",
            "I0420 22:31:44.369645 139904526645120 model_training_utils.py:450] Train Step: 20558/21936  / loss = 0.12188221514225006\n",
            "I0420 22:31:44.907320 139904526645120 model_training_utils.py:450] Train Step: 20559/21936  / loss = 0.16008014976978302\n",
            "I0420 22:31:45.444421 139904526645120 model_training_utils.py:450] Train Step: 20560/21936  / loss = 0.4439866542816162\n",
            "I0420 22:31:45.980850 139904526645120 model_training_utils.py:450] Train Step: 20561/21936  / loss = 0.026691589504480362\n",
            "I0420 22:31:46.522814 139904526645120 model_training_utils.py:450] Train Step: 20562/21936  / loss = 0.6611584424972534\n",
            "I0420 22:31:47.059615 139904526645120 model_training_utils.py:450] Train Step: 20563/21936  / loss = 0.1571369618177414\n",
            "I0420 22:31:47.596225 139904526645120 model_training_utils.py:450] Train Step: 20564/21936  / loss = 0.4337886571884155\n",
            "I0420 22:31:48.134272 139904526645120 model_training_utils.py:450] Train Step: 20565/21936  / loss = 0.923907995223999\n",
            "I0420 22:31:48.671945 139904526645120 model_training_utils.py:450] Train Step: 20566/21936  / loss = 0.3706519901752472\n",
            "I0420 22:31:49.210539 139904526645120 model_training_utils.py:450] Train Step: 20567/21936  / loss = 0.75957190990448\n",
            "I0420 22:31:49.748626 139904526645120 model_training_utils.py:450] Train Step: 20568/21936  / loss = 0.7370979189872742\n",
            "I0420 22:31:50.288106 139904526645120 keras_utils.py:122] TimeHistory: 26.95 seconds, 14.84 examples/second between steps 31487 and 31537\n",
            "I0420 22:31:50.290792 139904526645120 model_training_utils.py:450] Train Step: 20569/21936  / loss = 0.42576372623443604\n",
            "I0420 22:31:50.828349 139904526645120 model_training_utils.py:450] Train Step: 20570/21936  / loss = 1.0605796575546265\n",
            "I0420 22:31:51.363757 139904526645120 model_training_utils.py:450] Train Step: 20571/21936  / loss = 0.3827568292617798\n",
            "I0420 22:31:51.902403 139904526645120 model_training_utils.py:450] Train Step: 20572/21936  / loss = 0.10677366703748703\n",
            "I0420 22:31:52.438499 139904526645120 model_training_utils.py:450] Train Step: 20573/21936  / loss = 0.33500856161117554\n",
            "I0420 22:31:52.974956 139904526645120 model_training_utils.py:450] Train Step: 20574/21936  / loss = 0.2302381843328476\n",
            "I0420 22:31:53.513968 139904526645120 model_training_utils.py:450] Train Step: 20575/21936  / loss = 0.21942895650863647\n",
            "I0420 22:31:54.051903 139904526645120 model_training_utils.py:450] Train Step: 20576/21936  / loss = 0.24571208655834198\n",
            "I0420 22:31:54.589192 139904526645120 model_training_utils.py:450] Train Step: 20577/21936  / loss = 0.5664263963699341\n",
            "I0420 22:31:55.128028 139904526645120 model_training_utils.py:450] Train Step: 20578/21936  / loss = 0.2625237703323364\n",
            "I0420 22:31:55.663807 139904526645120 model_training_utils.py:450] Train Step: 20579/21936  / loss = 0.44324132800102234\n",
            "I0420 22:31:56.200839 139904526645120 model_training_utils.py:450] Train Step: 20580/21936  / loss = 0.2711755037307739\n",
            "I0420 22:31:56.738227 139904526645120 model_training_utils.py:450] Train Step: 20581/21936  / loss = 0.19562172889709473\n",
            "I0420 22:31:57.274604 139904526645120 model_training_utils.py:450] Train Step: 20582/21936  / loss = 0.1801622360944748\n",
            "I0420 22:31:57.811044 139904526645120 model_training_utils.py:450] Train Step: 20583/21936  / loss = 0.16310103237628937\n",
            "I0420 22:31:58.349037 139904526645120 model_training_utils.py:450] Train Step: 20584/21936  / loss = 0.14753234386444092\n",
            "I0420 22:31:58.887181 139904526645120 model_training_utils.py:450] Train Step: 20585/21936  / loss = 0.023404216393828392\n",
            "I0420 22:31:59.421918 139904526645120 model_training_utils.py:450] Train Step: 20586/21936  / loss = 0.425588458776474\n",
            "I0420 22:31:59.961272 139904526645120 model_training_utils.py:450] Train Step: 20587/21936  / loss = 0.22095343470573425\n",
            "I0420 22:32:00.496845 139904526645120 model_training_utils.py:450] Train Step: 20588/21936  / loss = 0.24977800250053406\n",
            "I0420 22:32:01.034029 139904526645120 model_training_utils.py:450] Train Step: 20589/21936  / loss = 0.18684975802898407\n",
            "I0420 22:32:01.574689 139904526645120 model_training_utils.py:450] Train Step: 20590/21936  / loss = 0.30732983350753784\n",
            "I0420 22:32:02.113858 139904526645120 model_training_utils.py:450] Train Step: 20591/21936  / loss = 0.5998494625091553\n",
            "I0420 22:32:02.652523 139904526645120 model_training_utils.py:450] Train Step: 20592/21936  / loss = 0.21444597840309143\n",
            "I0420 22:32:03.191094 139904526645120 model_training_utils.py:450] Train Step: 20593/21936  / loss = 0.1431717872619629\n",
            "I0420 22:32:03.729864 139904526645120 model_training_utils.py:450] Train Step: 20594/21936  / loss = 0.27722567319869995\n",
            "I0420 22:32:04.270240 139904526645120 model_training_utils.py:450] Train Step: 20595/21936  / loss = 1.1527938842773438\n",
            "I0420 22:32:04.811245 139904526645120 model_training_utils.py:450] Train Step: 20596/21936  / loss = 0.10043033957481384\n",
            "I0420 22:32:05.349101 139904526645120 model_training_utils.py:450] Train Step: 20597/21936  / loss = 0.7949163317680359\n",
            "I0420 22:32:05.890205 139904526645120 model_training_utils.py:450] Train Step: 20598/21936  / loss = 0.3988437056541443\n",
            "I0420 22:32:06.430859 139904526645120 model_training_utils.py:450] Train Step: 20599/21936  / loss = 0.7295057773590088\n",
            "I0420 22:32:06.965150 139904526645120 model_training_utils.py:450] Train Step: 20600/21936  / loss = 0.1680925488471985\n",
            "I0420 22:32:07.502750 139904526645120 model_training_utils.py:450] Train Step: 20601/21936  / loss = 0.35858282446861267\n",
            "I0420 22:32:08.040225 139904526645120 model_training_utils.py:450] Train Step: 20602/21936  / loss = 1.1616733074188232\n",
            "I0420 22:32:08.576728 139904526645120 model_training_utils.py:450] Train Step: 20603/21936  / loss = 0.1008775532245636\n",
            "I0420 22:32:09.116147 139904526645120 model_training_utils.py:450] Train Step: 20604/21936  / loss = 0.5638285875320435\n",
            "I0420 22:32:09.655714 139904526645120 model_training_utils.py:450] Train Step: 20605/21936  / loss = 0.5698621273040771\n",
            "I0420 22:32:10.193271 139904526645120 model_training_utils.py:450] Train Step: 20606/21936  / loss = 0.21051162481307983\n",
            "I0420 22:32:10.733856 139904526645120 model_training_utils.py:450] Train Step: 20607/21936  / loss = 0.4547260105609894\n",
            "I0420 22:32:11.275953 139904526645120 model_training_utils.py:450] Train Step: 20608/21936  / loss = 0.07872848212718964\n",
            "I0420 22:32:11.814072 139904526645120 model_training_utils.py:450] Train Step: 20609/21936  / loss = 0.21031661331653595\n",
            "I0420 22:32:12.350681 139904526645120 model_training_utils.py:450] Train Step: 20610/21936  / loss = 0.0429835245013237\n",
            "I0420 22:32:12.889836 139904526645120 model_training_utils.py:450] Train Step: 20611/21936  / loss = 1.0137569904327393\n",
            "I0420 22:32:13.430821 139904526645120 model_training_utils.py:450] Train Step: 20612/21936  / loss = 0.3200257122516632\n",
            "I0420 22:32:13.970912 139904526645120 model_training_utils.py:450] Train Step: 20613/21936  / loss = 0.17792174220085144\n",
            "I0420 22:32:14.524600 139904526645120 model_training_utils.py:450] Train Step: 20614/21936  / loss = 0.41842687129974365\n",
            "I0420 22:32:15.072538 139904526645120 model_training_utils.py:450] Train Step: 20615/21936  / loss = 0.23320041596889496\n",
            "I0420 22:32:15.612132 139904526645120 model_training_utils.py:450] Train Step: 20616/21936  / loss = 0.3007199764251709\n",
            "I0420 22:32:16.148584 139904526645120 model_training_utils.py:450] Train Step: 20617/21936  / loss = 0.5289697647094727\n",
            "I0420 22:32:16.689929 139904526645120 model_training_utils.py:450] Train Step: 20618/21936  / loss = 0.9352598786354065\n",
            "I0420 22:32:17.226144 139904526645120 keras_utils.py:122] TimeHistory: 26.93 seconds, 14.85 examples/second between steps 31537 and 31587\n",
            "I0420 22:32:17.228897 139904526645120 model_training_utils.py:450] Train Step: 20619/21936  / loss = 1.511462688446045\n",
            "I0420 22:32:17.766614 139904526645120 model_training_utils.py:450] Train Step: 20620/21936  / loss = 0.2540241479873657\n",
            "I0420 22:32:18.304767 139904526645120 model_training_utils.py:450] Train Step: 20621/21936  / loss = 0.7326899766921997\n",
            "I0420 22:32:18.842229 139904526645120 model_training_utils.py:450] Train Step: 20622/21936  / loss = 0.26258954405784607\n",
            "I0420 22:32:19.380842 139904526645120 model_training_utils.py:450] Train Step: 20623/21936  / loss = 0.7132946252822876\n",
            "I0420 22:32:19.923174 139904526645120 model_training_utils.py:450] Train Step: 20624/21936  / loss = 0.853256344795227\n",
            "I0420 22:32:20.461611 139904526645120 model_training_utils.py:450] Train Step: 20625/21936  / loss = 0.8105258941650391\n",
            "I0420 22:32:20.999350 139904526645120 model_training_utils.py:450] Train Step: 20626/21936  / loss = 0.43292954564094543\n",
            "I0420 22:32:21.537204 139904526645120 model_training_utils.py:450] Train Step: 20627/21936  / loss = 0.2596816122531891\n",
            "I0420 22:32:22.076337 139904526645120 model_training_utils.py:450] Train Step: 20628/21936  / loss = 0.6656789183616638\n",
            "I0420 22:32:22.613137 139904526645120 model_training_utils.py:450] Train Step: 20629/21936  / loss = 1.2831320762634277\n",
            "I0420 22:32:23.147949 139904526645120 model_training_utils.py:450] Train Step: 20630/21936  / loss = 1.2997353076934814\n",
            "I0420 22:32:23.688915 139904526645120 model_training_utils.py:450] Train Step: 20631/21936  / loss = 0.8102129697799683\n",
            "I0420 22:32:24.223634 139904526645120 model_training_utils.py:450] Train Step: 20632/21936  / loss = 0.2926614284515381\n",
            "I0420 22:32:24.758212 139904526645120 model_training_utils.py:450] Train Step: 20633/21936  / loss = 1.358891487121582\n",
            "I0420 22:32:25.295860 139904526645120 model_training_utils.py:450] Train Step: 20634/21936  / loss = 0.9700630307197571\n",
            "I0420 22:32:25.834362 139904526645120 model_training_utils.py:450] Train Step: 20635/21936  / loss = 0.7031608819961548\n",
            "I0420 22:32:26.369955 139904526645120 model_training_utils.py:450] Train Step: 20636/21936  / loss = 0.3729342818260193\n",
            "I0420 22:32:26.906636 139904526645120 model_training_utils.py:450] Train Step: 20637/21936  / loss = 0.38858044147491455\n",
            "I0420 22:32:27.445818 139904526645120 model_training_utils.py:450] Train Step: 20638/21936  / loss = 1.2074000835418701\n",
            "I0420 22:32:27.984266 139904526645120 model_training_utils.py:450] Train Step: 20639/21936  / loss = 0.6242499947547913\n",
            "I0420 22:32:28.521274 139904526645120 model_training_utils.py:450] Train Step: 20640/21936  / loss = 0.5077900886535645\n",
            "I0420 22:32:29.059182 139904526645120 model_training_utils.py:450] Train Step: 20641/21936  / loss = 0.6829872727394104\n",
            "I0420 22:32:29.596329 139904526645120 model_training_utils.py:450] Train Step: 20642/21936  / loss = 0.6042108535766602\n",
            "I0420 22:32:30.132276 139904526645120 model_training_utils.py:450] Train Step: 20643/21936  / loss = 0.37373536825180054\n",
            "I0420 22:32:30.671130 139904526645120 model_training_utils.py:450] Train Step: 20644/21936  / loss = 0.4967327415943146\n",
            "I0420 22:32:31.209998 139904526645120 model_training_utils.py:450] Train Step: 20645/21936  / loss = 0.7380056977272034\n",
            "I0420 22:32:31.746814 139904526645120 model_training_utils.py:450] Train Step: 20646/21936  / loss = 0.46404343843460083\n",
            "I0420 22:32:32.284547 139904526645120 model_training_utils.py:450] Train Step: 20647/21936  / loss = 0.5321104526519775\n",
            "I0420 22:32:32.822541 139904526645120 model_training_utils.py:450] Train Step: 20648/21936  / loss = 0.7054529190063477\n",
            "I0420 22:32:33.366886 139904526645120 model_training_utils.py:450] Train Step: 20649/21936  / loss = 1.2874603271484375\n",
            "I0420 22:32:33.904687 139904526645120 model_training_utils.py:450] Train Step: 20650/21936  / loss = 0.057345353066921234\n",
            "I0420 22:32:34.441594 139904526645120 model_training_utils.py:450] Train Step: 20651/21936  / loss = 0.7814053297042847\n",
            "I0420 22:32:34.978596 139904526645120 model_training_utils.py:450] Train Step: 20652/21936  / loss = 0.5889567136764526\n",
            "I0420 22:32:35.516722 139904526645120 model_training_utils.py:450] Train Step: 20653/21936  / loss = 0.6675938963890076\n",
            "I0420 22:32:36.057181 139904526645120 model_training_utils.py:450] Train Step: 20654/21936  / loss = 0.6505228281021118\n",
            "I0420 22:32:36.593910 139904526645120 model_training_utils.py:450] Train Step: 20655/21936  / loss = 0.27118271589279175\n",
            "I0420 22:32:37.131418 139904526645120 model_training_utils.py:450] Train Step: 20656/21936  / loss = 0.4043424725532532\n",
            "I0420 22:32:37.667978 139904526645120 model_training_utils.py:450] Train Step: 20657/21936  / loss = 0.3814485967159271\n",
            "I0420 22:32:38.203843 139904526645120 model_training_utils.py:450] Train Step: 20658/21936  / loss = 0.18149375915527344\n",
            "I0420 22:32:38.744009 139904526645120 model_training_utils.py:450] Train Step: 20659/21936  / loss = 0.18690769374370575\n",
            "I0420 22:32:39.279388 139904526645120 model_training_utils.py:450] Train Step: 20660/21936  / loss = 0.2489262968301773\n",
            "I0420 22:32:39.816452 139904526645120 model_training_utils.py:450] Train Step: 20661/21936  / loss = 0.2729325294494629\n",
            "I0420 22:32:40.357418 139904526645120 model_training_utils.py:450] Train Step: 20662/21936  / loss = 1.2376142740249634\n",
            "I0420 22:32:40.896196 139904526645120 model_training_utils.py:450] Train Step: 20663/21936  / loss = 0.11776939034461975\n",
            "I0420 22:32:41.438196 139904526645120 model_training_utils.py:450] Train Step: 20664/21936  / loss = 0.5262990593910217\n",
            "I0420 22:32:41.978169 139904526645120 model_training_utils.py:450] Train Step: 20665/21936  / loss = 0.737027645111084\n",
            "I0420 22:32:42.517101 139904526645120 model_training_utils.py:450] Train Step: 20666/21936  / loss = 2.037952423095703\n",
            "I0420 22:32:43.068957 139904526645120 model_training_utils.py:450] Train Step: 20667/21936  / loss = 1.6808918714523315\n",
            "I0420 22:32:43.606739 139904526645120 model_training_utils.py:450] Train Step: 20668/21936  / loss = 0.8377704620361328\n",
            "I0420 22:32:44.143090 139904526645120 keras_utils.py:122] TimeHistory: 26.91 seconds, 14.86 examples/second between steps 31587 and 31637\n",
            "I0420 22:32:44.145761 139904526645120 model_training_utils.py:450] Train Step: 20669/21936  / loss = 1.3570888042449951\n",
            "I0420 22:32:44.684239 139904526645120 model_training_utils.py:450] Train Step: 20670/21936  / loss = 0.6499907970428467\n",
            "I0420 22:32:45.223034 139904526645120 model_training_utils.py:450] Train Step: 20671/21936  / loss = 1.7857592105865479\n",
            "I0420 22:32:45.761137 139904526645120 model_training_utils.py:450] Train Step: 20672/21936  / loss = 1.4861629009246826\n",
            "I0420 22:32:46.299250 139904526645120 model_training_utils.py:450] Train Step: 20673/21936  / loss = 2.277409315109253\n",
            "I0420 22:32:46.840782 139904526645120 model_training_utils.py:450] Train Step: 20674/21936  / loss = 2.6288676261901855\n",
            "I0420 22:32:47.379048 139904526645120 model_training_utils.py:450] Train Step: 20675/21936  / loss = 1.2163729667663574\n",
            "I0420 22:32:47.916370 139904526645120 model_training_utils.py:450] Train Step: 20676/21936  / loss = 2.402681589126587\n",
            "I0420 22:32:48.454441 139904526645120 model_training_utils.py:450] Train Step: 20677/21936  / loss = 1.605194091796875\n",
            "I0420 22:32:48.993176 139904526645120 model_training_utils.py:450] Train Step: 20678/21936  / loss = 1.428565263748169\n",
            "I0420 22:32:49.531538 139904526645120 model_training_utils.py:450] Train Step: 20679/21936  / loss = 1.7598525285720825\n",
            "I0420 22:32:50.065812 139904526645120 model_training_utils.py:450] Train Step: 20680/21936  / loss = 0.9244139194488525\n",
            "I0420 22:32:50.601165 139904526645120 model_training_utils.py:450] Train Step: 20681/21936  / loss = 2.393012285232544\n",
            "I0420 22:32:51.139584 139904526645120 model_training_utils.py:450] Train Step: 20682/21936  / loss = 1.8799446821212769\n",
            "I0420 22:32:51.676080 139904526645120 model_training_utils.py:450] Train Step: 20683/21936  / loss = 0.20126411318778992\n",
            "I0420 22:32:52.211085 139904526645120 model_training_utils.py:450] Train Step: 20684/21936  / loss = 1.2063217163085938\n",
            "I0420 22:32:52.747166 139904526645120 model_training_utils.py:450] Train Step: 20685/21936  / loss = 0.9303036332130432\n",
            "I0420 22:32:53.283543 139904526645120 model_training_utils.py:450] Train Step: 20686/21936  / loss = 2.5633363723754883\n",
            "I0420 22:32:53.821045 139904526645120 model_training_utils.py:450] Train Step: 20687/21936  / loss = 1.7857128381729126\n",
            "I0420 22:32:54.357539 139904526645120 model_training_utils.py:450] Train Step: 20688/21936  / loss = 1.3370685577392578\n",
            "I0420 22:32:54.893773 139904526645120 model_training_utils.py:450] Train Step: 20689/21936  / loss = 1.461641788482666\n",
            "I0420 22:32:55.430833 139904526645120 model_training_utils.py:450] Train Step: 20690/21936  / loss = 1.2642892599105835\n",
            "I0420 22:32:55.969951 139904526645120 model_training_utils.py:450] Train Step: 20691/21936  / loss = 2.5042943954467773\n",
            "I0420 22:32:56.507469 139904526645120 model_training_utils.py:450] Train Step: 20692/21936  / loss = 1.348625659942627\n",
            "I0420 22:32:57.042106 139904526645120 model_training_utils.py:450] Train Step: 20693/21936  / loss = 2.8581223487854004\n",
            "I0420 22:32:57.579916 139904526645120 model_training_utils.py:450] Train Step: 20694/21936  / loss = 0.6244943141937256\n",
            "I0420 22:32:58.124685 139904526645120 model_training_utils.py:450] Train Step: 20695/21936  / loss = 2.0115578174591064\n",
            "I0420 22:32:58.662557 139904526645120 model_training_utils.py:450] Train Step: 20696/21936  / loss = 0.6748725175857544\n",
            "I0420 22:32:59.197398 139904526645120 model_training_utils.py:450] Train Step: 20697/21936  / loss = 1.2947428226470947\n",
            "I0420 22:32:59.734701 139904526645120 model_training_utils.py:450] Train Step: 20698/21936  / loss = 2.222738027572632\n",
            "I0420 22:33:00.271859 139904526645120 model_training_utils.py:450] Train Step: 20699/21936  / loss = 0.6092039346694946\n",
            "I0420 22:33:00.808134 139904526645120 model_training_utils.py:450] Train Step: 20700/21936  / loss = 1.1921851634979248\n",
            "I0420 22:33:01.344611 139904526645120 model_training_utils.py:450] Train Step: 20701/21936  / loss = 1.5237398147583008\n",
            "I0420 22:33:01.880361 139904526645120 model_training_utils.py:450] Train Step: 20702/21936  / loss = 1.1885004043579102\n",
            "I0420 22:33:02.415744 139904526645120 model_training_utils.py:450] Train Step: 20703/21936  / loss = 2.147371292114258\n",
            "I0420 22:33:02.953107 139904526645120 model_training_utils.py:450] Train Step: 20704/21936  / loss = 1.8329625129699707\n",
            "I0420 22:33:03.491508 139904526645120 model_training_utils.py:450] Train Step: 20705/21936  / loss = 1.0766355991363525\n",
            "I0420 22:33:04.029596 139904526645120 model_training_utils.py:450] Train Step: 20706/21936  / loss = 0.9669349193572998\n",
            "I0420 22:33:04.567582 139904526645120 model_training_utils.py:450] Train Step: 20707/21936  / loss = 1.584869146347046\n",
            "I0420 22:33:05.105442 139904526645120 model_training_utils.py:450] Train Step: 20708/21936  / loss = 0.5498580932617188\n",
            "I0420 22:33:05.643527 139904526645120 model_training_utils.py:450] Train Step: 20709/21936  / loss = 0.48561978340148926\n",
            "I0420 22:33:06.181852 139904526645120 model_training_utils.py:450] Train Step: 20710/21936  / loss = 1.1587563753128052\n",
            "I0420 22:33:06.719754 139904526645120 model_training_utils.py:450] Train Step: 20711/21936  / loss = 0.7914977073669434\n",
            "I0420 22:33:07.258291 139904526645120 model_training_utils.py:450] Train Step: 20712/21936  / loss = 0.9503988027572632\n",
            "I0420 22:33:07.794813 139904526645120 model_training_utils.py:450] Train Step: 20713/21936  / loss = 1.1593351364135742\n",
            "I0420 22:33:08.336126 139904526645120 model_training_utils.py:450] Train Step: 20714/21936  / loss = 0.518941342830658\n",
            "I0420 22:33:08.870211 139904526645120 model_training_utils.py:450] Train Step: 20715/21936  / loss = 0.7443637847900391\n",
            "I0420 22:33:09.407403 139904526645120 model_training_utils.py:450] Train Step: 20716/21936  / loss = 0.9279483556747437\n",
            "I0420 22:33:09.944332 139904526645120 model_training_utils.py:450] Train Step: 20717/21936  / loss = 0.9317001104354858\n",
            "I0420 22:33:10.480889 139904526645120 model_training_utils.py:450] Train Step: 20718/21936  / loss = 0.7164307236671448\n",
            "I0420 22:33:11.018724 139904526645120 keras_utils.py:122] TimeHistory: 26.87 seconds, 14.89 examples/second between steps 31637 and 31687\n",
            "I0420 22:33:11.021334 139904526645120 model_training_utils.py:450] Train Step: 20719/21936  / loss = 0.4566904902458191\n",
            "I0420 22:33:11.559622 139904526645120 model_training_utils.py:450] Train Step: 20720/21936  / loss = 0.8089897632598877\n",
            "I0420 22:33:12.097762 139904526645120 model_training_utils.py:450] Train Step: 20721/21936  / loss = 0.4261663556098938\n",
            "I0420 22:33:12.639238 139904526645120 model_training_utils.py:450] Train Step: 20722/21936  / loss = 0.6766057014465332\n",
            "I0420 22:33:13.174872 139904526645120 model_training_utils.py:450] Train Step: 20723/21936  / loss = 0.4932275414466858\n",
            "I0420 22:33:13.711927 139904526645120 model_training_utils.py:450] Train Step: 20724/21936  / loss = 0.8181041479110718\n",
            "I0420 22:33:14.251670 139904526645120 model_training_utils.py:450] Train Step: 20725/21936  / loss = 0.36595213413238525\n",
            "I0420 22:33:14.789633 139904526645120 model_training_utils.py:450] Train Step: 20726/21936  / loss = 0.9936264753341675\n",
            "I0420 22:33:15.326487 139904526645120 model_training_utils.py:450] Train Step: 20727/21936  / loss = 0.352988600730896\n",
            "I0420 22:33:15.862301 139904526645120 model_training_utils.py:450] Train Step: 20728/21936  / loss = 0.18965107202529907\n",
            "I0420 22:33:16.403723 139904526645120 model_training_utils.py:450] Train Step: 20729/21936  / loss = 0.5565539598464966\n",
            "I0420 22:33:16.940945 139904526645120 model_training_utils.py:450] Train Step: 20730/21936  / loss = 0.19804668426513672\n",
            "I0420 22:33:17.477985 139904526645120 model_training_utils.py:450] Train Step: 20731/21936  / loss = 0.8833977580070496\n",
            "I0420 22:33:18.016199 139904526645120 model_training_utils.py:450] Train Step: 20732/21936  / loss = 0.24885298311710358\n",
            "I0420 22:33:18.554406 139904526645120 model_training_utils.py:450] Train Step: 20733/21936  / loss = 0.28251078724861145\n",
            "I0420 22:33:19.092729 139904526645120 model_training_utils.py:450] Train Step: 20734/21936  / loss = 0.9032505750656128\n",
            "I0420 22:33:19.631446 139904526645120 model_training_utils.py:450] Train Step: 20735/21936  / loss = 0.22068266570568085\n",
            "I0420 22:33:20.173159 139904526645120 model_training_utils.py:450] Train Step: 20736/21936  / loss = 0.41736266016960144\n",
            "I0420 22:33:20.710175 139904526645120 model_training_utils.py:450] Train Step: 20737/21936  / loss = 0.35313713550567627\n",
            "I0420 22:33:21.248130 139904526645120 model_training_utils.py:450] Train Step: 20738/21936  / loss = 0.36818990111351013\n",
            "I0420 22:33:21.786952 139904526645120 model_training_utils.py:450] Train Step: 20739/21936  / loss = 0.5694892406463623\n",
            "I0420 22:33:22.324337 139904526645120 model_training_utils.py:450] Train Step: 20740/21936  / loss = 1.056060552597046\n",
            "I0420 22:33:22.862029 139904526645120 model_training_utils.py:450] Train Step: 20741/21936  / loss = 0.6474006175994873\n",
            "I0420 22:33:23.401511 139904526645120 model_training_utils.py:450] Train Step: 20742/21936  / loss = 0.5576927065849304\n",
            "I0420 22:33:23.939248 139904526645120 model_training_utils.py:450] Train Step: 20743/21936  / loss = 1.260624885559082\n",
            "I0420 22:33:24.475995 139904526645120 model_training_utils.py:450] Train Step: 20744/21936  / loss = 1.0102431774139404\n",
            "I0420 22:33:25.015415 139904526645120 model_training_utils.py:450] Train Step: 20745/21936  / loss = 0.7874536514282227\n",
            "I0420 22:33:25.555065 139904526645120 model_training_utils.py:450] Train Step: 20746/21936  / loss = 0.46585404872894287\n",
            "I0420 22:33:26.090066 139904526645120 model_training_utils.py:450] Train Step: 20747/21936  / loss = 0.39165163040161133\n",
            "I0420 22:33:26.624235 139904526645120 model_training_utils.py:450] Train Step: 20748/21936  / loss = 0.36231324076652527\n",
            "I0420 22:33:27.159224 139904526645120 model_training_utils.py:450] Train Step: 20749/21936  / loss = 0.6376135349273682\n",
            "I0420 22:33:27.696025 139904526645120 model_training_utils.py:450] Train Step: 20750/21936  / loss = 0.3089015483856201\n",
            "I0420 22:33:28.234668 139904526645120 model_training_utils.py:450] Train Step: 20751/21936  / loss = 0.4861970543861389\n",
            "I0420 22:33:28.771908 139904526645120 model_training_utils.py:450] Train Step: 20752/21936  / loss = 0.7336055040359497\n",
            "I0420 22:33:29.310271 139904526645120 model_training_utils.py:450] Train Step: 20753/21936  / loss = 0.32122308015823364\n",
            "I0420 22:33:29.851160 139904526645120 model_training_utils.py:450] Train Step: 20754/21936  / loss = 0.6171201467514038\n",
            "I0420 22:33:30.388555 139904526645120 model_training_utils.py:450] Train Step: 20755/21936  / loss = 0.2065410017967224\n",
            "I0420 22:33:30.922253 139904526645120 model_training_utils.py:450] Train Step: 20756/21936  / loss = 0.5696402192115784\n",
            "I0420 22:33:31.460681 139904526645120 model_training_utils.py:450] Train Step: 20757/21936  / loss = 0.33729657530784607\n",
            "I0420 22:33:31.997433 139904526645120 model_training_utils.py:450] Train Step: 20758/21936  / loss = 0.5476013422012329\n",
            "I0420 22:33:32.535669 139904526645120 model_training_utils.py:450] Train Step: 20759/21936  / loss = 0.5374761819839478\n",
            "I0420 22:33:33.073365 139904526645120 model_training_utils.py:450] Train Step: 20760/21936  / loss = 0.6613315343856812\n",
            "I0420 22:33:33.614083 139904526645120 model_training_utils.py:450] Train Step: 20761/21936  / loss = 0.3602994680404663\n",
            "I0420 22:33:34.150410 139904526645120 model_training_utils.py:450] Train Step: 20762/21936  / loss = 0.605811595916748\n",
            "I0420 22:33:34.687972 139904526645120 model_training_utils.py:450] Train Step: 20763/21936  / loss = 0.3699425756931305\n",
            "I0420 22:33:35.226606 139904526645120 model_training_utils.py:450] Train Step: 20764/21936  / loss = 0.18928766250610352\n",
            "I0420 22:33:35.765367 139904526645120 model_training_utils.py:450] Train Step: 20765/21936  / loss = 0.364111989736557\n",
            "I0420 22:33:36.305541 139904526645120 model_training_utils.py:450] Train Step: 20766/21936  / loss = 0.6632676720619202\n",
            "I0420 22:33:36.843253 139904526645120 model_training_utils.py:450] Train Step: 20767/21936  / loss = 0.16510777175426483\n",
            "I0420 22:33:37.380353 139904526645120 model_training_utils.py:450] Train Step: 20768/21936  / loss = 0.9671599268913269\n",
            "I0420 22:33:37.916579 139904526645120 keras_utils.py:122] TimeHistory: 26.89 seconds, 14.87 examples/second between steps 31687 and 31737\n",
            "I0420 22:33:37.919419 139904526645120 model_training_utils.py:450] Train Step: 20769/21936  / loss = 0.1720753312110901\n",
            "I0420 22:33:38.456185 139904526645120 model_training_utils.py:450] Train Step: 20770/21936  / loss = 0.5753175616264343\n",
            "I0420 22:33:38.994529 139904526645120 model_training_utils.py:450] Train Step: 20771/21936  / loss = 0.6694861054420471\n",
            "I0420 22:33:39.530765 139904526645120 model_training_utils.py:450] Train Step: 20772/21936  / loss = 0.3978683352470398\n",
            "I0420 22:33:40.071619 139904526645120 model_training_utils.py:450] Train Step: 20773/21936  / loss = 0.24559059739112854\n",
            "I0420 22:33:40.609220 139904526645120 model_training_utils.py:450] Train Step: 20774/21936  / loss = 0.4452182650566101\n",
            "I0420 22:33:41.150300 139904526645120 model_training_utils.py:450] Train Step: 20775/21936  / loss = 1.1580479145050049\n",
            "I0420 22:33:41.690156 139904526645120 model_training_utils.py:450] Train Step: 20776/21936  / loss = 0.15800544619560242\n",
            "I0420 22:33:42.231837 139904526645120 model_training_utils.py:450] Train Step: 20777/21936  / loss = 0.1452900469303131\n",
            "I0420 22:33:42.770937 139904526645120 model_training_utils.py:450] Train Step: 20778/21936  / loss = 0.26966801285743713\n",
            "I0420 22:33:43.306610 139904526645120 model_training_utils.py:450] Train Step: 20779/21936  / loss = 0.37641799449920654\n",
            "I0420 22:33:43.843558 139904526645120 model_training_utils.py:450] Train Step: 20780/21936  / loss = 0.4575740694999695\n",
            "I0420 22:33:44.377712 139904526645120 model_training_utils.py:450] Train Step: 20781/21936  / loss = 0.8307845592498779\n",
            "I0420 22:33:44.915600 139904526645120 model_training_utils.py:450] Train Step: 20782/21936  / loss = 0.128914937376976\n",
            "I0420 22:33:45.452179 139904526645120 model_training_utils.py:450] Train Step: 20783/21936  / loss = 0.6193461418151855\n",
            "I0420 22:33:45.990311 139904526645120 model_training_utils.py:450] Train Step: 20784/21936  / loss = 0.1661057472229004\n",
            "I0420 22:33:46.525838 139904526645120 model_training_utils.py:450] Train Step: 20785/21936  / loss = 0.37500888109207153\n",
            "I0420 22:33:47.062461 139904526645120 model_training_utils.py:450] Train Step: 20786/21936  / loss = 0.32721656560897827\n",
            "I0420 22:33:47.599543 139904526645120 model_training_utils.py:450] Train Step: 20787/21936  / loss = 0.8203282356262207\n",
            "I0420 22:33:48.137883 139904526645120 model_training_utils.py:450] Train Step: 20788/21936  / loss = 0.20771542191505432\n",
            "I0420 22:33:48.677065 139904526645120 model_training_utils.py:450] Train Step: 20789/21936  / loss = 0.22292424738407135\n",
            "I0420 22:33:49.223006 139904526645120 model_training_utils.py:450] Train Step: 20790/21936  / loss = 0.06273849308490753\n",
            "I0420 22:33:49.762711 139904526645120 model_training_utils.py:450] Train Step: 20791/21936  / loss = 0.10403807461261749\n",
            "I0420 22:33:50.298871 139904526645120 model_training_utils.py:450] Train Step: 20792/21936  / loss = 0.19764310121536255\n",
            "I0420 22:33:50.833747 139904526645120 model_training_utils.py:450] Train Step: 20793/21936  / loss = 0.12842322885990143\n",
            "I0420 22:33:51.371047 139904526645120 model_training_utils.py:450] Train Step: 20794/21936  / loss = 0.3030683994293213\n",
            "I0420 22:33:51.909119 139904526645120 model_training_utils.py:450] Train Step: 20795/21936  / loss = 0.2869502305984497\n",
            "I0420 22:33:52.442834 139904526645120 model_training_utils.py:450] Train Step: 20796/21936  / loss = 0.16804161667823792\n",
            "I0420 22:33:52.981034 139904526645120 model_training_utils.py:450] Train Step: 20797/21936  / loss = 0.06666404008865356\n",
            "I0420 22:33:53.517248 139904526645120 model_training_utils.py:450] Train Step: 20798/21936  / loss = 0.06330523639917374\n",
            "I0420 22:33:54.057661 139904526645120 model_training_utils.py:450] Train Step: 20799/21936  / loss = 0.09948114305734634\n",
            "I0420 22:33:54.594640 139904526645120 model_training_utils.py:450] Train Step: 20800/21936  / loss = 0.4156085252761841\n",
            "I0420 22:33:55.132747 139904526645120 model_training_utils.py:450] Train Step: 20801/21936  / loss = 0.46487370133399963\n",
            "I0420 22:33:55.667221 139904526645120 model_training_utils.py:450] Train Step: 20802/21936  / loss = 0.22675681114196777\n",
            "I0420 22:33:56.203963 139904526645120 model_training_utils.py:450] Train Step: 20803/21936  / loss = 0.325225293636322\n",
            "I0420 22:33:56.742219 139904526645120 model_training_utils.py:450] Train Step: 20804/21936  / loss = 0.2883380949497223\n",
            "I0420 22:33:57.279369 139904526645120 model_training_utils.py:450] Train Step: 20805/21936  / loss = 0.044162992388010025\n",
            "I0420 22:33:57.816145 139904526645120 model_training_utils.py:450] Train Step: 20806/21936  / loss = 0.19476619362831116\n",
            "I0420 22:33:58.351260 139904526645120 model_training_utils.py:450] Train Step: 20807/21936  / loss = 0.24972888827323914\n",
            "I0420 22:33:58.897332 139904526645120 model_training_utils.py:450] Train Step: 20808/21936  / loss = 0.24633674323558807\n",
            "I0420 22:33:59.435001 139904526645120 model_training_utils.py:450] Train Step: 20809/21936  / loss = 0.26656630635261536\n",
            "I0420 22:33:59.972182 139904526645120 model_training_utils.py:450] Train Step: 20810/21936  / loss = 0.8586984276771545\n",
            "I0420 22:34:00.507734 139904526645120 model_training_utils.py:450] Train Step: 20811/21936  / loss = 0.20078101754188538\n",
            "I0420 22:34:01.046716 139904526645120 model_training_utils.py:450] Train Step: 20812/21936  / loss = 0.3878728747367859\n",
            "I0420 22:34:01.583819 139904526645120 model_training_utils.py:450] Train Step: 20813/21936  / loss = 0.19202247262001038\n",
            "I0420 22:34:02.121221 139904526645120 model_training_utils.py:450] Train Step: 20814/21936  / loss = 0.37422654032707214\n",
            "I0420 22:34:02.659828 139904526645120 model_training_utils.py:450] Train Step: 20815/21936  / loss = 0.35647010803222656\n",
            "I0420 22:34:03.194332 139904526645120 model_training_utils.py:450] Train Step: 20816/21936  / loss = 0.1272197812795639\n",
            "I0420 22:34:03.731739 139904526645120 model_training_utils.py:450] Train Step: 20817/21936  / loss = 0.414619505405426\n",
            "I0420 22:34:04.269298 139904526645120 model_training_utils.py:450] Train Step: 20818/21936  / loss = 1.0886216163635254\n",
            "I0420 22:34:04.807248 139904526645120 keras_utils.py:122] TimeHistory: 26.89 seconds, 14.88 examples/second between steps 31737 and 31787\n",
            "I0420 22:34:04.810047 139904526645120 model_training_utils.py:450] Train Step: 20819/21936  / loss = 0.09294480085372925\n",
            "I0420 22:34:05.346835 139904526645120 model_training_utils.py:450] Train Step: 20820/21936  / loss = 0.1902475208044052\n",
            "I0420 22:34:05.884828 139904526645120 model_training_utils.py:450] Train Step: 20821/21936  / loss = 0.043913520872592926\n",
            "I0420 22:34:06.422162 139904526645120 model_training_utils.py:450] Train Step: 20822/21936  / loss = 0.7366557717323303\n",
            "I0420 22:34:06.957379 139904526645120 model_training_utils.py:450] Train Step: 20823/21936  / loss = 0.24346739053726196\n",
            "I0420 22:34:07.497004 139904526645120 model_training_utils.py:450] Train Step: 20824/21936  / loss = 0.544625461101532\n",
            "I0420 22:34:08.036067 139904526645120 model_training_utils.py:450] Train Step: 20825/21936  / loss = 0.12133844196796417\n",
            "I0420 22:34:08.573231 139904526645120 model_training_utils.py:450] Train Step: 20826/21936  / loss = 0.5261324644088745\n",
            "I0420 22:34:09.110552 139904526645120 model_training_utils.py:450] Train Step: 20827/21936  / loss = 0.1933937966823578\n",
            "I0420 22:34:09.647979 139904526645120 model_training_utils.py:450] Train Step: 20828/21936  / loss = 0.5017580986022949\n",
            "I0420 22:34:10.183024 139904526645120 model_training_utils.py:450] Train Step: 20829/21936  / loss = 0.05643456429243088\n",
            "I0420 22:34:10.720405 139904526645120 model_training_utils.py:450] Train Step: 20830/21936  / loss = 0.1080472469329834\n",
            "I0420 22:34:11.256388 139904526645120 model_training_utils.py:450] Train Step: 20831/21936  / loss = 0.5938681364059448\n",
            "I0420 22:34:11.792332 139904526645120 model_training_utils.py:450] Train Step: 20832/21936  / loss = 0.2590812146663666\n",
            "I0420 22:34:12.329614 139904526645120 model_training_utils.py:450] Train Step: 20833/21936  / loss = 0.20472992956638336\n",
            "I0420 22:34:12.867002 139904526645120 model_training_utils.py:450] Train Step: 20834/21936  / loss = 0.39588260650634766\n",
            "I0420 22:34:13.403470 139904526645120 model_training_utils.py:450] Train Step: 20835/21936  / loss = 0.15105374157428741\n",
            "I0420 22:34:13.939924 139904526645120 model_training_utils.py:450] Train Step: 20836/21936  / loss = 0.12312258034944534\n",
            "I0420 22:34:14.478679 139904526645120 model_training_utils.py:450] Train Step: 20837/21936  / loss = 0.13711664080619812\n",
            "I0420 22:34:15.021259 139904526645120 model_training_utils.py:450] Train Step: 20838/21936  / loss = 0.357025146484375\n",
            "I0420 22:34:15.558152 139904526645120 model_training_utils.py:450] Train Step: 20839/21936  / loss = 0.025876102969050407\n",
            "I0420 22:34:16.095887 139904526645120 model_training_utils.py:450] Train Step: 20840/21936  / loss = 0.1744704693555832\n",
            "I0420 22:34:16.634338 139904526645120 model_training_utils.py:450] Train Step: 20841/21936  / loss = 0.03902871906757355\n",
            "I0420 22:34:17.171529 139904526645120 model_training_utils.py:450] Train Step: 20842/21936  / loss = 0.033699341118335724\n",
            "I0420 22:34:17.709858 139904526645120 model_training_utils.py:450] Train Step: 20843/21936  / loss = 0.08358019590377808\n",
            "I0420 22:34:18.247126 139904526645120 model_training_utils.py:450] Train Step: 20844/21936  / loss = 0.2545345425605774\n",
            "I0420 22:34:18.782027 139904526645120 model_training_utils.py:450] Train Step: 20845/21936  / loss = 0.03345051035284996\n",
            "I0420 22:34:19.318712 139904526645120 model_training_utils.py:450] Train Step: 20846/21936  / loss = 0.015617609024047852\n",
            "I0420 22:34:19.857244 139904526645120 model_training_utils.py:450] Train Step: 20847/21936  / loss = 0.2465837597846985\n",
            "I0420 22:34:20.395730 139904526645120 model_training_utils.py:450] Train Step: 20848/21936  / loss = 0.34695374965667725\n",
            "I0420 22:34:20.935104 139904526645120 model_training_utils.py:450] Train Step: 20849/21936  / loss = 0.17865248024463654\n",
            "I0420 22:34:21.472247 139904526645120 model_training_utils.py:450] Train Step: 20850/21936  / loss = 0.1950533539056778\n",
            "I0420 22:34:22.008741 139904526645120 model_training_utils.py:450] Train Step: 20851/21936  / loss = 0.6134617924690247\n",
            "I0420 22:34:22.547053 139904526645120 model_training_utils.py:450] Train Step: 20852/21936  / loss = 0.044668011367321014\n",
            "I0420 22:34:23.082873 139904526645120 model_training_utils.py:450] Train Step: 20853/21936  / loss = 0.31257525086402893\n",
            "I0420 22:34:23.618463 139904526645120 model_training_utils.py:450] Train Step: 20854/21936  / loss = 0.13767705857753754\n",
            "I0420 22:34:24.153442 139904526645120 model_training_utils.py:450] Train Step: 20855/21936  / loss = 0.3728685677051544\n",
            "I0420 22:34:24.689172 139904526645120 model_training_utils.py:450] Train Step: 20856/21936  / loss = 0.32043328881263733\n",
            "I0420 22:34:25.226126 139904526645120 model_training_utils.py:450] Train Step: 20857/21936  / loss = 0.13663020730018616\n",
            "I0420 22:34:25.764186 139904526645120 model_training_utils.py:450] Train Step: 20858/21936  / loss = 0.23500864207744598\n",
            "I0420 22:34:26.301859 139904526645120 model_training_utils.py:450] Train Step: 20859/21936  / loss = 0.9794126749038696\n",
            "I0420 22:34:26.840023 139904526645120 model_training_utils.py:450] Train Step: 20860/21936  / loss = 0.18773099780082703\n",
            "I0420 22:34:27.376967 139904526645120 model_training_utils.py:450] Train Step: 20861/21936  / loss = 0.37029898166656494\n",
            "I0420 22:34:27.913026 139904526645120 model_training_utils.py:450] Train Step: 20862/21936  / loss = 0.1332748830318451\n",
            "I0420 22:34:28.454923 139904526645120 model_training_utils.py:450] Train Step: 20863/21936  / loss = 0.3034016788005829\n",
            "I0420 22:34:28.991214 139904526645120 model_training_utils.py:450] Train Step: 20864/21936  / loss = 0.5031574368476868\n",
            "I0420 22:34:29.531575 139904526645120 model_training_utils.py:450] Train Step: 20865/21936  / loss = 0.2149847149848938\n",
            "I0420 22:34:30.069621 139904526645120 model_training_utils.py:450] Train Step: 20866/21936  / loss = 0.13787716627120972\n",
            "I0420 22:34:30.606167 139904526645120 model_training_utils.py:450] Train Step: 20867/21936  / loss = 1.2679855823516846\n",
            "I0420 22:34:31.142954 139904526645120 model_training_utils.py:450] Train Step: 20868/21936  / loss = 0.8065637350082397\n",
            "I0420 22:34:31.681751 139904526645120 keras_utils.py:122] TimeHistory: 26.87 seconds, 14.89 examples/second between steps 31787 and 31837\n",
            "I0420 22:34:31.684431 139904526645120 model_training_utils.py:450] Train Step: 20869/21936  / loss = 0.953450620174408\n",
            "I0420 22:34:32.220147 139904526645120 model_training_utils.py:450] Train Step: 20870/21936  / loss = 0.3679808974266052\n",
            "I0420 22:34:32.756774 139904526645120 model_training_utils.py:450] Train Step: 20871/21936  / loss = 0.3196982741355896\n",
            "I0420 22:34:33.296237 139904526645120 model_training_utils.py:450] Train Step: 20872/21936  / loss = 0.6243177652359009\n",
            "I0420 22:34:33.834647 139904526645120 model_training_utils.py:450] Train Step: 20873/21936  / loss = 0.23568975925445557\n",
            "I0420 22:34:34.371371 139904526645120 model_training_utils.py:450] Train Step: 20874/21936  / loss = 0.5985754132270813\n",
            "I0420 22:34:34.917483 139904526645120 model_training_utils.py:450] Train Step: 20875/21936  / loss = 0.2702474594116211\n",
            "I0420 22:34:35.454378 139904526645120 model_training_utils.py:450] Train Step: 20876/21936  / loss = 0.10244879126548767\n",
            "I0420 22:34:35.991688 139904526645120 model_training_utils.py:450] Train Step: 20877/21936  / loss = 0.18741053342819214\n",
            "I0420 22:34:36.535223 139904526645120 model_training_utils.py:450] Train Step: 20878/21936  / loss = 0.803329586982727\n",
            "I0420 22:34:37.080543 139904526645120 model_training_utils.py:450] Train Step: 20879/21936  / loss = 0.6135214567184448\n",
            "I0420 22:34:37.616852 139904526645120 model_training_utils.py:450] Train Step: 20880/21936  / loss = 0.5166187286376953\n",
            "I0420 22:34:38.155350 139904526645120 model_training_utils.py:450] Train Step: 20881/21936  / loss = 0.11908969283103943\n",
            "I0420 22:34:38.693455 139904526645120 model_training_utils.py:450] Train Step: 20882/21936  / loss = 0.0669773668050766\n",
            "I0420 22:34:39.231791 139904526645120 model_training_utils.py:450] Train Step: 20883/21936  / loss = 0.1934441775083542\n",
            "I0420 22:34:39.770791 139904526645120 model_training_utils.py:450] Train Step: 20884/21936  / loss = 0.41939640045166016\n",
            "I0420 22:34:40.309494 139904526645120 model_training_utils.py:450] Train Step: 20885/21936  / loss = 0.6568689942359924\n",
            "I0420 22:34:40.850495 139904526645120 model_training_utils.py:450] Train Step: 20886/21936  / loss = 0.38718196749687195\n",
            "I0420 22:34:41.391790 139904526645120 model_training_utils.py:450] Train Step: 20887/21936  / loss = 0.10237754136323929\n",
            "I0420 22:34:41.931210 139904526645120 model_training_utils.py:450] Train Step: 20888/21936  / loss = 1.0011571645736694\n",
            "I0420 22:34:42.470035 139904526645120 model_training_utils.py:450] Train Step: 20889/21936  / loss = 0.17068137228488922\n",
            "I0420 22:34:43.008881 139904526645120 model_training_utils.py:450] Train Step: 20890/21936  / loss = 0.28439173102378845\n",
            "I0420 22:34:43.543495 139904526645120 model_training_utils.py:450] Train Step: 20891/21936  / loss = 0.25975868105888367\n",
            "I0420 22:34:44.078968 139904526645120 model_training_utils.py:450] Train Step: 20892/21936  / loss = 0.3434469401836395\n",
            "I0420 22:34:44.615753 139904526645120 model_training_utils.py:450] Train Step: 20893/21936  / loss = 1.1878352165222168\n",
            "I0420 22:34:45.153651 139904526645120 model_training_utils.py:450] Train Step: 20894/21936  / loss = 0.6259060502052307\n",
            "I0420 22:34:45.691862 139904526645120 model_training_utils.py:450] Train Step: 20895/21936  / loss = 0.42947709560394287\n",
            "I0420 22:34:46.230298 139904526645120 model_training_utils.py:450] Train Step: 20896/21936  / loss = 0.06320349872112274\n",
            "I0420 22:34:46.768703 139904526645120 model_training_utils.py:450] Train Step: 20897/21936  / loss = 0.5310746431350708\n",
            "I0420 22:34:47.311521 139904526645120 model_training_utils.py:450] Train Step: 20898/21936  / loss = 0.33363401889801025\n",
            "I0420 22:34:47.848136 139904526645120 model_training_utils.py:450] Train Step: 20899/21936  / loss = 0.46724608540534973\n",
            "I0420 22:34:48.384811 139904526645120 model_training_utils.py:450] Train Step: 20900/21936  / loss = 0.45374393463134766\n",
            "I0420 22:34:48.922333 139904526645120 model_training_utils.py:450] Train Step: 20901/21936  / loss = 0.6475984454154968\n",
            "I0420 22:34:49.462904 139904526645120 model_training_utils.py:450] Train Step: 20902/21936  / loss = 1.2834258079528809\n",
            "I0420 22:34:50.002188 139904526645120 model_training_utils.py:450] Train Step: 20903/21936  / loss = 0.40835314989089966\n",
            "I0420 22:34:50.541899 139904526645120 model_training_utils.py:450] Train Step: 20904/21936  / loss = 0.6082795262336731\n",
            "I0420 22:34:51.085453 139904526645120 model_training_utils.py:450] Train Step: 20905/21936  / loss = 0.13022173941135406\n",
            "I0420 22:34:51.626275 139904526645120 model_training_utils.py:450] Train Step: 20906/21936  / loss = 0.24704571068286896\n",
            "I0420 22:34:52.163313 139904526645120 model_training_utils.py:450] Train Step: 20907/21936  / loss = 0.39169543981552124\n",
            "I0420 22:34:52.706850 139904526645120 model_training_utils.py:450] Train Step: 20908/21936  / loss = 0.34332653880119324\n",
            "I0420 22:34:53.248258 139904526645120 model_training_utils.py:450] Train Step: 20909/21936  / loss = 0.6236423254013062\n",
            "I0420 22:34:53.791018 139904526645120 model_training_utils.py:450] Train Step: 20910/21936  / loss = 0.5882058143615723\n",
            "I0420 22:34:54.329958 139904526645120 model_training_utils.py:450] Train Step: 20911/21936  / loss = 0.41944393515586853\n",
            "I0420 22:34:54.869814 139904526645120 model_training_utils.py:450] Train Step: 20912/21936  / loss = 0.332101047039032\n",
            "I0420 22:34:55.409697 139904526645120 model_training_utils.py:450] Train Step: 20913/21936  / loss = 0.3588747978210449\n",
            "I0420 22:34:55.948833 139904526645120 model_training_utils.py:450] Train Step: 20914/21936  / loss = 0.8167333006858826\n",
            "I0420 22:34:56.483869 139904526645120 model_training_utils.py:450] Train Step: 20915/21936  / loss = 0.24405445158481598\n",
            "I0420 22:34:57.021611 139904526645120 model_training_utils.py:450] Train Step: 20916/21936  / loss = 0.26767367124557495\n",
            "I0420 22:34:57.565247 139904526645120 model_training_utils.py:450] Train Step: 20917/21936  / loss = 0.4626317322254181\n",
            "I0420 22:34:58.103634 139904526645120 model_training_utils.py:450] Train Step: 20918/21936  / loss = 0.25550103187561035\n",
            "I0420 22:34:58.641514 139904526645120 keras_utils.py:122] TimeHistory: 26.96 seconds, 14.84 examples/second between steps 31837 and 31887\n",
            "I0420 22:34:58.644139 139904526645120 model_training_utils.py:450] Train Step: 20919/21936  / loss = 0.974284291267395\n",
            "I0420 22:34:59.182223 139904526645120 model_training_utils.py:450] Train Step: 20920/21936  / loss = 0.62399822473526\n",
            "I0420 22:34:59.719629 139904526645120 model_training_utils.py:450] Train Step: 20921/21936  / loss = 1.1562830209732056\n",
            "I0420 22:35:00.258442 139904526645120 model_training_utils.py:450] Train Step: 20922/21936  / loss = 0.1205424964427948\n",
            "I0420 22:35:00.796961 139904526645120 model_training_utils.py:450] Train Step: 20923/21936  / loss = 1.0496199131011963\n",
            "I0420 22:35:01.337600 139904526645120 model_training_utils.py:450] Train Step: 20924/21936  / loss = 0.44963568449020386\n",
            "I0420 22:35:01.877187 139904526645120 model_training_utils.py:450] Train Step: 20925/21936  / loss = 0.14339525997638702\n",
            "I0420 22:35:02.412883 139904526645120 model_training_utils.py:450] Train Step: 20926/21936  / loss = 0.4999423325061798\n",
            "I0420 22:35:02.949053 139904526645120 model_training_utils.py:450] Train Step: 20927/21936  / loss = 0.22547578811645508\n",
            "I0420 22:35:03.493072 139904526645120 model_training_utils.py:450] Train Step: 20928/21936  / loss = 0.47947242856025696\n",
            "I0420 22:35:04.035022 139904526645120 model_training_utils.py:450] Train Step: 20929/21936  / loss = 0.6480767130851746\n",
            "I0420 22:35:04.586474 139904526645120 model_training_utils.py:450] Train Step: 20930/21936  / loss = 1.361580729484558\n",
            "I0420 22:35:05.124606 139904526645120 model_training_utils.py:450] Train Step: 20931/21936  / loss = 1.0301642417907715\n",
            "I0420 22:35:05.662400 139904526645120 model_training_utils.py:450] Train Step: 20932/21936  / loss = 0.4907883405685425\n",
            "I0420 22:35:06.202810 139904526645120 model_training_utils.py:450] Train Step: 20933/21936  / loss = 0.6711524128913879\n",
            "I0420 22:35:06.740992 139904526645120 model_training_utils.py:450] Train Step: 20934/21936  / loss = 0.6314001679420471\n",
            "I0420 22:35:07.278244 139904526645120 model_training_utils.py:450] Train Step: 20935/21936  / loss = 1.1385642290115356\n",
            "I0420 22:35:07.816041 139904526645120 model_training_utils.py:450] Train Step: 20936/21936  / loss = 0.6954343318939209\n",
            "I0420 22:35:08.353521 139904526645120 model_training_utils.py:450] Train Step: 20937/21936  / loss = 1.1661508083343506\n",
            "I0420 22:35:08.891497 139904526645120 model_training_utils.py:450] Train Step: 20938/21936  / loss = 0.9277320504188538\n",
            "I0420 22:35:09.427853 139904526645120 model_training_utils.py:450] Train Step: 20939/21936  / loss = 0.5063477158546448\n",
            "I0420 22:35:09.968099 139904526645120 model_training_utils.py:450] Train Step: 20940/21936  / loss = 0.3310915231704712\n",
            "I0420 22:35:10.506094 139904526645120 model_training_utils.py:450] Train Step: 20941/21936  / loss = 0.7798281908035278\n",
            "I0420 22:35:11.044194 139904526645120 model_training_utils.py:450] Train Step: 20942/21936  / loss = 0.22197921574115753\n",
            "I0420 22:35:11.582026 139904526645120 model_training_utils.py:450] Train Step: 20943/21936  / loss = 0.16539740562438965\n",
            "I0420 22:35:12.118752 139904526645120 model_training_utils.py:450] Train Step: 20944/21936  / loss = 0.12802721560001373\n",
            "I0420 22:35:12.656385 139904526645120 model_training_utils.py:450] Train Step: 20945/21936  / loss = 0.2753448486328125\n",
            "I0420 22:35:13.196338 139904526645120 model_training_utils.py:450] Train Step: 20946/21936  / loss = 0.5787373781204224\n",
            "I0420 22:35:13.733941 139904526645120 model_training_utils.py:450] Train Step: 20947/21936  / loss = 0.21731609106063843\n",
            "I0420 22:35:14.272753 139904526645120 model_training_utils.py:450] Train Step: 20948/21936  / loss = 0.2537055015563965\n",
            "I0420 22:35:14.817177 139904526645120 model_training_utils.py:450] Train Step: 20949/21936  / loss = 1.4805095195770264\n",
            "I0420 22:35:15.357236 139904526645120 model_training_utils.py:450] Train Step: 20950/21936  / loss = 0.08890675008296967\n",
            "I0420 22:35:15.895851 139904526645120 model_training_utils.py:450] Train Step: 20951/21936  / loss = 0.14083904027938843\n",
            "I0420 22:35:16.435142 139904526645120 model_training_utils.py:450] Train Step: 20952/21936  / loss = 0.3682458996772766\n",
            "I0420 22:35:16.975587 139904526645120 model_training_utils.py:450] Train Step: 20953/21936  / loss = 0.35741353034973145\n",
            "I0420 22:35:17.512951 139904526645120 model_training_utils.py:450] Train Step: 20954/21936  / loss = 0.3131583333015442\n",
            "I0420 22:35:18.055786 139904526645120 model_training_utils.py:450] Train Step: 20955/21936  / loss = 0.23751795291900635\n",
            "I0420 22:35:18.593763 139904526645120 model_training_utils.py:450] Train Step: 20956/21936  / loss = 0.2911142110824585\n",
            "I0420 22:35:19.134508 139904526645120 model_training_utils.py:450] Train Step: 20957/21936  / loss = 0.45564478635787964\n",
            "I0420 22:35:19.672641 139904526645120 model_training_utils.py:450] Train Step: 20958/21936  / loss = 0.5416610836982727\n",
            "I0420 22:35:20.212450 139904526645120 model_training_utils.py:450] Train Step: 20959/21936  / loss = 0.48040956258773804\n",
            "I0420 22:35:20.754213 139904526645120 model_training_utils.py:450] Train Step: 20960/21936  / loss = 0.5214909315109253\n",
            "I0420 22:35:21.293252 139904526645120 model_training_utils.py:450] Train Step: 20961/21936  / loss = 0.30024757981300354\n",
            "I0420 22:35:21.831617 139904526645120 model_training_utils.py:450] Train Step: 20962/21936  / loss = 0.5070620179176331\n",
            "I0420 22:35:22.367197 139904526645120 model_training_utils.py:450] Train Step: 20963/21936  / loss = 0.652741551399231\n",
            "I0420 22:35:22.905468 139904526645120 model_training_utils.py:450] Train Step: 20964/21936  / loss = 0.31260907649993896\n",
            "I0420 22:35:23.442583 139904526645120 model_training_utils.py:450] Train Step: 20965/21936  / loss = 0.17967890202999115\n",
            "I0420 22:35:23.983225 139904526645120 model_training_utils.py:450] Train Step: 20966/21936  / loss = 0.47897300124168396\n",
            "I0420 22:35:24.522729 139904526645120 model_training_utils.py:450] Train Step: 20967/21936  / loss = 0.22972367703914642\n",
            "I0420 22:35:25.061057 139904526645120 model_training_utils.py:450] Train Step: 20968/21936  / loss = 0.1779162585735321\n",
            "I0420 22:35:25.599817 139904526645120 keras_utils.py:122] TimeHistory: 26.95 seconds, 14.84 examples/second between steps 31887 and 31937\n",
            "I0420 22:35:25.602474 139904526645120 model_training_utils.py:450] Train Step: 20969/21936  / loss = 0.42177635431289673\n",
            "I0420 22:35:26.141364 139904526645120 model_training_utils.py:450] Train Step: 20970/21936  / loss = 0.572765588760376\n",
            "I0420 22:35:26.681431 139904526645120 model_training_utils.py:450] Train Step: 20971/21936  / loss = 0.1890041083097458\n",
            "I0420 22:35:27.219607 139904526645120 model_training_utils.py:450] Train Step: 20972/21936  / loss = 0.2537958323955536\n",
            "I0420 22:35:27.757073 139904526645120 model_training_utils.py:450] Train Step: 20973/21936  / loss = 0.19315941631793976\n",
            "I0420 22:35:28.296048 139904526645120 model_training_utils.py:450] Train Step: 20974/21936  / loss = 0.13696569204330444\n",
            "I0420 22:35:28.834881 139904526645120 model_training_utils.py:450] Train Step: 20975/21936  / loss = 0.13232460618019104\n",
            "I0420 22:35:29.372627 139904526645120 model_training_utils.py:450] Train Step: 20976/21936  / loss = 0.6848393678665161\n",
            "I0420 22:35:29.911312 139904526645120 model_training_utils.py:450] Train Step: 20977/21936  / loss = 0.25598788261413574\n",
            "I0420 22:35:30.450967 139904526645120 model_training_utils.py:450] Train Step: 20978/21936  / loss = 0.8387610912322998\n",
            "I0420 22:35:30.993908 139904526645120 model_training_utils.py:450] Train Step: 20979/21936  / loss = 1.0172804594039917\n",
            "I0420 22:35:31.531664 139904526645120 model_training_utils.py:450] Train Step: 20980/21936  / loss = 0.8456759452819824\n",
            "I0420 22:35:32.071480 139904526645120 model_training_utils.py:450] Train Step: 20981/21936  / loss = 0.37490835785865784\n",
            "I0420 22:35:32.610977 139904526645120 model_training_utils.py:450] Train Step: 20982/21936  / loss = 1.2929041385650635\n",
            "I0420 22:35:33.148848 139904526645120 model_training_utils.py:450] Train Step: 20983/21936  / loss = 0.8153994083404541\n",
            "I0420 22:35:33.690340 139904526645120 model_training_utils.py:450] Train Step: 20984/21936  / loss = 0.3823026120662689\n",
            "I0420 22:35:34.228204 139904526645120 model_training_utils.py:450] Train Step: 20985/21936  / loss = 0.2942062318325043\n",
            "I0420 22:35:34.764358 139904526645120 model_training_utils.py:450] Train Step: 20986/21936  / loss = 1.097905158996582\n",
            "I0420 22:35:35.304429 139904526645120 model_training_utils.py:450] Train Step: 20987/21936  / loss = 0.30522364377975464\n",
            "I0420 22:35:35.843181 139904526645120 model_training_utils.py:450] Train Step: 20988/21936  / loss = 0.10620982944965363\n",
            "I0420 22:35:36.380904 139904526645120 model_training_utils.py:450] Train Step: 20989/21936  / loss = 0.39783018827438354\n",
            "I0420 22:35:36.919070 139904526645120 model_training_utils.py:450] Train Step: 20990/21936  / loss = 0.9843331575393677\n",
            "I0420 22:35:37.457067 139904526645120 model_training_utils.py:450] Train Step: 20991/21936  / loss = 0.1919902116060257\n",
            "I0420 22:35:37.994637 139904526645120 model_training_utils.py:450] Train Step: 20992/21936  / loss = 0.1762116551399231\n",
            "I0420 22:35:38.532486 139904526645120 model_training_utils.py:450] Train Step: 20993/21936  / loss = 0.76715487241745\n",
            "I0420 22:35:39.074054 139904526645120 model_training_utils.py:450] Train Step: 20994/21936  / loss = 0.315479040145874\n",
            "I0420 22:35:39.613637 139904526645120 model_training_utils.py:450] Train Step: 20995/21936  / loss = 0.16892890632152557\n",
            "I0420 22:35:40.151369 139904526645120 model_training_utils.py:450] Train Step: 20996/21936  / loss = 0.2339288592338562\n",
            "I0420 22:35:40.689128 139904526645120 model_training_utils.py:450] Train Step: 20997/21936  / loss = 0.5748215317726135\n",
            "I0420 22:35:41.227411 139904526645120 model_training_utils.py:450] Train Step: 20998/21936  / loss = 1.4606329202651978\n",
            "I0420 22:35:41.767042 139904526645120 model_training_utils.py:450] Train Step: 20999/21936  / loss = 0.7822803258895874\n",
            "I0420 22:35:42.306308 139904526645120 model_training_utils.py:450] Train Step: 21000/21936  / loss = 0.2639646530151367\n",
            "I0420 22:35:42.846402 139904526645120 model_training_utils.py:450] Train Step: 21001/21936  / loss = 1.0470917224884033\n",
            "I0420 22:35:43.384084 139904526645120 model_training_utils.py:450] Train Step: 21002/21936  / loss = 0.12301532924175262\n",
            "I0420 22:35:43.922285 139904526645120 model_training_utils.py:450] Train Step: 21003/21936  / loss = 1.5332638025283813\n",
            "I0420 22:35:44.462446 139904526645120 model_training_utils.py:450] Train Step: 21004/21936  / loss = 0.5647914409637451\n",
            "I0420 22:35:45.005864 139904526645120 model_training_utils.py:450] Train Step: 21005/21936  / loss = 0.43059778213500977\n",
            "I0420 22:35:45.551936 139904526645120 model_training_utils.py:450] Train Step: 21006/21936  / loss = 0.5689371228218079\n",
            "I0420 22:35:46.088047 139904526645120 model_training_utils.py:450] Train Step: 21007/21936  / loss = 1.263249397277832\n",
            "I0420 22:35:46.624046 139904526645120 model_training_utils.py:450] Train Step: 21008/21936  / loss = 0.4477154612541199\n",
            "I0420 22:35:47.161833 139904526645120 model_training_utils.py:450] Train Step: 21009/21936  / loss = 0.4726403057575226\n",
            "I0420 22:35:47.699222 139904526645120 model_training_utils.py:450] Train Step: 21010/21936  / loss = 0.21518272161483765\n",
            "I0420 22:35:48.235515 139904526645120 model_training_utils.py:450] Train Step: 21011/21936  / loss = 0.9478408098220825\n",
            "I0420 22:35:48.774144 139904526645120 model_training_utils.py:450] Train Step: 21012/21936  / loss = 0.8157984018325806\n",
            "I0420 22:35:49.313957 139904526645120 model_training_utils.py:450] Train Step: 21013/21936  / loss = 0.2016468346118927\n",
            "I0420 22:35:49.858705 139904526645120 model_training_utils.py:450] Train Step: 21014/21936  / loss = 1.1885367631912231\n",
            "I0420 22:35:50.394654 139904526645120 model_training_utils.py:450] Train Step: 21015/21936  / loss = 0.547288179397583\n",
            "I0420 22:35:50.929899 139904526645120 model_training_utils.py:450] Train Step: 21016/21936  / loss = 0.33849257230758667\n",
            "I0420 22:35:51.466404 139904526645120 model_training_utils.py:450] Train Step: 21017/21936  / loss = 0.44743871688842773\n",
            "I0420 22:35:52.001285 139904526645120 model_training_utils.py:450] Train Step: 21018/21936  / loss = 0.07956627756357193\n",
            "I0420 22:35:52.537466 139904526645120 keras_utils.py:122] TimeHistory: 26.93 seconds, 14.85 examples/second between steps 31937 and 31987\n",
            "I0420 22:35:52.540068 139904526645120 model_training_utils.py:450] Train Step: 21019/21936  / loss = 0.8892420530319214\n",
            "I0420 22:35:53.078215 139904526645120 model_training_utils.py:450] Train Step: 21020/21936  / loss = 0.5275588631629944\n",
            "I0420 22:35:53.616960 139904526645120 model_training_utils.py:450] Train Step: 21021/21936  / loss = 1.3149620294570923\n",
            "I0420 22:35:54.156121 139904526645120 model_training_utils.py:450] Train Step: 21022/21936  / loss = 0.3512292206287384\n",
            "I0420 22:35:54.694464 139904526645120 model_training_utils.py:450] Train Step: 21023/21936  / loss = 0.4934542179107666\n",
            "I0420 22:35:55.231924 139904526645120 model_training_utils.py:450] Train Step: 21024/21936  / loss = 0.34499406814575195\n",
            "I0420 22:35:55.771465 139904526645120 model_training_utils.py:450] Train Step: 21025/21936  / loss = 0.5142924785614014\n",
            "I0420 22:35:56.309293 139904526645120 model_training_utils.py:450] Train Step: 21026/21936  / loss = 0.10902735590934753\n",
            "I0420 22:35:56.852865 139904526645120 model_training_utils.py:450] Train Step: 21027/21936  / loss = 0.3344271779060364\n",
            "I0420 22:35:57.391333 139904526645120 model_training_utils.py:450] Train Step: 21028/21936  / loss = 0.5213701725006104\n",
            "I0420 22:35:57.931644 139904526645120 model_training_utils.py:450] Train Step: 21029/21936  / loss = 0.2114149034023285\n",
            "I0420 22:35:58.469221 139904526645120 model_training_utils.py:450] Train Step: 21030/21936  / loss = 0.16000527143478394\n",
            "I0420 22:35:59.005100 139904526645120 model_training_utils.py:450] Train Step: 21031/21936  / loss = 0.2564466893672943\n",
            "I0420 22:35:59.543124 139904526645120 model_training_utils.py:450] Train Step: 21032/21936  / loss = 0.43671607971191406\n",
            "I0420 22:36:00.080708 139904526645120 model_training_utils.py:450] Train Step: 21033/21936  / loss = 0.17263400554656982\n",
            "I0420 22:36:00.617383 139904526645120 model_training_utils.py:450] Train Step: 21034/21936  / loss = 0.09512374550104141\n",
            "I0420 22:36:01.156703 139904526645120 model_training_utils.py:450] Train Step: 21035/21936  / loss = 0.06826746463775635\n",
            "I0420 22:36:01.698253 139904526645120 model_training_utils.py:450] Train Step: 21036/21936  / loss = 0.3733714520931244\n",
            "I0420 22:36:02.234105 139904526645120 model_training_utils.py:450] Train Step: 21037/21936  / loss = 0.5673490166664124\n",
            "I0420 22:36:02.776431 139904526645120 model_training_utils.py:450] Train Step: 21038/21936  / loss = 0.22089719772338867\n",
            "I0420 22:36:03.312544 139904526645120 model_training_utils.py:450] Train Step: 21039/21936  / loss = 0.31087520718574524\n",
            "I0420 22:36:03.850790 139904526645120 model_training_utils.py:450] Train Step: 21040/21936  / loss = 0.22741243243217468\n",
            "I0420 22:36:04.388920 139904526645120 model_training_utils.py:450] Train Step: 21041/21936  / loss = 0.06438730657100677\n",
            "I0420 22:36:04.927242 139904526645120 model_training_utils.py:450] Train Step: 21042/21936  / loss = 0.43368321657180786\n",
            "I0420 22:36:05.463472 139904526645120 model_training_utils.py:450] Train Step: 21043/21936  / loss = 0.232245534658432\n",
            "I0420 22:36:05.999476 139904526645120 model_training_utils.py:450] Train Step: 21044/21936  / loss = 0.11282094568014145\n",
            "I0420 22:36:06.538359 139904526645120 model_training_utils.py:450] Train Step: 21045/21936  / loss = 0.5766098499298096\n",
            "I0420 22:36:07.075303 139904526645120 model_training_utils.py:450] Train Step: 21046/21936  / loss = 0.5184508562088013\n",
            "I0420 22:36:07.612249 139904526645120 model_training_utils.py:450] Train Step: 21047/21936  / loss = 0.25254926085472107\n",
            "I0420 22:36:08.149780 139904526645120 model_training_utils.py:450] Train Step: 21048/21936  / loss = 0.30537983775138855\n",
            "I0420 22:36:08.686026 139904526645120 model_training_utils.py:450] Train Step: 21049/21936  / loss = 0.19980177283287048\n",
            "I0420 22:36:09.223746 139904526645120 model_training_utils.py:450] Train Step: 21050/21936  / loss = 0.044200919568538666\n",
            "I0420 22:36:09.763115 139904526645120 model_training_utils.py:450] Train Step: 21051/21936  / loss = 0.1145332008600235\n",
            "I0420 22:36:10.309315 139904526645120 model_training_utils.py:450] Train Step: 21052/21936  / loss = 0.20557600259780884\n",
            "I0420 22:36:10.849808 139904526645120 model_training_utils.py:450] Train Step: 21053/21936  / loss = 0.4917517900466919\n",
            "I0420 22:36:11.392275 139904526645120 model_training_utils.py:450] Train Step: 21054/21936  / loss = 0.2285681962966919\n",
            "I0420 22:36:11.929593 139904526645120 model_training_utils.py:450] Train Step: 21055/21936  / loss = 0.8698484897613525\n",
            "I0420 22:36:12.468594 139904526645120 model_training_utils.py:450] Train Step: 21056/21936  / loss = 0.15610262751579285\n",
            "I0420 22:36:13.007132 139904526645120 model_training_utils.py:450] Train Step: 21057/21936  / loss = 0.21512162685394287\n",
            "I0420 22:36:13.543203 139904526645120 model_training_utils.py:450] Train Step: 21058/21936  / loss = 0.4107717275619507\n",
            "I0420 22:36:14.081521 139904526645120 model_training_utils.py:450] Train Step: 21059/21936  / loss = 0.25873878598213196\n",
            "I0420 22:36:14.623157 139904526645120 model_training_utils.py:450] Train Step: 21060/21936  / loss = 0.050593480467796326\n",
            "I0420 22:36:15.160249 139904526645120 model_training_utils.py:450] Train Step: 21061/21936  / loss = 0.25836434960365295\n",
            "I0420 22:36:15.695856 139904526645120 model_training_utils.py:450] Train Step: 21062/21936  / loss = 0.19578266143798828\n",
            "I0420 22:36:16.233632 139904526645120 model_training_utils.py:450] Train Step: 21063/21936  / loss = 0.1905345618724823\n",
            "I0420 22:36:16.770630 139904526645120 model_training_utils.py:450] Train Step: 21064/21936  / loss = 0.2544441521167755\n",
            "I0420 22:36:17.307835 139904526645120 model_training_utils.py:450] Train Step: 21065/21936  / loss = 1.6913610696792603\n",
            "I0420 22:36:17.847127 139904526645120 model_training_utils.py:450] Train Step: 21066/21936  / loss = 0.35302257537841797\n",
            "I0420 22:36:18.395919 139904526645120 model_training_utils.py:450] Train Step: 21067/21936  / loss = 0.38444048166275024\n",
            "I0420 22:36:18.932398 139904526645120 model_training_utils.py:450] Train Step: 21068/21936  / loss = 1.1892914772033691\n",
            "I0420 22:36:19.478149 139904526645120 keras_utils.py:122] TimeHistory: 26.94 seconds, 14.85 examples/second between steps 31987 and 32037\n",
            "I0420 22:36:19.480716 139904526645120 model_training_utils.py:450] Train Step: 21069/21936  / loss = 0.457618772983551\n",
            "I0420 22:36:20.018303 139904526645120 model_training_utils.py:450] Train Step: 21070/21936  / loss = 0.7881956100463867\n",
            "I0420 22:36:20.556468 139904526645120 model_training_utils.py:450] Train Step: 21071/21936  / loss = 0.391512006521225\n",
            "I0420 22:36:21.094218 139904526645120 model_training_utils.py:450] Train Step: 21072/21936  / loss = 0.6505684852600098\n",
            "I0420 22:36:21.635251 139904526645120 model_training_utils.py:450] Train Step: 21073/21936  / loss = 0.6849830150604248\n",
            "I0420 22:36:22.171131 139904526645120 model_training_utils.py:450] Train Step: 21074/21936  / loss = 0.38127440214157104\n",
            "I0420 22:36:22.706964 139904526645120 model_training_utils.py:450] Train Step: 21075/21936  / loss = 0.12015500664710999\n",
            "I0420 22:36:23.244302 139904526645120 model_training_utils.py:450] Train Step: 21076/21936  / loss = 0.4617915451526642\n",
            "I0420 22:36:23.785185 139904526645120 model_training_utils.py:450] Train Step: 21077/21936  / loss = 0.5060490965843201\n",
            "I0420 22:36:24.322051 139904526645120 model_training_utils.py:450] Train Step: 21078/21936  / loss = 0.40758633613586426\n",
            "I0420 22:36:24.858517 139904526645120 model_training_utils.py:450] Train Step: 21079/21936  / loss = 0.3854207992553711\n",
            "I0420 22:36:25.395291 139904526645120 model_training_utils.py:450] Train Step: 21080/21936  / loss = 0.13791266083717346\n",
            "I0420 22:36:25.932653 139904526645120 model_training_utils.py:450] Train Step: 21081/21936  / loss = 0.6292953491210938\n",
            "I0420 22:36:26.472079 139904526645120 model_training_utils.py:450] Train Step: 21082/21936  / loss = 0.6519317626953125\n",
            "I0420 22:36:27.008857 139904526645120 model_training_utils.py:450] Train Step: 21083/21936  / loss = 1.0724066495895386\n",
            "I0420 22:36:27.547127 139904526645120 model_training_utils.py:450] Train Step: 21084/21936  / loss = 0.8655139803886414\n",
            "I0420 22:36:28.084861 139904526645120 model_training_utils.py:450] Train Step: 21085/21936  / loss = 0.4071241021156311\n",
            "I0420 22:36:28.623176 139904526645120 model_training_utils.py:450] Train Step: 21086/21936  / loss = 1.508934497833252\n",
            "I0420 22:36:29.161103 139904526645120 model_training_utils.py:450] Train Step: 21087/21936  / loss = 0.12369225919246674\n",
            "I0420 22:36:29.700664 139904526645120 model_training_utils.py:450] Train Step: 21088/21936  / loss = 1.6165645122528076\n",
            "I0420 22:36:30.248252 139904526645120 model_training_utils.py:450] Train Step: 21089/21936  / loss = 0.5977014899253845\n",
            "I0420 22:36:30.785545 139904526645120 model_training_utils.py:450] Train Step: 21090/21936  / loss = 0.1835668832063675\n",
            "I0420 22:36:31.322613 139904526645120 model_training_utils.py:450] Train Step: 21091/21936  / loss = 0.11494135111570358\n",
            "I0420 22:36:31.867769 139904526645120 model_training_utils.py:450] Train Step: 21092/21936  / loss = 1.2852494716644287\n",
            "I0420 22:36:32.405279 139904526645120 model_training_utils.py:450] Train Step: 21093/21936  / loss = 0.48389920592308044\n",
            "I0420 22:36:32.949462 139904526645120 model_training_utils.py:450] Train Step: 21094/21936  / loss = 0.30610498785972595\n",
            "I0420 22:36:33.487336 139904526645120 model_training_utils.py:450] Train Step: 21095/21936  / loss = 0.19891931116580963\n",
            "I0420 22:36:34.030846 139904526645120 model_training_utils.py:450] Train Step: 21096/21936  / loss = 0.3950098156929016\n",
            "I0420 22:36:34.566904 139904526645120 model_training_utils.py:450] Train Step: 21097/21936  / loss = 0.551580011844635\n",
            "I0420 22:36:35.104718 139904526645120 model_training_utils.py:450] Train Step: 21098/21936  / loss = 1.0272636413574219\n",
            "I0420 22:36:35.641356 139904526645120 model_training_utils.py:450] Train Step: 21099/21936  / loss = 0.2682740390300751\n",
            "I0420 22:36:36.179391 139904526645120 model_training_utils.py:450] Train Step: 21100/21936  / loss = 0.6543991565704346\n",
            "I0420 22:36:36.715234 139904526645120 model_training_utils.py:450] Train Step: 21101/21936  / loss = 0.4091109335422516\n",
            "I0420 22:36:37.251942 139904526645120 model_training_utils.py:450] Train Step: 21102/21936  / loss = 0.23238332569599152\n",
            "I0420 22:36:37.788832 139904526645120 model_training_utils.py:450] Train Step: 21103/21936  / loss = 1.0338329076766968\n",
            "I0420 22:36:38.322968 139904526645120 model_training_utils.py:450] Train Step: 21104/21936  / loss = 0.16861063241958618\n",
            "I0420 22:36:38.860951 139904526645120 model_training_utils.py:450] Train Step: 21105/21936  / loss = 0.24126672744750977\n",
            "I0420 22:36:39.399507 139904526645120 model_training_utils.py:450] Train Step: 21106/21936  / loss = 0.4038912057876587\n",
            "I0420 22:36:39.936846 139904526645120 model_training_utils.py:450] Train Step: 21107/21936  / loss = 0.4083671569824219\n",
            "I0420 22:36:40.473752 139904526645120 model_training_utils.py:450] Train Step: 21108/21936  / loss = 0.391746461391449\n",
            "I0420 22:36:41.011483 139904526645120 model_training_utils.py:450] Train Step: 21109/21936  / loss = 0.49090576171875\n",
            "I0420 22:36:41.551211 139904526645120 model_training_utils.py:450] Train Step: 21110/21936  / loss = 0.3416972756385803\n",
            "I0420 22:36:42.091691 139904526645120 model_training_utils.py:450] Train Step: 21111/21936  / loss = 0.9681251645088196\n",
            "I0420 22:36:42.628367 139904526645120 model_training_utils.py:450] Train Step: 21112/21936  / loss = 0.13354980945587158\n",
            "I0420 22:36:43.164904 139904526645120 model_training_utils.py:450] Train Step: 21113/21936  / loss = 0.38230836391448975\n",
            "I0420 22:36:43.700611 139904526645120 model_training_utils.py:450] Train Step: 21114/21936  / loss = 0.49126172065734863\n",
            "I0420 22:36:44.238677 139904526645120 model_training_utils.py:450] Train Step: 21115/21936  / loss = 0.48440197110176086\n",
            "I0420 22:36:44.778103 139904526645120 model_training_utils.py:450] Train Step: 21116/21936  / loss = 0.6119447946548462\n",
            "I0420 22:36:45.315501 139904526645120 model_training_utils.py:450] Train Step: 21117/21936  / loss = 0.4138094484806061\n",
            "I0420 22:36:45.851587 139904526645120 model_training_utils.py:450] Train Step: 21118/21936  / loss = 0.24947208166122437\n",
            "I0420 22:36:46.389094 139904526645120 keras_utils.py:122] TimeHistory: 26.91 seconds, 14.87 examples/second between steps 32037 and 32087\n",
            "I0420 22:36:46.391750 139904526645120 model_training_utils.py:450] Train Step: 21119/21936  / loss = 0.5894674062728882\n",
            "I0420 22:36:46.927614 139904526645120 model_training_utils.py:450] Train Step: 21120/21936  / loss = 1.1013835668563843\n",
            "I0420 22:36:47.465475 139904526645120 model_training_utils.py:450] Train Step: 21121/21936  / loss = 1.294435739517212\n",
            "I0420 22:36:48.002429 139904526645120 model_training_utils.py:450] Train Step: 21122/21936  / loss = 1.7116038799285889\n",
            "I0420 22:36:48.541169 139904526645120 model_training_utils.py:450] Train Step: 21123/21936  / loss = 0.08340305835008621\n",
            "I0420 22:36:49.077845 139904526645120 model_training_utils.py:450] Train Step: 21124/21936  / loss = 0.5240963101387024\n",
            "I0420 22:36:49.612241 139904526645120 model_training_utils.py:450] Train Step: 21125/21936  / loss = 0.46209120750427246\n",
            "I0420 22:36:50.148388 139904526645120 model_training_utils.py:450] Train Step: 21126/21936  / loss = 0.9361234903335571\n",
            "I0420 22:36:50.685074 139904526645120 model_training_utils.py:450] Train Step: 21127/21936  / loss = 0.26893991231918335\n",
            "I0420 22:36:51.222885 139904526645120 model_training_utils.py:450] Train Step: 21128/21936  / loss = 1.1815392971038818\n",
            "I0420 22:36:51.768528 139904526645120 model_training_utils.py:450] Train Step: 21129/21936  / loss = 0.2644224762916565\n",
            "I0420 22:36:52.306076 139904526645120 model_training_utils.py:450] Train Step: 21130/21936  / loss = 0.2037927359342575\n",
            "I0420 22:36:52.843760 139904526645120 model_training_utils.py:450] Train Step: 21131/21936  / loss = 0.11052629351615906\n",
            "I0420 22:36:53.381507 139904526645120 model_training_utils.py:450] Train Step: 21132/21936  / loss = 0.737747848033905\n",
            "I0420 22:36:53.919222 139904526645120 model_training_utils.py:450] Train Step: 21133/21936  / loss = 0.4478145241737366\n",
            "I0420 22:36:54.455161 139904526645120 model_training_utils.py:450] Train Step: 21134/21936  / loss = 0.3374834954738617\n",
            "I0420 22:36:54.993257 139904526645120 model_training_utils.py:450] Train Step: 21135/21936  / loss = 0.2453024834394455\n",
            "I0420 22:36:55.533401 139904526645120 model_training_utils.py:450] Train Step: 21136/21936  / loss = 0.34282296895980835\n",
            "I0420 22:36:56.072175 139904526645120 model_training_utils.py:450] Train Step: 21137/21936  / loss = 0.07432802021503448\n",
            "I0420 22:36:56.611743 139904526645120 model_training_utils.py:450] Train Step: 21138/21936  / loss = 0.19420970976352692\n",
            "I0420 22:36:57.147104 139904526645120 model_training_utils.py:450] Train Step: 21139/21936  / loss = 0.379335880279541\n",
            "I0420 22:36:57.690850 139904526645120 model_training_utils.py:450] Train Step: 21140/21936  / loss = 0.34659087657928467\n",
            "I0420 22:36:58.239771 139904526645120 model_training_utils.py:450] Train Step: 21141/21936  / loss = 0.4315944314002991\n",
            "I0420 22:36:58.776945 139904526645120 model_training_utils.py:450] Train Step: 21142/21936  / loss = 1.1419786214828491\n",
            "I0420 22:36:59.314888 139904526645120 model_training_utils.py:450] Train Step: 21143/21936  / loss = 0.6332612633705139\n",
            "I0420 22:36:59.850589 139904526645120 model_training_utils.py:450] Train Step: 21144/21936  / loss = 0.12659195065498352\n",
            "I0420 22:37:00.389684 139904526645120 model_training_utils.py:450] Train Step: 21145/21936  / loss = 0.9153463840484619\n",
            "I0420 22:37:00.926267 139904526645120 model_training_utils.py:450] Train Step: 21146/21936  / loss = 0.891872227191925\n",
            "I0420 22:37:01.465784 139904526645120 model_training_utils.py:450] Train Step: 21147/21936  / loss = 1.2575645446777344\n",
            "I0420 22:37:02.002327 139904526645120 model_training_utils.py:450] Train Step: 21148/21936  / loss = 0.26807960867881775\n",
            "I0420 22:37:02.543105 139904526645120 model_training_utils.py:450] Train Step: 21149/21936  / loss = 0.686295747756958\n",
            "I0420 22:37:03.080782 139904526645120 model_training_utils.py:450] Train Step: 21150/21936  / loss = 1.6827161312103271\n",
            "I0420 22:37:03.621256 139904526645120 model_training_utils.py:450] Train Step: 21151/21936  / loss = 1.9853063821792603\n",
            "I0420 22:37:04.158976 139904526645120 model_training_utils.py:450] Train Step: 21152/21936  / loss = 2.604180335998535\n",
            "I0420 22:37:04.694011 139904526645120 model_training_utils.py:450] Train Step: 21153/21936  / loss = 1.5455825328826904\n",
            "I0420 22:37:05.235590 139904526645120 model_training_utils.py:450] Train Step: 21154/21936  / loss = 1.7780508995056152\n",
            "I0420 22:37:05.773471 139904526645120 model_training_utils.py:450] Train Step: 21155/21936  / loss = 3.9094152450561523\n",
            "I0420 22:37:06.310243 139904526645120 model_training_utils.py:450] Train Step: 21156/21936  / loss = 2.0347204208374023\n",
            "I0420 22:37:06.851288 139904526645120 model_training_utils.py:450] Train Step: 21157/21936  / loss = 2.804213047027588\n",
            "I0420 22:37:07.389239 139904526645120 model_training_utils.py:450] Train Step: 21158/21936  / loss = 1.526082992553711\n",
            "I0420 22:37:07.926108 139904526645120 model_training_utils.py:450] Train Step: 21159/21936  / loss = 1.5497000217437744\n",
            "I0420 22:37:08.463959 139904526645120 model_training_utils.py:450] Train Step: 21160/21936  / loss = 1.1577845811843872\n",
            "I0420 22:37:09.001925 139904526645120 model_training_utils.py:450] Train Step: 21161/21936  / loss = 1.1418581008911133\n",
            "I0420 22:37:09.541547 139904526645120 model_training_utils.py:450] Train Step: 21162/21936  / loss = 1.7736042737960815\n",
            "I0420 22:37:10.079961 139904526645120 model_training_utils.py:450] Train Step: 21163/21936  / loss = 0.6258037686347961\n",
            "I0420 22:37:10.618525 139904526645120 model_training_utils.py:450] Train Step: 21164/21936  / loss = 1.1075713634490967\n",
            "I0420 22:37:11.156178 139904526645120 model_training_utils.py:450] Train Step: 21165/21936  / loss = 0.7042711973190308\n",
            "I0420 22:37:11.695702 139904526645120 model_training_utils.py:450] Train Step: 21166/21936  / loss = 1.9292550086975098\n",
            "I0420 22:37:12.231524 139904526645120 model_training_utils.py:450] Train Step: 21167/21936  / loss = 0.16725033521652222\n",
            "I0420 22:37:12.767119 139904526645120 model_training_utils.py:450] Train Step: 21168/21936  / loss = 2.284008026123047\n",
            "I0420 22:37:13.305939 139904526645120 keras_utils.py:122] TimeHistory: 26.91 seconds, 14.86 examples/second between steps 32087 and 32137\n",
            "I0420 22:37:13.308687 139904526645120 model_training_utils.py:450] Train Step: 21169/21936  / loss = 0.3111804127693176\n",
            "I0420 22:37:13.846199 139904526645120 model_training_utils.py:450] Train Step: 21170/21936  / loss = 1.4518485069274902\n",
            "I0420 22:37:14.382732 139904526645120 model_training_utils.py:450] Train Step: 21171/21936  / loss = 1.6497694253921509\n",
            "I0420 22:37:14.920216 139904526645120 model_training_utils.py:450] Train Step: 21172/21936  / loss = 0.6412492990493774\n",
            "I0420 22:37:15.456297 139904526645120 model_training_utils.py:450] Train Step: 21173/21936  / loss = 0.6153910160064697\n",
            "I0420 22:37:15.995051 139904526645120 model_training_utils.py:450] Train Step: 21174/21936  / loss = 0.4380497932434082\n",
            "I0420 22:37:16.533266 139904526645120 model_training_utils.py:450] Train Step: 21175/21936  / loss = 0.927420973777771\n",
            "I0420 22:37:17.070003 139904526645120 model_training_utils.py:450] Train Step: 21176/21936  / loss = 0.8128763437271118\n",
            "I0420 22:37:17.609942 139904526645120 model_training_utils.py:450] Train Step: 21177/21936  / loss = 0.06052936613559723\n",
            "I0420 22:37:18.148889 139904526645120 model_training_utils.py:450] Train Step: 21178/21936  / loss = 0.47758203744888306\n",
            "I0420 22:37:18.686689 139904526645120 model_training_utils.py:450] Train Step: 21179/21936  / loss = 0.9529501795768738\n",
            "I0420 22:37:19.226788 139904526645120 model_training_utils.py:450] Train Step: 21180/21936  / loss = 0.11406443268060684\n",
            "I0420 22:37:19.768434 139904526645120 model_training_utils.py:450] Train Step: 21181/21936  / loss = 0.16507357358932495\n",
            "I0420 22:37:20.306039 139904526645120 model_training_utils.py:450] Train Step: 21182/21936  / loss = 0.13688932359218597\n",
            "I0420 22:37:20.844724 139904526645120 model_training_utils.py:450] Train Step: 21183/21936  / loss = 1.347153663635254\n",
            "I0420 22:37:21.389393 139904526645120 model_training_utils.py:450] Train Step: 21184/21936  / loss = 0.5972375869750977\n",
            "I0420 22:37:21.930658 139904526645120 model_training_utils.py:450] Train Step: 21185/21936  / loss = 0.3572949171066284\n",
            "I0420 22:37:22.469332 139904526645120 model_training_utils.py:450] Train Step: 21186/21936  / loss = 0.4141893982887268\n",
            "I0420 22:37:23.011688 139904526645120 model_training_utils.py:450] Train Step: 21187/21936  / loss = 0.03143967688083649\n",
            "I0420 22:37:23.549655 139904526645120 model_training_utils.py:450] Train Step: 21188/21936  / loss = 0.08884044736623764\n",
            "I0420 22:37:24.092772 139904526645120 model_training_utils.py:450] Train Step: 21189/21936  / loss = 0.7260019779205322\n",
            "I0420 22:37:24.638826 139904526645120 model_training_utils.py:450] Train Step: 21190/21936  / loss = 0.5229095220565796\n",
            "I0420 22:37:25.177112 139904526645120 model_training_utils.py:450] Train Step: 21191/21936  / loss = 0.37361857295036316\n",
            "I0420 22:37:25.714212 139904526645120 model_training_utils.py:450] Train Step: 21192/21936  / loss = 0.2695363461971283\n",
            "I0420 22:37:26.250209 139904526645120 model_training_utils.py:450] Train Step: 21193/21936  / loss = 0.2623789310455322\n",
            "I0420 22:37:26.785581 139904526645120 model_training_utils.py:450] Train Step: 21194/21936  / loss = 0.332466185092926\n",
            "I0420 22:37:27.321673 139904526645120 model_training_utils.py:450] Train Step: 21195/21936  / loss = 1.3098421096801758\n",
            "I0420 22:37:27.859344 139904526645120 model_training_utils.py:450] Train Step: 21196/21936  / loss = 0.8319966793060303\n",
            "I0420 22:37:28.397198 139904526645120 model_training_utils.py:450] Train Step: 21197/21936  / loss = 0.24640914797782898\n",
            "I0420 22:37:28.935085 139904526645120 model_training_utils.py:450] Train Step: 21198/21936  / loss = 0.6882986426353455\n",
            "I0420 22:37:29.475203 139904526645120 model_training_utils.py:450] Train Step: 21199/21936  / loss = 0.39569008350372314\n",
            "I0420 22:37:30.012113 139904526645120 model_training_utils.py:450] Train Step: 21200/21936  / loss = 0.6844640970230103\n",
            "I0420 22:37:30.550594 139904526645120 model_training_utils.py:450] Train Step: 21201/21936  / loss = 0.6855847835540771\n",
            "I0420 22:37:31.088814 139904526645120 model_training_utils.py:450] Train Step: 21202/21936  / loss = 0.7099016308784485\n",
            "I0420 22:37:31.624680 139904526645120 model_training_utils.py:450] Train Step: 21203/21936  / loss = 1.49212646484375\n",
            "I0420 22:37:32.160785 139904526645120 model_training_utils.py:450] Train Step: 21204/21936  / loss = 0.3229544758796692\n",
            "I0420 22:37:32.700464 139904526645120 model_training_utils.py:450] Train Step: 21205/21936  / loss = 0.30818307399749756\n",
            "I0420 22:37:33.238718 139904526645120 model_training_utils.py:450] Train Step: 21206/21936  / loss = 0.5246521234512329\n",
            "I0420 22:37:33.776840 139904526645120 model_training_utils.py:450] Train Step: 21207/21936  / loss = 0.3466808795928955\n",
            "I0420 22:37:34.314923 139904526645120 model_training_utils.py:450] Train Step: 21208/21936  / loss = 0.07684862613677979\n",
            "I0420 22:37:34.851641 139904526645120 model_training_utils.py:450] Train Step: 21209/21936  / loss = 0.4104189872741699\n",
            "I0420 22:37:35.387649 139904526645120 model_training_utils.py:450] Train Step: 21210/21936  / loss = 1.6911489963531494\n",
            "I0420 22:37:35.925218 139904526645120 model_training_utils.py:450] Train Step: 21211/21936  / loss = 0.27862176299095154\n",
            "I0420 22:37:36.462063 139904526645120 model_training_utils.py:450] Train Step: 21212/21936  / loss = 0.8663142919540405\n",
            "I0420 22:37:36.999588 139904526645120 model_training_utils.py:450] Train Step: 21213/21936  / loss = 0.8265676498413086\n",
            "I0420 22:37:37.537173 139904526645120 model_training_utils.py:450] Train Step: 21214/21936  / loss = 0.16698169708251953\n",
            "I0420 22:37:38.073113 139904526645120 model_training_utils.py:450] Train Step: 21215/21936  / loss = 0.5465559959411621\n",
            "I0420 22:37:38.610032 139904526645120 model_training_utils.py:450] Train Step: 21216/21936  / loss = 0.4117017984390259\n",
            "I0420 22:37:39.152995 139904526645120 model_training_utils.py:450] Train Step: 21217/21936  / loss = 0.1964312642812729\n",
            "I0420 22:37:39.695542 139904526645120 model_training_utils.py:450] Train Step: 21218/21936  / loss = 0.06089267134666443\n",
            "I0420 22:37:40.235859 139904526645120 keras_utils.py:122] TimeHistory: 26.93 seconds, 14.86 examples/second between steps 32137 and 32187\n",
            "I0420 22:37:40.238681 139904526645120 model_training_utils.py:450] Train Step: 21219/21936  / loss = 0.22044986486434937\n",
            "I0420 22:37:40.776250 139904526645120 model_training_utils.py:450] Train Step: 21220/21936  / loss = 0.32218319177627563\n",
            "I0420 22:37:41.316506 139904526645120 model_training_utils.py:450] Train Step: 21221/21936  / loss = 0.34245938062667847\n",
            "I0420 22:37:41.855767 139904526645120 model_training_utils.py:450] Train Step: 21222/21936  / loss = 0.45289939641952515\n",
            "I0420 22:37:42.397197 139904526645120 model_training_utils.py:450] Train Step: 21223/21936  / loss = 0.9402098059654236\n",
            "I0420 22:37:42.936866 139904526645120 model_training_utils.py:450] Train Step: 21224/21936  / loss = 0.4069685935974121\n",
            "I0420 22:37:43.474911 139904526645120 model_training_utils.py:450] Train Step: 21225/21936  / loss = 1.3302241563796997\n",
            "I0420 22:37:44.014842 139904526645120 model_training_utils.py:450] Train Step: 21226/21936  / loss = 0.5339588522911072\n",
            "I0420 22:37:44.556028 139904526645120 model_training_utils.py:450] Train Step: 21227/21936  / loss = 0.21945922076702118\n",
            "I0420 22:37:45.091759 139904526645120 model_training_utils.py:450] Train Step: 21228/21936  / loss = 1.166658878326416\n",
            "I0420 22:37:45.627535 139904526645120 model_training_utils.py:450] Train Step: 21229/21936  / loss = 0.7772105932235718\n",
            "I0420 22:37:46.166780 139904526645120 model_training_utils.py:450] Train Step: 21230/21936  / loss = 0.27851641178131104\n",
            "I0420 22:37:46.705531 139904526645120 model_training_utils.py:450] Train Step: 21231/21936  / loss = 1.388521432876587\n",
            "I0420 22:37:47.247593 139904526645120 model_training_utils.py:450] Train Step: 21232/21936  / loss = 0.9388506412506104\n",
            "I0420 22:37:47.788022 139904526645120 model_training_utils.py:450] Train Step: 21233/21936  / loss = 0.192792147397995\n",
            "I0420 22:37:48.326198 139904526645120 model_training_utils.py:450] Train Step: 21234/21936  / loss = 0.48264604806900024\n",
            "I0420 22:37:48.864826 139904526645120 model_training_utils.py:450] Train Step: 21235/21936  / loss = 0.10977379977703094\n",
            "I0420 22:37:49.403481 139904526645120 model_training_utils.py:450] Train Step: 21236/21936  / loss = 0.6037933826446533\n",
            "I0420 22:37:49.941706 139904526645120 model_training_utils.py:450] Train Step: 21237/21936  / loss = 0.48318228125572205\n",
            "I0420 22:37:50.480244 139904526645120 model_training_utils.py:450] Train Step: 21238/21936  / loss = 0.1780087798833847\n",
            "I0420 22:37:51.018238 139904526645120 model_training_utils.py:450] Train Step: 21239/21936  / loss = 0.72010338306427\n",
            "I0420 22:37:51.555314 139904526645120 model_training_utils.py:450] Train Step: 21240/21936  / loss = 0.8493633270263672\n",
            "I0420 22:37:52.094838 139904526645120 model_training_utils.py:450] Train Step: 21241/21936  / loss = 0.7064914107322693\n",
            "I0420 22:37:52.630927 139904526645120 model_training_utils.py:450] Train Step: 21242/21936  / loss = 1.491697072982788\n",
            "I0420 22:37:53.167290 139904526645120 model_training_utils.py:450] Train Step: 21243/21936  / loss = 0.7190921306610107\n",
            "I0420 22:37:53.706582 139904526645120 model_training_utils.py:450] Train Step: 21244/21936  / loss = 0.37105631828308105\n",
            "I0420 22:37:54.246129 139904526645120 model_training_utils.py:450] Train Step: 21245/21936  / loss = 0.7958440184593201\n",
            "I0420 22:37:54.784247 139904526645120 model_training_utils.py:450] Train Step: 21246/21936  / loss = 0.3388022780418396\n",
            "I0420 22:37:55.320494 139904526645120 model_training_utils.py:450] Train Step: 21247/21936  / loss = 0.3886633813381195\n",
            "I0420 22:37:55.862667 139904526645120 model_training_utils.py:450] Train Step: 21248/21936  / loss = 1.0341171026229858\n",
            "I0420 22:37:56.403908 139904526645120 model_training_utils.py:450] Train Step: 21249/21936  / loss = 0.5278586149215698\n",
            "I0420 22:37:56.941129 139904526645120 model_training_utils.py:450] Train Step: 21250/21936  / loss = 0.5990336537361145\n",
            "I0420 22:37:57.480218 139904526645120 model_training_utils.py:450] Train Step: 21251/21936  / loss = 0.7662723064422607\n",
            "I0420 22:37:58.016858 139904526645120 model_training_utils.py:450] Train Step: 21252/21936  / loss = 0.21604761481285095\n",
            "I0420 22:37:58.553804 139904526645120 model_training_utils.py:450] Train Step: 21253/21936  / loss = 0.4781067967414856\n",
            "I0420 22:37:59.095745 139904526645120 model_training_utils.py:450] Train Step: 21254/21936  / loss = 0.7196757793426514\n",
            "I0420 22:37:59.636742 139904526645120 model_training_utils.py:450] Train Step: 21255/21936  / loss = 0.6380387544631958\n",
            "I0420 22:38:00.175079 139904526645120 model_training_utils.py:450] Train Step: 21256/21936  / loss = 0.6541988253593445\n",
            "I0420 22:38:00.715510 139904526645120 model_training_utils.py:450] Train Step: 21257/21936  / loss = 0.09366239607334137\n",
            "I0420 22:38:01.253761 139904526645120 model_training_utils.py:450] Train Step: 21258/21936  / loss = 0.12084263563156128\n",
            "I0420 22:38:01.792713 139904526645120 model_training_utils.py:450] Train Step: 21259/21936  / loss = 0.259664386510849\n",
            "I0420 22:38:02.330711 139904526645120 model_training_utils.py:450] Train Step: 21260/21936  / loss = 0.18318411707878113\n",
            "I0420 22:38:02.867738 139904526645120 model_training_utils.py:450] Train Step: 21261/21936  / loss = 0.3634472191333771\n",
            "I0420 22:38:03.408460 139904526645120 model_training_utils.py:450] Train Step: 21262/21936  / loss = 0.8744902610778809\n",
            "I0420 22:38:03.946981 139904526645120 model_training_utils.py:450] Train Step: 21263/21936  / loss = 0.754550039768219\n",
            "I0420 22:38:04.484991 139904526645120 model_training_utils.py:450] Train Step: 21264/21936  / loss = 0.2221030741930008\n",
            "I0420 22:38:05.023847 139904526645120 model_training_utils.py:450] Train Step: 21265/21936  / loss = 0.21875914931297302\n",
            "I0420 22:38:05.563725 139904526645120 model_training_utils.py:450] Train Step: 21266/21936  / loss = 0.1578000783920288\n",
            "I0420 22:38:06.102663 139904526645120 model_training_utils.py:450] Train Step: 21267/21936  / loss = 0.39759695529937744\n",
            "I0420 22:38:06.640822 139904526645120 model_training_utils.py:450] Train Step: 21268/21936  / loss = 0.2050396353006363\n",
            "I0420 22:38:07.181449 139904526645120 keras_utils.py:122] TimeHistory: 26.94 seconds, 14.85 examples/second between steps 32187 and 32237\n",
            "I0420 22:38:07.184249 139904526645120 model_training_utils.py:450] Train Step: 21269/21936  / loss = 0.11958710849285126\n",
            "I0420 22:38:07.722500 139904526645120 model_training_utils.py:450] Train Step: 21270/21936  / loss = 0.3760823607444763\n",
            "I0420 22:38:08.263754 139904526645120 model_training_utils.py:450] Train Step: 21271/21936  / loss = 0.1251753568649292\n",
            "I0420 22:38:08.802993 139904526645120 model_training_utils.py:450] Train Step: 21272/21936  / loss = 0.43989354372024536\n",
            "I0420 22:38:09.345293 139904526645120 model_training_utils.py:450] Train Step: 21273/21936  / loss = 1.176676630973816\n",
            "I0420 22:38:09.883135 139904526645120 model_training_utils.py:450] Train Step: 21274/21936  / loss = 1.2986817359924316\n",
            "I0420 22:38:10.420549 139904526645120 model_training_utils.py:450] Train Step: 21275/21936  / loss = 0.26020267605781555\n",
            "I0420 22:38:10.957870 139904526645120 model_training_utils.py:450] Train Step: 21276/21936  / loss = 0.15154345333576202\n",
            "I0420 22:38:11.498382 139904526645120 model_training_utils.py:450] Train Step: 21277/21936  / loss = 0.2098826766014099\n",
            "I0420 22:38:12.036480 139904526645120 model_training_utils.py:450] Train Step: 21278/21936  / loss = 0.373526930809021\n",
            "I0420 22:38:12.573829 139904526645120 model_training_utils.py:450] Train Step: 21279/21936  / loss = 0.10186930000782013\n",
            "I0420 22:38:13.111760 139904526645120 model_training_utils.py:450] Train Step: 21280/21936  / loss = 0.5079674124717712\n",
            "I0420 22:38:13.651638 139904526645120 model_training_utils.py:450] Train Step: 21281/21936  / loss = 0.4804914891719818\n",
            "I0420 22:38:14.187270 139904526645120 model_training_utils.py:450] Train Step: 21282/21936  / loss = 0.3144172430038452\n",
            "I0420 22:38:14.724052 139904526645120 model_training_utils.py:450] Train Step: 21283/21936  / loss = 0.4701438248157501\n",
            "I0420 22:38:15.259696 139904526645120 model_training_utils.py:450] Train Step: 21284/21936  / loss = 1.3710768222808838\n",
            "I0420 22:38:15.798143 139904526645120 model_training_utils.py:450] Train Step: 21285/21936  / loss = 0.18321387469768524\n",
            "I0420 22:38:16.335217 139904526645120 model_training_utils.py:450] Train Step: 21286/21936  / loss = 0.9124507904052734\n",
            "I0420 22:38:16.876037 139904526645120 model_training_utils.py:450] Train Step: 21287/21936  / loss = 0.13439549505710602\n",
            "I0420 22:38:17.412352 139904526645120 model_training_utils.py:450] Train Step: 21288/21936  / loss = 0.32585716247558594\n",
            "I0420 22:38:17.951739 139904526645120 model_training_utils.py:450] Train Step: 21289/21936  / loss = 0.8568205833435059\n",
            "I0420 22:38:18.491344 139904526645120 model_training_utils.py:450] Train Step: 21290/21936  / loss = 0.7490840554237366\n",
            "I0420 22:38:19.030529 139904526645120 model_training_utils.py:450] Train Step: 21291/21936  / loss = 1.073933720588684\n",
            "I0420 22:38:19.568693 139904526645120 model_training_utils.py:450] Train Step: 21292/21936  / loss = 0.3215547800064087\n",
            "I0420 22:38:20.105858 139904526645120 model_training_utils.py:450] Train Step: 21293/21936  / loss = 0.2413623332977295\n",
            "I0420 22:38:20.642296 139904526645120 model_training_utils.py:450] Train Step: 21294/21936  / loss = 0.38885146379470825\n",
            "I0420 22:38:21.179683 139904526645120 model_training_utils.py:450] Train Step: 21295/21936  / loss = 0.14688915014266968\n",
            "I0420 22:38:21.717375 139904526645120 model_training_utils.py:450] Train Step: 21296/21936  / loss = 0.7222164869308472\n",
            "I0420 22:38:22.255857 139904526645120 model_training_utils.py:450] Train Step: 21297/21936  / loss = 0.09135216474533081\n",
            "I0420 22:38:22.794961 139904526645120 model_training_utils.py:450] Train Step: 21298/21936  / loss = 0.08529924601316452\n",
            "I0420 22:38:23.334842 139904526645120 model_training_utils.py:450] Train Step: 21299/21936  / loss = 0.06976629048585892\n",
            "I0420 22:38:23.874703 139904526645120 model_training_utils.py:450] Train Step: 21300/21936  / loss = 0.09862609207630157\n",
            "I0420 22:38:24.413364 139904526645120 model_training_utils.py:450] Train Step: 21301/21936  / loss = 0.20500081777572632\n",
            "I0420 22:38:24.951317 139904526645120 model_training_utils.py:450] Train Step: 21302/21936  / loss = 0.07941144704818726\n",
            "I0420 22:38:25.486247 139904526645120 model_training_utils.py:450] Train Step: 21303/21936  / loss = 0.1796102225780487\n",
            "I0420 22:38:26.030262 139904526645120 model_training_utils.py:450] Train Step: 21304/21936  / loss = 0.9538266062736511\n",
            "I0420 22:38:26.569556 139904526645120 model_training_utils.py:450] Train Step: 21305/21936  / loss = 0.8178688287734985\n",
            "I0420 22:38:27.107863 139904526645120 model_training_utils.py:450] Train Step: 21306/21936  / loss = 0.8131192922592163\n",
            "I0420 22:38:27.646018 139904526645120 model_training_utils.py:450] Train Step: 21307/21936  / loss = 0.5565280914306641\n",
            "I0420 22:38:28.181680 139904526645120 model_training_utils.py:450] Train Step: 21308/21936  / loss = 0.5423071384429932\n",
            "I0420 22:38:28.724238 139904526645120 model_training_utils.py:450] Train Step: 21309/21936  / loss = 0.3838096261024475\n",
            "I0420 22:38:29.261086 139904526645120 model_training_utils.py:450] Train Step: 21310/21936  / loss = 0.6133649349212646\n",
            "I0420 22:38:29.800395 139904526645120 model_training_utils.py:450] Train Step: 21311/21936  / loss = 0.28013214468955994\n",
            "I0420 22:38:30.336431 139904526645120 model_training_utils.py:450] Train Step: 21312/21936  / loss = 0.6202911734580994\n",
            "I0420 22:38:30.873951 139904526645120 model_training_utils.py:450] Train Step: 21313/21936  / loss = 0.31241485476493835\n",
            "I0420 22:38:31.415686 139904526645120 model_training_utils.py:450] Train Step: 21314/21936  / loss = 0.38925766944885254\n",
            "I0420 22:38:31.953091 139904526645120 model_training_utils.py:450] Train Step: 21315/21936  / loss = 0.3074299693107605\n",
            "I0420 22:38:32.490832 139904526645120 model_training_utils.py:450] Train Step: 21316/21936  / loss = 0.5075541734695435\n",
            "I0420 22:38:33.028265 139904526645120 model_training_utils.py:450] Train Step: 21317/21936  / loss = 0.45269775390625\n",
            "I0420 22:38:33.565820 139904526645120 model_training_utils.py:450] Train Step: 21318/21936  / loss = 0.5255866050720215\n",
            "I0420 22:38:34.104004 139904526645120 keras_utils.py:122] TimeHistory: 26.92 seconds, 14.86 examples/second between steps 32237 and 32287\n",
            "I0420 22:38:34.106926 139904526645120 model_training_utils.py:450] Train Step: 21319/21936  / loss = 0.35906606912612915\n",
            "I0420 22:38:34.644330 139904526645120 model_training_utils.py:450] Train Step: 21320/21936  / loss = 0.32124078273773193\n",
            "I0420 22:38:35.182631 139904526645120 model_training_utils.py:450] Train Step: 21321/21936  / loss = 0.9658392667770386\n",
            "I0420 22:38:35.724350 139904526645120 model_training_utils.py:450] Train Step: 21322/21936  / loss = 0.546888530254364\n",
            "I0420 22:38:36.262027 139904526645120 model_training_utils.py:450] Train Step: 21323/21936  / loss = 0.7314053773880005\n",
            "I0420 22:38:36.801021 139904526645120 model_training_utils.py:450] Train Step: 21324/21936  / loss = 0.352963387966156\n",
            "I0420 22:38:37.340758 139904526645120 model_training_utils.py:450] Train Step: 21325/21936  / loss = 0.38305237889289856\n",
            "I0420 22:38:37.879512 139904526645120 model_training_utils.py:450] Train Step: 21326/21936  / loss = 0.7281997799873352\n",
            "I0420 22:38:38.423167 139904526645120 model_training_utils.py:450] Train Step: 21327/21936  / loss = 0.7191485166549683\n",
            "I0420 22:38:38.961298 139904526645120 model_training_utils.py:450] Train Step: 21328/21936  / loss = 0.29584044218063354\n",
            "I0420 22:38:39.497382 139904526645120 model_training_utils.py:450] Train Step: 21329/21936  / loss = 0.3716694712638855\n",
            "I0420 22:38:40.032741 139904526645120 model_training_utils.py:450] Train Step: 21330/21936  / loss = 0.24609708786010742\n",
            "I0420 22:38:40.568522 139904526645120 model_training_utils.py:450] Train Step: 21331/21936  / loss = 0.13167521357536316\n",
            "I0420 22:38:41.105857 139904526645120 model_training_utils.py:450] Train Step: 21332/21936  / loss = 0.2140800654888153\n",
            "I0420 22:38:41.646257 139904526645120 model_training_utils.py:450] Train Step: 21333/21936  / loss = 0.10943406075239182\n",
            "I0420 22:38:42.183749 139904526645120 model_training_utils.py:450] Train Step: 21334/21936  / loss = 0.29083308577537537\n",
            "I0420 22:38:42.723341 139904526645120 model_training_utils.py:450] Train Step: 21335/21936  / loss = 0.3504698574542999\n",
            "I0420 22:38:43.260068 139904526645120 model_training_utils.py:450] Train Step: 21336/21936  / loss = 0.2809978723526001\n",
            "I0420 22:38:43.802785 139904526645120 model_training_utils.py:450] Train Step: 21337/21936  / loss = 0.5609135627746582\n",
            "I0420 22:38:44.340784 139904526645120 model_training_utils.py:450] Train Step: 21338/21936  / loss = 0.3716951906681061\n",
            "I0420 22:38:44.878330 139904526645120 model_training_utils.py:450] Train Step: 21339/21936  / loss = 0.34591519832611084\n",
            "I0420 22:38:45.418008 139904526645120 model_training_utils.py:450] Train Step: 21340/21936  / loss = 0.4509197473526001\n",
            "I0420 22:38:45.956953 139904526645120 model_training_utils.py:450] Train Step: 21341/21936  / loss = 0.3480053246021271\n",
            "I0420 22:38:46.492508 139904526645120 model_training_utils.py:450] Train Step: 21342/21936  / loss = 1.4730231761932373\n",
            "I0420 22:38:47.030058 139904526645120 model_training_utils.py:450] Train Step: 21343/21936  / loss = 0.44755280017852783\n",
            "I0420 22:38:47.566386 139904526645120 model_training_utils.py:450] Train Step: 21344/21936  / loss = 0.3338563144207001\n",
            "I0420 22:38:48.104670 139904526645120 model_training_utils.py:450] Train Step: 21345/21936  / loss = 0.45973920822143555\n",
            "I0420 22:38:48.639800 139904526645120 model_training_utils.py:450] Train Step: 21346/21936  / loss = 0.17545297741889954\n",
            "I0420 22:38:49.176533 139904526645120 model_training_utils.py:450] Train Step: 21347/21936  / loss = 0.3791247606277466\n",
            "I0420 22:38:49.716022 139904526645120 model_training_utils.py:450] Train Step: 21348/21936  / loss = 0.28796395659446716\n",
            "I0420 22:38:50.252523 139904526645120 model_training_utils.py:450] Train Step: 21349/21936  / loss = 0.17336204648017883\n",
            "I0420 22:38:50.788754 139904526645120 model_training_utils.py:450] Train Step: 21350/21936  / loss = 0.2207048237323761\n",
            "I0420 22:38:51.326213 139904526645120 model_training_utils.py:450] Train Step: 21351/21936  / loss = 0.45741331577301025\n",
            "I0420 22:38:51.863424 139904526645120 model_training_utils.py:450] Train Step: 21352/21936  / loss = 0.8258405923843384\n",
            "I0420 22:38:52.413260 139904526645120 model_training_utils.py:450] Train Step: 21353/21936  / loss = 0.24523407220840454\n",
            "I0420 22:38:52.951290 139904526645120 model_training_utils.py:450] Train Step: 21354/21936  / loss = 0.26407405734062195\n",
            "I0420 22:38:53.493638 139904526645120 model_training_utils.py:450] Train Step: 21355/21936  / loss = 0.617717981338501\n",
            "I0420 22:38:54.028429 139904526645120 model_training_utils.py:450] Train Step: 21356/21936  / loss = 0.41978850960731506\n",
            "I0420 22:38:54.565883 139904526645120 model_training_utils.py:450] Train Step: 21357/21936  / loss = 0.19441749155521393\n",
            "I0420 22:38:55.103299 139904526645120 model_training_utils.py:450] Train Step: 21358/21936  / loss = 0.5991941094398499\n",
            "I0420 22:38:55.640416 139904526645120 model_training_utils.py:450] Train Step: 21359/21936  / loss = 0.933045506477356\n",
            "I0420 22:38:56.178477 139904526645120 model_training_utils.py:450] Train Step: 21360/21936  / loss = 0.24268624186515808\n",
            "I0420 22:38:56.716272 139904526645120 model_training_utils.py:450] Train Step: 21361/21936  / loss = 0.3240809738636017\n",
            "I0420 22:38:57.252944 139904526645120 model_training_utils.py:450] Train Step: 21362/21936  / loss = 0.44213786721229553\n",
            "I0420 22:38:57.793416 139904526645120 model_training_utils.py:450] Train Step: 21363/21936  / loss = 0.626555860042572\n",
            "I0420 22:38:58.328361 139904526645120 model_training_utils.py:450] Train Step: 21364/21936  / loss = 0.5090821385383606\n",
            "I0420 22:38:58.866448 139904526645120 model_training_utils.py:450] Train Step: 21365/21936  / loss = 0.182095468044281\n",
            "I0420 22:38:59.404609 139904526645120 model_training_utils.py:450] Train Step: 21366/21936  / loss = 0.23193705081939697\n",
            "I0420 22:38:59.943653 139904526645120 model_training_utils.py:450] Train Step: 21367/21936  / loss = 0.2556306719779968\n",
            "I0420 22:39:00.484124 139904526645120 model_training_utils.py:450] Train Step: 21368/21936  / loss = 0.5954521894454956\n",
            "I0420 22:39:01.020890 139904526645120 keras_utils.py:122] TimeHistory: 26.91 seconds, 14.86 examples/second between steps 32287 and 32337\n",
            "I0420 22:39:01.023715 139904526645120 model_training_utils.py:450] Train Step: 21369/21936  / loss = 0.48172736167907715\n",
            "I0420 22:39:01.560619 139904526645120 model_training_utils.py:450] Train Step: 21370/21936  / loss = 0.06043917313218117\n",
            "I0420 22:39:02.100665 139904526645120 model_training_utils.py:450] Train Step: 21371/21936  / loss = 0.29603341221809387\n",
            "I0420 22:39:02.642162 139904526645120 model_training_utils.py:450] Train Step: 21372/21936  / loss = 0.2487717866897583\n",
            "I0420 22:39:03.181092 139904526645120 model_training_utils.py:450] Train Step: 21373/21936  / loss = 0.5272315740585327\n",
            "I0420 22:39:03.717676 139904526645120 model_training_utils.py:450] Train Step: 21374/21936  / loss = 0.468138188123703\n",
            "I0420 22:39:04.259041 139904526645120 model_training_utils.py:450] Train Step: 21375/21936  / loss = 0.7208988666534424\n",
            "I0420 22:39:04.795105 139904526645120 model_training_utils.py:450] Train Step: 21376/21936  / loss = 0.28897586464881897\n",
            "I0420 22:39:05.338518 139904526645120 model_training_utils.py:450] Train Step: 21377/21936  / loss = 0.6284257769584656\n",
            "I0420 22:39:05.876856 139904526645120 model_training_utils.py:450] Train Step: 21378/21936  / loss = 0.4132944643497467\n",
            "I0420 22:39:06.415421 139904526645120 model_training_utils.py:450] Train Step: 21379/21936  / loss = 0.6736051440238953\n",
            "I0420 22:39:06.953550 139904526645120 model_training_utils.py:450] Train Step: 21380/21936  / loss = 0.35262757539749146\n",
            "I0420 22:39:07.490523 139904526645120 model_training_utils.py:450] Train Step: 21381/21936  / loss = 1.581874132156372\n",
            "I0420 22:39:08.027200 139904526645120 model_training_utils.py:450] Train Step: 21382/21936  / loss = 0.039092570543289185\n",
            "I0420 22:39:08.566352 139904526645120 model_training_utils.py:450] Train Step: 21383/21936  / loss = 0.2751827538013458\n",
            "I0420 22:39:09.103580 139904526645120 model_training_utils.py:450] Train Step: 21384/21936  / loss = 0.33509767055511475\n",
            "I0420 22:39:09.638110 139904526645120 model_training_utils.py:450] Train Step: 21385/21936  / loss = 0.9853813648223877\n",
            "I0420 22:39:10.175639 139904526645120 model_training_utils.py:450] Train Step: 21386/21936  / loss = 0.9167463183403015\n",
            "I0420 22:39:10.712329 139904526645120 model_training_utils.py:450] Train Step: 21387/21936  / loss = 0.5899133682250977\n",
            "I0420 22:39:11.248741 139904526645120 model_training_utils.py:450] Train Step: 21388/21936  / loss = 0.34297749400138855\n",
            "I0420 22:39:11.783380 139904526645120 model_training_utils.py:450] Train Step: 21389/21936  / loss = 0.36439675092697144\n",
            "I0420 22:39:12.320470 139904526645120 model_training_utils.py:450] Train Step: 21390/21936  / loss = 0.909392237663269\n",
            "I0420 22:39:12.857224 139904526645120 model_training_utils.py:450] Train Step: 21391/21936  / loss = 0.28062519431114197\n",
            "I0420 22:39:13.393365 139904526645120 model_training_utils.py:450] Train Step: 21392/21936  / loss = 0.31624945998191833\n",
            "I0420 22:39:13.929225 139904526645120 model_training_utils.py:450] Train Step: 21393/21936  / loss = 0.6425853967666626\n",
            "I0420 22:39:14.465980 139904526645120 model_training_utils.py:450] Train Step: 21394/21936  / loss = 0.3129674196243286\n",
            "I0420 22:39:15.003502 139904526645120 model_training_utils.py:450] Train Step: 21395/21936  / loss = 0.1531749665737152\n",
            "I0420 22:39:15.543055 139904526645120 model_training_utils.py:450] Train Step: 21396/21936  / loss = 0.637061595916748\n",
            "I0420 22:39:16.090073 139904526645120 model_training_utils.py:450] Train Step: 21397/21936  / loss = 0.5474564433097839\n",
            "I0420 22:39:16.633470 139904526645120 model_training_utils.py:450] Train Step: 21398/21936  / loss = 0.41126543283462524\n",
            "I0420 22:39:17.169359 139904526645120 model_training_utils.py:450] Train Step: 21399/21936  / loss = 0.08371485024690628\n",
            "I0420 22:39:17.706463 139904526645120 model_training_utils.py:450] Train Step: 21400/21936  / loss = 1.7484521865844727\n",
            "I0420 22:39:18.242928 139904526645120 model_training_utils.py:450] Train Step: 21401/21936  / loss = 0.16796813905239105\n",
            "I0420 22:39:18.779490 139904526645120 model_training_utils.py:450] Train Step: 21402/21936  / loss = 0.06704629957675934\n",
            "I0420 22:39:19.318639 139904526645120 model_training_utils.py:450] Train Step: 21403/21936  / loss = 0.13465259969234467\n",
            "I0420 22:39:19.855539 139904526645120 model_training_utils.py:450] Train Step: 21404/21936  / loss = 0.3106757402420044\n",
            "I0420 22:39:20.394217 139904526645120 model_training_utils.py:450] Train Step: 21405/21936  / loss = 1.0772342681884766\n",
            "I0420 22:39:20.931041 139904526645120 model_training_utils.py:450] Train Step: 21406/21936  / loss = 0.8453112244606018\n",
            "I0420 22:39:21.471213 139904526645120 model_training_utils.py:450] Train Step: 21407/21936  / loss = 0.8211997151374817\n",
            "I0420 22:39:22.010466 139904526645120 model_training_utils.py:450] Train Step: 21408/21936  / loss = 0.31227558851242065\n",
            "I0420 22:39:22.546422 139904526645120 model_training_utils.py:450] Train Step: 21409/21936  / loss = 0.15029944479465485\n",
            "I0420 22:39:23.083418 139904526645120 model_training_utils.py:450] Train Step: 21410/21936  / loss = 0.2589702606201172\n",
            "I0420 22:39:23.619278 139904526645120 model_training_utils.py:450] Train Step: 21411/21936  / loss = 0.4455563426017761\n",
            "I0420 22:39:24.157537 139904526645120 model_training_utils.py:450] Train Step: 21412/21936  / loss = 0.6812769174575806\n",
            "I0420 22:39:24.693658 139904526645120 model_training_utils.py:450] Train Step: 21413/21936  / loss = 0.11346127837896347\n",
            "I0420 22:39:25.234808 139904526645120 model_training_utils.py:450] Train Step: 21414/21936  / loss = 1.1328930854797363\n",
            "I0420 22:39:25.773115 139904526645120 model_training_utils.py:450] Train Step: 21415/21936  / loss = 0.69029700756073\n",
            "I0420 22:39:26.309747 139904526645120 model_training_utils.py:450] Train Step: 21416/21936  / loss = 0.42898523807525635\n",
            "I0420 22:39:26.847340 139904526645120 model_training_utils.py:450] Train Step: 21417/21936  / loss = 0.7643784880638123\n",
            "I0420 22:39:27.384502 139904526645120 model_training_utils.py:450] Train Step: 21418/21936  / loss = 0.8458954095840454\n",
            "I0420 22:39:27.923060 139904526645120 keras_utils.py:122] TimeHistory: 26.90 seconds, 14.87 examples/second between steps 32337 and 32387\n",
            "I0420 22:39:27.925807 139904526645120 model_training_utils.py:450] Train Step: 21419/21936  / loss = 0.4844091534614563\n",
            "I0420 22:39:28.470431 139904526645120 model_training_utils.py:450] Train Step: 21420/21936  / loss = 0.9330353736877441\n",
            "I0420 22:39:29.008594 139904526645120 model_training_utils.py:450] Train Step: 21421/21936  / loss = 0.3638523519039154\n",
            "I0420 22:39:29.545638 139904526645120 model_training_utils.py:450] Train Step: 21422/21936  / loss = 0.6007040143013\n",
            "I0420 22:39:30.082279 139904526645120 model_training_utils.py:450] Train Step: 21423/21936  / loss = 0.41497910022735596\n",
            "I0420 22:39:30.617532 139904526645120 model_training_utils.py:450] Train Step: 21424/21936  / loss = 0.57284015417099\n",
            "I0420 22:39:31.159736 139904526645120 model_training_utils.py:450] Train Step: 21425/21936  / loss = 0.7281405329704285\n",
            "I0420 22:39:31.698154 139904526645120 model_training_utils.py:450] Train Step: 21426/21936  / loss = 0.6585966348648071\n",
            "I0420 22:39:32.236161 139904526645120 model_training_utils.py:450] Train Step: 21427/21936  / loss = 0.10506493598222733\n",
            "I0420 22:39:32.774496 139904526645120 model_training_utils.py:450] Train Step: 21428/21936  / loss = 0.9708687663078308\n",
            "I0420 22:39:33.311928 139904526645120 model_training_utils.py:450] Train Step: 21429/21936  / loss = 0.2226754128932953\n",
            "I0420 22:39:33.849103 139904526645120 model_training_utils.py:450] Train Step: 21430/21936  / loss = 0.2578299045562744\n",
            "I0420 22:39:34.389797 139904526645120 model_training_utils.py:450] Train Step: 21431/21936  / loss = 0.9182345867156982\n",
            "I0420 22:39:34.926821 139904526645120 model_training_utils.py:450] Train Step: 21432/21936  / loss = 0.5219366550445557\n",
            "I0420 22:39:35.463465 139904526645120 model_training_utils.py:450] Train Step: 21433/21936  / loss = 0.40620851516723633\n",
            "I0420 22:39:36.003319 139904526645120 model_training_utils.py:450] Train Step: 21434/21936  / loss = 0.35494524240493774\n",
            "I0420 22:39:36.540297 139904526645120 model_training_utils.py:450] Train Step: 21435/21936  / loss = 0.33921119570732117\n",
            "I0420 22:39:37.083993 139904526645120 model_training_utils.py:450] Train Step: 21436/21936  / loss = 0.2155619114637375\n",
            "I0420 22:39:37.623994 139904526645120 model_training_utils.py:450] Train Step: 21437/21936  / loss = 1.3765625953674316\n",
            "I0420 22:39:38.167943 139904526645120 model_training_utils.py:450] Train Step: 21438/21936  / loss = 0.3330167531967163\n",
            "I0420 22:39:38.707599 139904526645120 model_training_utils.py:450] Train Step: 21439/21936  / loss = 0.5698273181915283\n",
            "I0420 22:39:39.245959 139904526645120 model_training_utils.py:450] Train Step: 21440/21936  / loss = 0.5478134155273438\n",
            "I0420 22:39:39.783535 139904526645120 model_training_utils.py:450] Train Step: 21441/21936  / loss = 0.451957643032074\n",
            "I0420 22:39:40.322781 139904526645120 model_training_utils.py:450] Train Step: 21442/21936  / loss = 0.17745646834373474\n",
            "I0420 22:39:40.861866 139904526645120 model_training_utils.py:450] Train Step: 21443/21936  / loss = 0.08905180543661118\n",
            "I0420 22:39:41.401453 139904526645120 model_training_utils.py:450] Train Step: 21444/21936  / loss = 0.10146530717611313\n",
            "I0420 22:39:41.940022 139904526645120 model_training_utils.py:450] Train Step: 21445/21936  / loss = 0.7376948595046997\n",
            "I0420 22:39:42.476202 139904526645120 model_training_utils.py:450] Train Step: 21446/21936  / loss = 0.2770613431930542\n",
            "I0420 22:39:43.013373 139904526645120 model_training_utils.py:450] Train Step: 21447/21936  / loss = 0.5137707591056824\n",
            "I0420 22:39:43.552751 139904526645120 model_training_utils.py:450] Train Step: 21448/21936  / loss = 0.3717743158340454\n",
            "I0420 22:39:44.091232 139904526645120 model_training_utils.py:450] Train Step: 21449/21936  / loss = 0.13373734056949615\n",
            "I0420 22:39:44.627712 139904526645120 model_training_utils.py:450] Train Step: 21450/21936  / loss = 1.28286874294281\n",
            "I0420 22:39:45.167060 139904526645120 model_training_utils.py:450] Train Step: 21451/21936  / loss = 0.3385762572288513\n",
            "I0420 22:39:45.703044 139904526645120 model_training_utils.py:450] Train Step: 21452/21936  / loss = 0.39328932762145996\n",
            "I0420 22:39:46.241786 139904526645120 model_training_utils.py:450] Train Step: 21453/21936  / loss = 0.9839033484458923\n",
            "I0420 22:39:46.778975 139904526645120 model_training_utils.py:450] Train Step: 21454/21936  / loss = 0.049776773899793625\n",
            "I0420 22:39:47.313688 139904526645120 model_training_utils.py:450] Train Step: 21455/21936  / loss = 1.7394428253173828\n",
            "I0420 22:39:47.848162 139904526645120 model_training_utils.py:450] Train Step: 21456/21936  / loss = 0.11727426946163177\n",
            "I0420 22:39:48.385819 139904526645120 model_training_utils.py:450] Train Step: 21457/21936  / loss = 0.3213914632797241\n",
            "I0420 22:39:48.925275 139904526645120 model_training_utils.py:450] Train Step: 21458/21936  / loss = 1.1612086296081543\n",
            "I0420 22:39:49.463489 139904526645120 model_training_utils.py:450] Train Step: 21459/21936  / loss = 0.5859501361846924\n",
            "I0420 22:39:50.007969 139904526645120 model_training_utils.py:450] Train Step: 21460/21936  / loss = 0.2703433036804199\n",
            "I0420 22:39:50.546690 139904526645120 model_training_utils.py:450] Train Step: 21461/21936  / loss = 0.14297530055046082\n",
            "I0420 22:39:51.083950 139904526645120 model_training_utils.py:450] Train Step: 21462/21936  / loss = 0.08925645798444748\n",
            "I0420 22:39:51.621895 139904526645120 model_training_utils.py:450] Train Step: 21463/21936  / loss = 0.21412000060081482\n",
            "I0420 22:39:52.156658 139904526645120 model_training_utils.py:450] Train Step: 21464/21936  / loss = 0.8981016874313354\n",
            "I0420 22:39:52.693647 139904526645120 model_training_utils.py:450] Train Step: 21465/21936  / loss = 1.065605878829956\n",
            "I0420 22:39:53.232231 139904526645120 model_training_utils.py:450] Train Step: 21466/21936  / loss = 0.3843806982040405\n",
            "I0420 22:39:53.765865 139904526645120 model_training_utils.py:450] Train Step: 21467/21936  / loss = 1.2400492429733276\n",
            "I0420 22:39:54.300324 139904526645120 model_training_utils.py:450] Train Step: 21468/21936  / loss = 0.5242583751678467\n",
            "I0420 22:39:54.838113 139904526645120 keras_utils.py:122] TimeHistory: 26.91 seconds, 14.86 examples/second between steps 32387 and 32437\n",
            "I0420 22:39:54.840733 139904526645120 model_training_utils.py:450] Train Step: 21469/21936  / loss = 0.5049817562103271\n",
            "I0420 22:39:55.376784 139904526645120 model_training_utils.py:450] Train Step: 21470/21936  / loss = 0.7408542633056641\n",
            "I0420 22:39:55.911627 139904526645120 model_training_utils.py:450] Train Step: 21471/21936  / loss = 0.8119101524353027\n",
            "I0420 22:39:56.448540 139904526645120 model_training_utils.py:450] Train Step: 21472/21936  / loss = 0.6496560573577881\n",
            "I0420 22:39:56.986298 139904526645120 model_training_utils.py:450] Train Step: 21473/21936  / loss = 0.11462472379207611\n",
            "I0420 22:39:57.522691 139904526645120 model_training_utils.py:450] Train Step: 21474/21936  / loss = 0.38061410188674927\n",
            "I0420 22:39:58.059772 139904526645120 model_training_utils.py:450] Train Step: 21475/21936  / loss = 0.18756374716758728\n",
            "I0420 22:39:58.596855 139904526645120 model_training_utils.py:450] Train Step: 21476/21936  / loss = 0.7578124403953552\n",
            "I0420 22:39:59.135022 139904526645120 model_training_utils.py:450] Train Step: 21477/21936  / loss = 0.9797828197479248\n",
            "I0420 22:39:59.670777 139904526645120 model_training_utils.py:450] Train Step: 21478/21936  / loss = 1.1039854288101196\n",
            "I0420 22:40:00.207114 139904526645120 model_training_utils.py:450] Train Step: 21479/21936  / loss = 0.32411137223243713\n",
            "I0420 22:40:00.744190 139904526645120 model_training_utils.py:450] Train Step: 21480/21936  / loss = 1.2443745136260986\n",
            "I0420 22:40:01.279169 139904526645120 model_training_utils.py:450] Train Step: 21481/21936  / loss = 0.68843674659729\n",
            "I0420 22:40:01.816267 139904526645120 model_training_utils.py:450] Train Step: 21482/21936  / loss = 0.19293458759784698\n",
            "I0420 22:40:02.354523 139904526645120 model_training_utils.py:450] Train Step: 21483/21936  / loss = 0.5254449248313904\n",
            "I0420 22:40:02.890177 139904526645120 model_training_utils.py:450] Train Step: 21484/21936  / loss = 0.4759942889213562\n",
            "I0420 22:40:03.434660 139904526645120 model_training_utils.py:450] Train Step: 21485/21936  / loss = 0.6295620203018188\n",
            "I0420 22:40:03.968524 139904526645120 model_training_utils.py:450] Train Step: 21486/21936  / loss = 0.99691241979599\n",
            "I0420 22:40:04.502128 139904526645120 model_training_utils.py:450] Train Step: 21487/21936  / loss = 0.7151963710784912\n",
            "I0420 22:40:05.040029 139904526645120 model_training_utils.py:450] Train Step: 21488/21936  / loss = 0.6765362024307251\n",
            "I0420 22:40:05.579259 139904526645120 model_training_utils.py:450] Train Step: 21489/21936  / loss = 0.06739024072885513\n",
            "I0420 22:40:06.116267 139904526645120 model_training_utils.py:450] Train Step: 21490/21936  / loss = 0.34530651569366455\n",
            "I0420 22:40:06.651969 139904526645120 model_training_utils.py:450] Train Step: 21491/21936  / loss = 0.2796960771083832\n",
            "I0420 22:40:07.188359 139904526645120 model_training_utils.py:450] Train Step: 21492/21936  / loss = 0.4105168879032135\n",
            "I0420 22:40:07.724275 139904526645120 model_training_utils.py:450] Train Step: 21493/21936  / loss = 0.09530439972877502\n",
            "I0420 22:40:08.262552 139904526645120 model_training_utils.py:450] Train Step: 21494/21936  / loss = 0.8672473430633545\n",
            "I0420 22:40:08.799688 139904526645120 model_training_utils.py:450] Train Step: 21495/21936  / loss = 0.13066427409648895\n",
            "I0420 22:40:09.337137 139904526645120 model_training_utils.py:450] Train Step: 21496/21936  / loss = 0.24877718091011047\n",
            "I0420 22:40:09.874296 139904526645120 model_training_utils.py:450] Train Step: 21497/21936  / loss = 0.4902697205543518\n",
            "I0420 22:40:10.408436 139904526645120 model_training_utils.py:450] Train Step: 21498/21936  / loss = 1.1550567150115967\n",
            "I0420 22:40:10.944469 139904526645120 model_training_utils.py:450] Train Step: 21499/21936  / loss = 0.7317920327186584\n",
            "I0420 22:40:11.482955 139904526645120 model_training_utils.py:450] Train Step: 21500/21936  / loss = 0.39334458112716675\n",
            "I0420 22:40:12.020843 139904526645120 model_training_utils.py:450] Train Step: 21501/21936  / loss = 0.38888266682624817\n",
            "I0420 22:40:12.561322 139904526645120 model_training_utils.py:450] Train Step: 21502/21936  / loss = 0.4867628216743469\n",
            "I0420 22:40:13.099223 139904526645120 model_training_utils.py:450] Train Step: 21503/21936  / loss = 0.9489036202430725\n",
            "I0420 22:40:13.637286 139904526645120 model_training_utils.py:450] Train Step: 21504/21936  / loss = 0.7346560955047607\n",
            "I0420 22:40:14.174709 139904526645120 model_training_utils.py:450] Train Step: 21505/21936  / loss = 1.014481544494629\n",
            "I0420 22:40:14.713986 139904526645120 model_training_utils.py:450] Train Step: 21506/21936  / loss = 0.23349446058273315\n",
            "I0420 22:40:15.252197 139904526645120 model_training_utils.py:450] Train Step: 21507/21936  / loss = 0.8624982237815857\n",
            "I0420 22:40:15.787786 139904526645120 model_training_utils.py:450] Train Step: 21508/21936  / loss = 0.3140506148338318\n",
            "I0420 22:40:16.322782 139904526645120 model_training_utils.py:450] Train Step: 21509/21936  / loss = 0.07501735538244247\n",
            "I0420 22:40:16.859685 139904526645120 model_training_utils.py:450] Train Step: 21510/21936  / loss = 0.1617792546749115\n",
            "I0420 22:40:17.396270 139904526645120 model_training_utils.py:450] Train Step: 21511/21936  / loss = 0.033218950033187866\n",
            "I0420 22:40:17.933182 139904526645120 model_training_utils.py:450] Train Step: 21512/21936  / loss = 1.0642197132110596\n",
            "I0420 22:40:18.469909 139904526645120 model_training_utils.py:450] Train Step: 21513/21936  / loss = 0.6532958149909973\n",
            "I0420 22:40:19.003451 139904526645120 model_training_utils.py:450] Train Step: 21514/21936  / loss = 0.9731948971748352\n",
            "I0420 22:40:19.539062 139904526645120 model_training_utils.py:450] Train Step: 21515/21936  / loss = 0.3527717888355255\n",
            "I0420 22:40:20.076071 139904526645120 model_training_utils.py:450] Train Step: 21516/21936  / loss = 0.3335976004600525\n",
            "I0420 22:40:20.611898 139904526645120 model_training_utils.py:450] Train Step: 21517/21936  / loss = 0.2732866406440735\n",
            "I0420 22:40:21.147583 139904526645120 model_training_utils.py:450] Train Step: 21518/21936  / loss = 0.6334782242774963\n",
            "I0420 22:40:21.683018 139904526645120 keras_utils.py:122] TimeHistory: 26.84 seconds, 14.90 examples/second between steps 32437 and 32487\n",
            "I0420 22:40:21.685636 139904526645120 model_training_utils.py:450] Train Step: 21519/21936  / loss = 0.4667559266090393\n",
            "I0420 22:40:22.220461 139904526645120 model_training_utils.py:450] Train Step: 21520/21936  / loss = 0.31576019525527954\n",
            "I0420 22:40:22.755253 139904526645120 model_training_utils.py:450] Train Step: 21521/21936  / loss = 0.11505542695522308\n",
            "I0420 22:40:23.291302 139904526645120 model_training_utils.py:450] Train Step: 21522/21936  / loss = 0.28832536935806274\n",
            "I0420 22:40:23.828622 139904526645120 model_training_utils.py:450] Train Step: 21523/21936  / loss = 0.17806534469127655\n",
            "I0420 22:40:24.363699 139904526645120 model_training_utils.py:450] Train Step: 21524/21936  / loss = 0.5843730568885803\n",
            "I0420 22:40:24.902007 139904526645120 model_training_utils.py:450] Train Step: 21525/21936  / loss = 0.49579447507858276\n",
            "I0420 22:40:25.438260 139904526645120 model_training_utils.py:450] Train Step: 21526/21936  / loss = 0.45443195104599\n",
            "I0420 22:40:25.974406 139904526645120 model_training_utils.py:450] Train Step: 21527/21936  / loss = 0.3771626949310303\n",
            "I0420 22:40:26.509963 139904526645120 model_training_utils.py:450] Train Step: 21528/21936  / loss = 0.4785597324371338\n",
            "I0420 22:40:27.044398 139904526645120 model_training_utils.py:450] Train Step: 21529/21936  / loss = 0.5202714800834656\n",
            "I0420 22:40:27.579976 139904526645120 model_training_utils.py:450] Train Step: 21530/21936  / loss = 0.1957005113363266\n",
            "I0420 22:40:28.113187 139904526645120 model_training_utils.py:450] Train Step: 21531/21936  / loss = 0.6265313029289246\n",
            "I0420 22:40:28.649522 139904526645120 model_training_utils.py:450] Train Step: 21532/21936  / loss = 0.2830031216144562\n",
            "I0420 22:40:29.184858 139904526645120 model_training_utils.py:450] Train Step: 21533/21936  / loss = 0.36488068103790283\n",
            "I0420 22:40:29.723133 139904526645120 model_training_utils.py:450] Train Step: 21534/21936  / loss = 0.19019560515880585\n",
            "I0420 22:40:30.258412 139904526645120 model_training_utils.py:450] Train Step: 21535/21936  / loss = 0.03435548022389412\n",
            "I0420 22:40:30.795855 139904526645120 model_training_utils.py:450] Train Step: 21536/21936  / loss = 0.741203784942627\n",
            "I0420 22:40:31.342707 139904526645120 model_training_utils.py:450] Train Step: 21537/21936  / loss = 0.48536956310272217\n",
            "I0420 22:40:31.879422 139904526645120 model_training_utils.py:450] Train Step: 21538/21936  / loss = 0.3594815135002136\n",
            "I0420 22:40:32.416428 139904526645120 model_training_utils.py:450] Train Step: 21539/21936  / loss = 0.291684091091156\n",
            "I0420 22:40:32.953758 139904526645120 model_training_utils.py:450] Train Step: 21540/21936  / loss = 0.20021697878837585\n",
            "I0420 22:40:33.489261 139904526645120 model_training_utils.py:450] Train Step: 21541/21936  / loss = 0.4069284498691559\n",
            "I0420 22:40:34.025769 139904526645120 model_training_utils.py:450] Train Step: 21542/21936  / loss = 0.3370533883571625\n",
            "I0420 22:40:34.561193 139904526645120 model_training_utils.py:450] Train Step: 21543/21936  / loss = 0.333900511264801\n",
            "I0420 22:40:35.099646 139904526645120 model_training_utils.py:450] Train Step: 21544/21936  / loss = 0.2324422001838684\n",
            "I0420 22:40:35.636899 139904526645120 model_training_utils.py:450] Train Step: 21545/21936  / loss = 0.25369030237197876\n",
            "I0420 22:40:36.173645 139904526645120 model_training_utils.py:450] Train Step: 21546/21936  / loss = 0.5293262004852295\n",
            "I0420 22:40:36.707641 139904526645120 model_training_utils.py:450] Train Step: 21547/21936  / loss = 0.11394409835338593\n",
            "I0420 22:40:37.242605 139904526645120 model_training_utils.py:450] Train Step: 21548/21936  / loss = 0.41762444376945496\n",
            "I0420 22:40:37.780680 139904526645120 model_training_utils.py:450] Train Step: 21549/21936  / loss = 0.7609184384346008\n",
            "I0420 22:40:38.317460 139904526645120 model_training_utils.py:450] Train Step: 21550/21936  / loss = 0.26865699887275696\n",
            "I0420 22:40:38.856594 139904526645120 model_training_utils.py:450] Train Step: 21551/21936  / loss = 0.20063716173171997\n",
            "I0420 22:40:39.394412 139904526645120 model_training_utils.py:450] Train Step: 21552/21936  / loss = 0.21648792922496796\n",
            "I0420 22:40:39.931119 139904526645120 model_training_utils.py:450] Train Step: 21553/21936  / loss = 0.5190690159797668\n",
            "I0420 22:40:40.466807 139904526645120 model_training_utils.py:450] Train Step: 21554/21936  / loss = 0.6783942580223083\n",
            "I0420 22:40:41.003630 139904526645120 model_training_utils.py:450] Train Step: 21555/21936  / loss = 0.3470572233200073\n",
            "I0420 22:40:41.540888 139904526645120 model_training_utils.py:450] Train Step: 21556/21936  / loss = 0.2923477590084076\n",
            "I0420 22:40:42.078695 139904526645120 model_training_utils.py:450] Train Step: 21557/21936  / loss = 0.17253026366233826\n",
            "I0420 22:40:42.617731 139904526645120 model_training_utils.py:450] Train Step: 21558/21936  / loss = 0.46540433168411255\n",
            "I0420 22:40:43.159250 139904526645120 model_training_utils.py:450] Train Step: 21559/21936  / loss = 0.24604317545890808\n",
            "I0420 22:40:43.697317 139904526645120 model_training_utils.py:450] Train Step: 21560/21936  / loss = 0.2947372794151306\n",
            "I0420 22:40:44.234290 139904526645120 model_training_utils.py:450] Train Step: 21561/21936  / loss = 0.35615232586860657\n",
            "I0420 22:40:44.771803 139904526645120 model_training_utils.py:450] Train Step: 21562/21936  / loss = 0.26034584641456604\n",
            "I0420 22:40:45.309599 139904526645120 model_training_utils.py:450] Train Step: 21563/21936  / loss = 0.7953721284866333\n",
            "I0420 22:40:45.845764 139904526645120 model_training_utils.py:450] Train Step: 21564/21936  / loss = 0.4319465756416321\n",
            "I0420 22:40:46.383335 139904526645120 model_training_utils.py:450] Train Step: 21565/21936  / loss = 0.14240595698356628\n",
            "I0420 22:40:46.921625 139904526645120 model_training_utils.py:450] Train Step: 21566/21936  / loss = 0.1871439516544342\n",
            "I0420 22:40:47.459449 139904526645120 model_training_utils.py:450] Train Step: 21567/21936  / loss = 0.13649067282676697\n",
            "I0420 22:40:47.999002 139904526645120 model_training_utils.py:450] Train Step: 21568/21936  / loss = 0.15717865526676178\n",
            "I0420 22:40:48.534389 139904526645120 keras_utils.py:122] TimeHistory: 26.85 seconds, 14.90 examples/second between steps 32487 and 32537\n",
            "I0420 22:40:48.537027 139904526645120 model_training_utils.py:450] Train Step: 21569/21936  / loss = 0.26537761092185974\n",
            "I0420 22:40:49.074841 139904526645120 model_training_utils.py:450] Train Step: 21570/21936  / loss = 0.29451602697372437\n",
            "I0420 22:40:49.610163 139904526645120 model_training_utils.py:450] Train Step: 21571/21936  / loss = 0.21199508011341095\n",
            "I0420 22:40:50.147714 139904526645120 model_training_utils.py:450] Train Step: 21572/21936  / loss = 0.04332952946424484\n",
            "I0420 22:40:50.684499 139904526645120 model_training_utils.py:450] Train Step: 21573/21936  / loss = 0.3188770115375519\n",
            "I0420 22:40:51.221792 139904526645120 model_training_utils.py:450] Train Step: 21574/21936  / loss = 0.33438947796821594\n",
            "I0420 22:40:51.759157 139904526645120 model_training_utils.py:450] Train Step: 21575/21936  / loss = 0.3610694408416748\n",
            "I0420 22:40:52.297794 139904526645120 model_training_utils.py:450] Train Step: 21576/21936  / loss = 0.09058058261871338\n",
            "I0420 22:40:52.839019 139904526645120 model_training_utils.py:450] Train Step: 21577/21936  / loss = 0.09213998913764954\n",
            "I0420 22:40:53.374704 139904526645120 model_training_utils.py:450] Train Step: 21578/21936  / loss = 0.21760712563991547\n",
            "I0420 22:40:53.911028 139904526645120 model_training_utils.py:450] Train Step: 21579/21936  / loss = 1.2480943202972412\n",
            "I0420 22:40:54.449481 139904526645120 model_training_utils.py:450] Train Step: 21580/21936  / loss = 0.3146510720252991\n",
            "I0420 22:40:54.984779 139904526645120 model_training_utils.py:450] Train Step: 21581/21936  / loss = 0.435043066740036\n",
            "I0420 22:40:55.518993 139904526645120 model_training_utils.py:450] Train Step: 21582/21936  / loss = 0.5849169492721558\n",
            "I0420 22:40:56.054393 139904526645120 model_training_utils.py:450] Train Step: 21583/21936  / loss = 0.4265103042125702\n",
            "I0420 22:40:56.590166 139904526645120 model_training_utils.py:450] Train Step: 21584/21936  / loss = 0.2389964461326599\n",
            "I0420 22:40:57.124354 139904526645120 model_training_utils.py:450] Train Step: 21585/21936  / loss = 0.10199891030788422\n",
            "I0420 22:40:57.659834 139904526645120 model_training_utils.py:450] Train Step: 21586/21936  / loss = 0.10809497535228729\n",
            "I0420 22:40:58.196233 139904526645120 model_training_utils.py:450] Train Step: 21587/21936  / loss = 0.048400066792964935\n",
            "I0420 22:40:58.731812 139904526645120 model_training_utils.py:450] Train Step: 21588/21936  / loss = 2.354806661605835\n",
            "I0420 22:40:59.268866 139904526645120 model_training_utils.py:450] Train Step: 21589/21936  / loss = 0.07373948395252228\n",
            "I0420 22:40:59.805638 139904526645120 model_training_utils.py:450] Train Step: 21590/21936  / loss = 0.2241564244031906\n",
            "I0420 22:41:00.340960 139904526645120 model_training_utils.py:450] Train Step: 21591/21936  / loss = 0.596238374710083\n",
            "I0420 22:41:00.878686 139904526645120 model_training_utils.py:450] Train Step: 21592/21936  / loss = 0.2785313129425049\n",
            "I0420 22:41:01.417372 139904526645120 model_training_utils.py:450] Train Step: 21593/21936  / loss = 0.4558667242527008\n",
            "I0420 22:41:01.953880 139904526645120 model_training_utils.py:450] Train Step: 21594/21936  / loss = 0.5205946564674377\n",
            "I0420 22:41:02.491289 139904526645120 model_training_utils.py:450] Train Step: 21595/21936  / loss = 0.38078412413597107\n",
            "I0420 22:41:03.032484 139904526645120 model_training_utils.py:450] Train Step: 21596/21936  / loss = 0.23580136895179749\n",
            "I0420 22:41:03.566687 139904526645120 model_training_utils.py:450] Train Step: 21597/21936  / loss = 0.1545415222644806\n",
            "I0420 22:41:04.103606 139904526645120 model_training_utils.py:450] Train Step: 21598/21936  / loss = 0.20475201308727264\n",
            "I0420 22:41:04.639286 139904526645120 model_training_utils.py:450] Train Step: 21599/21936  / loss = 0.038830772042274475\n",
            "I0420 22:41:05.174199 139904526645120 model_training_utils.py:450] Train Step: 21600/21936  / loss = 0.19429849088191986\n",
            "I0420 22:41:05.712585 139904526645120 model_training_utils.py:450] Train Step: 21601/21936  / loss = 0.7105339169502258\n",
            "I0420 22:41:06.249696 139904526645120 model_training_utils.py:450] Train Step: 21602/21936  / loss = 0.3583274781703949\n",
            "I0420 22:41:06.794611 139904526645120 model_training_utils.py:450] Train Step: 21603/21936  / loss = 0.24483515322208405\n",
            "I0420 22:41:07.331043 139904526645120 model_training_utils.py:450] Train Step: 21604/21936  / loss = 0.2706868350505829\n",
            "I0420 22:41:07.869360 139904526645120 model_training_utils.py:450] Train Step: 21605/21936  / loss = 0.5579617023468018\n",
            "I0420 22:41:08.409706 139904526645120 model_training_utils.py:450] Train Step: 21606/21936  / loss = 0.12483161687850952\n",
            "I0420 22:41:08.951478 139904526645120 model_training_utils.py:450] Train Step: 21607/21936  / loss = 0.5283940434455872\n",
            "I0420 22:41:09.490255 139904526645120 model_training_utils.py:450] Train Step: 21608/21936  / loss = 0.16165675222873688\n",
            "I0420 22:41:10.027461 139904526645120 model_training_utils.py:450] Train Step: 21609/21936  / loss = 0.39031732082366943\n",
            "I0420 22:41:10.564623 139904526645120 model_training_utils.py:450] Train Step: 21610/21936  / loss = 0.24441921710968018\n",
            "I0420 22:41:11.101734 139904526645120 model_training_utils.py:450] Train Step: 21611/21936  / loss = 0.2280198037624359\n",
            "I0420 22:41:11.638529 139904526645120 model_training_utils.py:450] Train Step: 21612/21936  / loss = 0.5226054191589355\n",
            "I0420 22:41:12.178298 139904526645120 model_training_utils.py:450] Train Step: 21613/21936  / loss = 0.3187789022922516\n",
            "I0420 22:41:12.714380 139904526645120 model_training_utils.py:450] Train Step: 21614/21936  / loss = 0.38509857654571533\n",
            "I0420 22:41:13.251559 139904526645120 model_training_utils.py:450] Train Step: 21615/21936  / loss = 0.22083966434001923\n",
            "I0420 22:41:13.790693 139904526645120 model_training_utils.py:450] Train Step: 21616/21936  / loss = 0.23559245467185974\n",
            "I0420 22:41:14.325250 139904526645120 model_training_utils.py:450] Train Step: 21617/21936  / loss = 0.14525912702083588\n",
            "I0420 22:41:14.865692 139904526645120 model_training_utils.py:450] Train Step: 21618/21936  / loss = 0.24107098579406738\n",
            "I0420 22:41:15.409804 139904526645120 keras_utils.py:122] TimeHistory: 26.87 seconds, 14.89 examples/second between steps 32537 and 32587\n",
            "I0420 22:41:15.412537 139904526645120 model_training_utils.py:450] Train Step: 21619/21936  / loss = 0.1331714689731598\n",
            "I0420 22:41:15.951606 139904526645120 model_training_utils.py:450] Train Step: 21620/21936  / loss = 0.13991738855838776\n",
            "I0420 22:41:16.487695 139904526645120 model_training_utils.py:450] Train Step: 21621/21936  / loss = 1.8890130519866943\n",
            "I0420 22:41:17.024175 139904526645120 model_training_utils.py:450] Train Step: 21622/21936  / loss = 0.8183620572090149\n",
            "I0420 22:41:17.560542 139904526645120 model_training_utils.py:450] Train Step: 21623/21936  / loss = 0.3601391017436981\n",
            "I0420 22:41:18.095626 139904526645120 model_training_utils.py:450] Train Step: 21624/21936  / loss = 1.439011812210083\n",
            "I0420 22:41:18.633294 139904526645120 model_training_utils.py:450] Train Step: 21625/21936  / loss = 1.6202480792999268\n",
            "I0420 22:41:19.172672 139904526645120 model_training_utils.py:450] Train Step: 21626/21936  / loss = 1.020979642868042\n",
            "I0420 22:41:19.711001 139904526645120 model_training_utils.py:450] Train Step: 21627/21936  / loss = 0.6876271963119507\n",
            "I0420 22:41:20.248381 139904526645120 model_training_utils.py:450] Train Step: 21628/21936  / loss = 0.46374374628067017\n",
            "I0420 22:41:20.785150 139904526645120 model_training_utils.py:450] Train Step: 21629/21936  / loss = 0.6556035280227661\n",
            "I0420 22:41:21.326961 139904526645120 model_training_utils.py:450] Train Step: 21630/21936  / loss = 0.3813137114048004\n",
            "I0420 22:41:21.863838 139904526645120 model_training_utils.py:450] Train Step: 21631/21936  / loss = 0.5627114772796631\n",
            "I0420 22:41:22.399855 139904526645120 model_training_utils.py:450] Train Step: 21632/21936  / loss = 0.35078758001327515\n",
            "I0420 22:41:22.937198 139904526645120 model_training_utils.py:450] Train Step: 21633/21936  / loss = 0.5831235647201538\n",
            "I0420 22:41:23.473385 139904526645120 model_training_utils.py:450] Train Step: 21634/21936  / loss = 0.08929969370365143\n",
            "I0420 22:41:24.008602 139904526645120 model_training_utils.py:450] Train Step: 21635/21936  / loss = 0.5476292371749878\n",
            "I0420 22:41:24.543403 139904526645120 model_training_utils.py:450] Train Step: 21636/21936  / loss = 0.9144965410232544\n",
            "I0420 22:41:25.079617 139904526645120 model_training_utils.py:450] Train Step: 21637/21936  / loss = 0.7480727434158325\n",
            "I0420 22:41:25.617408 139904526645120 model_training_utils.py:450] Train Step: 21638/21936  / loss = 0.5502071976661682\n",
            "I0420 22:41:26.155206 139904526645120 model_training_utils.py:450] Train Step: 21639/21936  / loss = 0.9326217174530029\n",
            "I0420 22:41:26.694542 139904526645120 model_training_utils.py:450] Train Step: 21640/21936  / loss = 1.019299864768982\n",
            "I0420 22:41:27.233261 139904526645120 model_training_utils.py:450] Train Step: 21641/21936  / loss = 0.6365674734115601\n",
            "I0420 22:41:27.771076 139904526645120 model_training_utils.py:450] Train Step: 21642/21936  / loss = 0.20107907056808472\n",
            "I0420 22:41:28.310067 139904526645120 model_training_utils.py:450] Train Step: 21643/21936  / loss = 0.5542570948600769\n",
            "I0420 22:41:28.847126 139904526645120 model_training_utils.py:450] Train Step: 21644/21936  / loss = 0.3207845687866211\n",
            "I0420 22:41:29.384938 139904526645120 model_training_utils.py:450] Train Step: 21645/21936  / loss = 1.0208625793457031\n",
            "I0420 22:41:29.924913 139904526645120 model_training_utils.py:450] Train Step: 21646/21936  / loss = 0.8953129053115845\n",
            "I0420 22:41:30.468867 139904526645120 model_training_utils.py:450] Train Step: 21647/21936  / loss = 0.31516286730766296\n",
            "I0420 22:41:31.006413 139904526645120 model_training_utils.py:450] Train Step: 21648/21936  / loss = 0.16898411512374878\n",
            "I0420 22:41:31.543840 139904526645120 model_training_utils.py:450] Train Step: 21649/21936  / loss = 0.7810320258140564\n",
            "I0420 22:41:32.080874 139904526645120 model_training_utils.py:450] Train Step: 21650/21936  / loss = 0.04374724626541138\n",
            "I0420 22:41:32.618201 139904526645120 model_training_utils.py:450] Train Step: 21651/21936  / loss = 0.3866334557533264\n",
            "I0420 22:41:33.154498 139904526645120 model_training_utils.py:450] Train Step: 21652/21936  / loss = 0.5455545783042908\n",
            "I0420 22:41:33.693125 139904526645120 model_training_utils.py:450] Train Step: 21653/21936  / loss = 0.05033166706562042\n",
            "I0420 22:41:34.230412 139904526645120 model_training_utils.py:450] Train Step: 21654/21936  / loss = 0.007896285504102707\n",
            "I0420 22:41:34.767124 139904526645120 model_training_utils.py:450] Train Step: 21655/21936  / loss = 0.9074080586433411\n",
            "I0420 22:41:35.303384 139904526645120 model_training_utils.py:450] Train Step: 21656/21936  / loss = 0.2705259323120117\n",
            "I0420 22:41:35.845817 139904526645120 model_training_utils.py:450] Train Step: 21657/21936  / loss = 0.70403653383255\n",
            "I0420 22:41:36.386520 139904526645120 model_training_utils.py:450] Train Step: 21658/21936  / loss = 0.7468039393424988\n",
            "I0420 22:41:36.928925 139904526645120 model_training_utils.py:450] Train Step: 21659/21936  / loss = 0.20323896408081055\n",
            "I0420 22:41:37.467231 139904526645120 model_training_utils.py:450] Train Step: 21660/21936  / loss = 0.15391108393669128\n",
            "I0420 22:41:38.004222 139904526645120 model_training_utils.py:450] Train Step: 21661/21936  / loss = 0.5929059386253357\n",
            "I0420 22:41:38.545386 139904526645120 model_training_utils.py:450] Train Step: 21662/21936  / loss = 0.5154944658279419\n",
            "I0420 22:41:39.084766 139904526645120 model_training_utils.py:450] Train Step: 21663/21936  / loss = 0.36329007148742676\n",
            "I0420 22:41:39.621065 139904526645120 model_training_utils.py:450] Train Step: 21664/21936  / loss = 0.14420390129089355\n",
            "I0420 22:41:40.161139 139904526645120 model_training_utils.py:450] Train Step: 21665/21936  / loss = 0.17555448412895203\n",
            "I0420 22:41:40.699583 139904526645120 model_training_utils.py:450] Train Step: 21666/21936  / loss = 0.34070339798927307\n",
            "I0420 22:41:41.238692 139904526645120 model_training_utils.py:450] Train Step: 21667/21936  / loss = 0.5542058944702148\n",
            "I0420 22:41:41.779091 139904526645120 model_training_utils.py:450] Train Step: 21668/21936  / loss = 0.20675072073936462\n",
            "I0420 22:41:42.318774 139904526645120 keras_utils.py:122] TimeHistory: 26.91 seconds, 14.87 examples/second between steps 32587 and 32637\n",
            "I0420 22:41:42.321474 139904526645120 model_training_utils.py:450] Train Step: 21669/21936  / loss = 0.15625080466270447\n",
            "I0420 22:41:42.857708 139904526645120 model_training_utils.py:450] Train Step: 21670/21936  / loss = 0.11623898148536682\n",
            "I0420 22:41:43.398051 139904526645120 model_training_utils.py:450] Train Step: 21671/21936  / loss = 0.36936476826667786\n",
            "I0420 22:41:43.938110 139904526645120 model_training_utils.py:450] Train Step: 21672/21936  / loss = 0.3106149137020111\n",
            "I0420 22:41:44.476323 139904526645120 model_training_utils.py:450] Train Step: 21673/21936  / loss = 0.5973171591758728\n",
            "I0420 22:41:45.013374 139904526645120 model_training_utils.py:450] Train Step: 21674/21936  / loss = 0.23240084946155548\n",
            "I0420 22:41:45.551034 139904526645120 model_training_utils.py:450] Train Step: 21675/21936  / loss = 0.03468690812587738\n",
            "I0420 22:41:46.088489 139904526645120 model_training_utils.py:450] Train Step: 21676/21936  / loss = 0.227458193898201\n",
            "I0420 22:41:46.629213 139904526645120 model_training_utils.py:450] Train Step: 21677/21936  / loss = 0.13261835277080536\n",
            "I0420 22:41:47.176794 139904526645120 model_training_utils.py:450] Train Step: 21678/21936  / loss = 0.06516348570585251\n",
            "I0420 22:41:47.715124 139904526645120 model_training_utils.py:450] Train Step: 21679/21936  / loss = 0.37806177139282227\n",
            "I0420 22:41:48.252254 139904526645120 model_training_utils.py:450] Train Step: 21680/21936  / loss = 0.4962717294692993\n",
            "I0420 22:41:48.789948 139904526645120 model_training_utils.py:450] Train Step: 21681/21936  / loss = 0.19310200214385986\n",
            "I0420 22:41:49.327536 139904526645120 model_training_utils.py:450] Train Step: 21682/21936  / loss = 0.31558555364608765\n",
            "I0420 22:41:49.865950 139904526645120 model_training_utils.py:450] Train Step: 21683/21936  / loss = 0.4150503873825073\n",
            "I0420 22:41:50.402090 139904526645120 model_training_utils.py:450] Train Step: 21684/21936  / loss = 0.3224903643131256\n",
            "I0420 22:41:50.939031 139904526645120 model_training_utils.py:450] Train Step: 21685/21936  / loss = 0.14990754425525665\n",
            "I0420 22:41:51.478442 139904526645120 model_training_utils.py:450] Train Step: 21686/21936  / loss = 0.11345309764146805\n",
            "I0420 22:41:52.017913 139904526645120 model_training_utils.py:450] Train Step: 21687/21936  / loss = 0.16472093760967255\n",
            "I0420 22:41:52.556990 139904526645120 model_training_utils.py:450] Train Step: 21688/21936  / loss = 0.018378742039203644\n",
            "I0420 22:41:53.096044 139904526645120 model_training_utils.py:450] Train Step: 21689/21936  / loss = 0.6716617345809937\n",
            "I0420 22:41:53.635219 139904526645120 model_training_utils.py:450] Train Step: 21690/21936  / loss = 0.4845603108406067\n",
            "I0420 22:41:54.173966 139904526645120 model_training_utils.py:450] Train Step: 21691/21936  / loss = 0.6599359512329102\n",
            "I0420 22:41:54.712891 139904526645120 model_training_utils.py:450] Train Step: 21692/21936  / loss = 0.00998564250767231\n",
            "I0420 22:41:55.248437 139904526645120 model_training_utils.py:450] Train Step: 21693/21936  / loss = 0.45840391516685486\n",
            "I0420 22:41:55.786960 139904526645120 model_training_utils.py:450] Train Step: 21694/21936  / loss = 0.20858711004257202\n",
            "I0420 22:41:56.325609 139904526645120 model_training_utils.py:450] Train Step: 21695/21936  / loss = 0.2512620985507965\n",
            "I0420 22:41:56.863010 139904526645120 model_training_utils.py:450] Train Step: 21696/21936  / loss = 0.05155246704816818\n",
            "I0420 22:41:57.401714 139904526645120 model_training_utils.py:450] Train Step: 21697/21936  / loss = 0.14648158848285675\n",
            "I0420 22:41:57.937669 139904526645120 model_training_utils.py:450] Train Step: 21698/21936  / loss = 0.8904557824134827\n",
            "I0420 22:41:58.475351 139904526645120 model_training_utils.py:450] Train Step: 21699/21936  / loss = 0.12413527816534042\n",
            "I0420 22:41:59.014010 139904526645120 model_training_utils.py:450] Train Step: 21700/21936  / loss = 0.03987610340118408\n",
            "I0420 22:41:59.552653 139904526645120 model_training_utils.py:450] Train Step: 21701/21936  / loss = 0.2258583903312683\n",
            "I0420 22:42:00.089785 139904526645120 model_training_utils.py:450] Train Step: 21702/21936  / loss = 0.736567497253418\n",
            "I0420 22:42:00.625993 139904526645120 model_training_utils.py:450] Train Step: 21703/21936  / loss = 0.060100261121988297\n",
            "I0420 22:42:01.166247 139904526645120 model_training_utils.py:450] Train Step: 21704/21936  / loss = 0.18454836308956146\n",
            "I0420 22:42:01.703771 139904526645120 model_training_utils.py:450] Train Step: 21705/21936  / loss = 0.2138572484254837\n",
            "I0420 22:42:02.244503 139904526645120 model_training_utils.py:450] Train Step: 21706/21936  / loss = 0.15465670824050903\n",
            "I0420 22:42:02.783324 139904526645120 model_training_utils.py:450] Train Step: 21707/21936  / loss = 0.11728785932064056\n",
            "I0420 22:42:03.322273 139904526645120 model_training_utils.py:450] Train Step: 21708/21936  / loss = 0.3375178575515747\n",
            "I0420 22:42:03.858131 139904526645120 model_training_utils.py:450] Train Step: 21709/21936  / loss = 0.01336086355149746\n",
            "I0420 22:42:04.392173 139904526645120 model_training_utils.py:450] Train Step: 21710/21936  / loss = 0.36055830121040344\n",
            "I0420 22:42:04.930631 139904526645120 model_training_utils.py:450] Train Step: 21711/21936  / loss = 0.021320637315511703\n",
            "I0420 22:42:05.465239 139904526645120 model_training_utils.py:450] Train Step: 21712/21936  / loss = 0.013501081615686417\n",
            "I0420 22:42:06.003544 139904526645120 model_training_utils.py:450] Train Step: 21713/21936  / loss = 0.17009074985980988\n",
            "I0420 22:42:06.541510 139904526645120 model_training_utils.py:450] Train Step: 21714/21936  / loss = 0.20863576233386993\n",
            "I0420 22:42:07.082825 139904526645120 model_training_utils.py:450] Train Step: 21715/21936  / loss = 0.5641682744026184\n",
            "I0420 22:42:07.621199 139904526645120 model_training_utils.py:450] Train Step: 21716/21936  / loss = 0.1261240541934967\n",
            "I0420 22:42:08.160375 139904526645120 model_training_utils.py:450] Train Step: 21717/21936  / loss = 0.18239682912826538\n",
            "I0420 22:42:08.701598 139904526645120 model_training_utils.py:450] Train Step: 21718/21936  / loss = 0.32828083634376526\n",
            "I0420 22:42:09.239784 139904526645120 keras_utils.py:122] TimeHistory: 26.92 seconds, 14.86 examples/second between steps 32637 and 32687\n",
            "I0420 22:42:09.242736 139904526645120 model_training_utils.py:450] Train Step: 21719/21936  / loss = 0.5732640027999878\n",
            "I0420 22:42:09.778683 139904526645120 model_training_utils.py:450] Train Step: 21720/21936  / loss = 0.18146750330924988\n",
            "I0420 22:42:10.317059 139904526645120 model_training_utils.py:450] Train Step: 21721/21936  / loss = 0.39087796211242676\n",
            "I0420 22:42:10.855423 139904526645120 model_training_utils.py:450] Train Step: 21722/21936  / loss = 0.6951632499694824\n",
            "I0420 22:42:11.391587 139904526645120 model_training_utils.py:450] Train Step: 21723/21936  / loss = 0.14877301454544067\n",
            "I0420 22:42:11.928052 139904526645120 model_training_utils.py:450] Train Step: 21724/21936  / loss = 0.818787693977356\n",
            "I0420 22:42:12.466745 139904526645120 model_training_utils.py:450] Train Step: 21725/21936  / loss = 0.07843051850795746\n",
            "I0420 22:42:13.005372 139904526645120 model_training_utils.py:450] Train Step: 21726/21936  / loss = 0.7573760747909546\n",
            "I0420 22:42:13.544188 139904526645120 model_training_utils.py:450] Train Step: 21727/21936  / loss = 0.47476333379745483\n",
            "I0420 22:42:14.082041 139904526645120 model_training_utils.py:450] Train Step: 21728/21936  / loss = 0.2961547076702118\n",
            "I0420 22:42:14.618792 139904526645120 model_training_utils.py:450] Train Step: 21729/21936  / loss = 0.058684512972831726\n",
            "I0420 22:42:15.155158 139904526645120 model_training_utils.py:450] Train Step: 21730/21936  / loss = 0.653590202331543\n",
            "I0420 22:42:15.693348 139904526645120 model_training_utils.py:450] Train Step: 21731/21936  / loss = 0.6557884216308594\n",
            "I0420 22:42:16.231870 139904526645120 model_training_utils.py:450] Train Step: 21732/21936  / loss = 1.1681063175201416\n",
            "I0420 22:42:16.769527 139904526645120 model_training_utils.py:450] Train Step: 21733/21936  / loss = 0.2064640372991562\n",
            "I0420 22:42:17.306198 139904526645120 model_training_utils.py:450] Train Step: 21734/21936  / loss = 0.1767805516719818\n",
            "I0420 22:42:17.844895 139904526645120 model_training_utils.py:450] Train Step: 21735/21936  / loss = 0.06263606250286102\n",
            "I0420 22:42:18.383350 139904526645120 model_training_utils.py:450] Train Step: 21736/21936  / loss = 0.20123648643493652\n",
            "I0420 22:42:18.922708 139904526645120 model_training_utils.py:450] Train Step: 21737/21936  / loss = 0.14912067353725433\n",
            "I0420 22:42:19.471714 139904526645120 model_training_utils.py:450] Train Step: 21738/21936  / loss = 0.04090031608939171\n",
            "I0420 22:42:20.009705 139904526645120 model_training_utils.py:450] Train Step: 21739/21936  / loss = 0.23772893846035004\n",
            "I0420 22:42:20.549275 139904526645120 model_training_utils.py:450] Train Step: 21740/21936  / loss = 0.9021446108818054\n",
            "I0420 22:42:21.091239 139904526645120 model_training_utils.py:450] Train Step: 21741/21936  / loss = 0.845676064491272\n",
            "I0420 22:42:21.629137 139904526645120 model_training_utils.py:450] Train Step: 21742/21936  / loss = 0.21732206642627716\n",
            "I0420 22:42:22.169010 139904526645120 model_training_utils.py:450] Train Step: 21743/21936  / loss = 0.23737061023712158\n",
            "I0420 22:42:22.709700 139904526645120 model_training_utils.py:450] Train Step: 21744/21936  / loss = 0.12528306245803833\n",
            "I0420 22:42:23.249265 139904526645120 model_training_utils.py:450] Train Step: 21745/21936  / loss = 0.4651643931865692\n",
            "I0420 22:42:23.785382 139904526645120 model_training_utils.py:450] Train Step: 21746/21936  / loss = 0.36484506726264954\n",
            "I0420 22:42:24.325622 139904526645120 model_training_utils.py:450] Train Step: 21747/21936  / loss = 0.4410836696624756\n",
            "I0420 22:42:24.864647 139904526645120 model_training_utils.py:450] Train Step: 21748/21936  / loss = 0.37514567375183105\n",
            "I0420 22:42:25.402338 139904526645120 model_training_utils.py:450] Train Step: 21749/21936  / loss = 1.2839694023132324\n",
            "I0420 22:42:25.953790 139904526645120 model_training_utils.py:450] Train Step: 21750/21936  / loss = 0.22542065382003784\n",
            "I0420 22:42:26.490984 139904526645120 model_training_utils.py:450] Train Step: 21751/21936  / loss = 0.08427392691373825\n",
            "I0420 22:42:27.026587 139904526645120 model_training_utils.py:450] Train Step: 21752/21936  / loss = 0.11331550776958466\n",
            "I0420 22:42:27.564587 139904526645120 model_training_utils.py:450] Train Step: 21753/21936  / loss = 0.12713655829429626\n",
            "I0420 22:42:28.103109 139904526645120 model_training_utils.py:450] Train Step: 21754/21936  / loss = 0.30601784586906433\n",
            "I0420 22:42:28.640487 139904526645120 model_training_utils.py:450] Train Step: 21755/21936  / loss = 0.2618306279182434\n",
            "I0420 22:42:29.176517 139904526645120 model_training_utils.py:450] Train Step: 21756/21936  / loss = 0.07695823907852173\n",
            "I0420 22:42:29.713187 139904526645120 model_training_utils.py:450] Train Step: 21757/21936  / loss = 0.19317413866519928\n",
            "I0420 22:42:30.250424 139904526645120 model_training_utils.py:450] Train Step: 21758/21936  / loss = 0.21262657642364502\n",
            "I0420 22:42:30.788146 139904526645120 model_training_utils.py:450] Train Step: 21759/21936  / loss = 0.45303770899772644\n",
            "I0420 22:42:31.324971 139904526645120 model_training_utils.py:450] Train Step: 21760/21936  / loss = 0.05788657069206238\n",
            "I0420 22:42:31.862064 139904526645120 model_training_utils.py:450] Train Step: 21761/21936  / loss = 0.1075420081615448\n",
            "I0420 22:42:32.399202 139904526645120 model_training_utils.py:450] Train Step: 21762/21936  / loss = 0.14920799434185028\n",
            "I0420 22:42:32.932937 139904526645120 model_training_utils.py:450] Train Step: 21763/21936  / loss = 0.048159159719944\n",
            "I0420 22:42:33.470476 139904526645120 model_training_utils.py:450] Train Step: 21764/21936  / loss = 1.3747210502624512\n",
            "I0420 22:42:34.005917 139904526645120 model_training_utils.py:450] Train Step: 21765/21936  / loss = 1.4366958141326904\n",
            "I0420 22:42:34.543924 139904526645120 model_training_utils.py:450] Train Step: 21766/21936  / loss = 0.4123679995536804\n",
            "I0420 22:42:35.088461 139904526645120 model_training_utils.py:450] Train Step: 21767/21936  / loss = 0.05593230575323105\n",
            "I0420 22:42:35.625660 139904526645120 model_training_utils.py:450] Train Step: 21768/21936  / loss = 1.3969862461090088\n",
            "I0420 22:42:36.163767 139904526645120 keras_utils.py:122] TimeHistory: 26.92 seconds, 14.86 examples/second between steps 32687 and 32737\n",
            "I0420 22:42:36.166686 139904526645120 model_training_utils.py:450] Train Step: 21769/21936  / loss = 0.6639414429664612\n",
            "I0420 22:42:36.703898 139904526645120 model_training_utils.py:450] Train Step: 21770/21936  / loss = 0.7991052865982056\n",
            "I0420 22:42:37.239260 139904526645120 model_training_utils.py:450] Train Step: 21771/21936  / loss = 0.36320972442626953\n",
            "I0420 22:42:37.777154 139904526645120 model_training_utils.py:450] Train Step: 21772/21936  / loss = 1.38594651222229\n",
            "I0420 22:42:38.313596 139904526645120 model_training_utils.py:450] Train Step: 21773/21936  / loss = 0.3040028214454651\n",
            "I0420 22:42:38.849423 139904526645120 model_training_utils.py:450] Train Step: 21774/21936  / loss = 0.16284233331680298\n",
            "I0420 22:42:39.383281 139904526645120 model_training_utils.py:450] Train Step: 21775/21936  / loss = 0.21802473068237305\n",
            "I0420 22:42:39.919522 139904526645120 model_training_utils.py:450] Train Step: 21776/21936  / loss = 0.09534750133752823\n",
            "I0420 22:42:40.458818 139904526645120 model_training_utils.py:450] Train Step: 21777/21936  / loss = 0.6752597093582153\n",
            "I0420 22:42:40.995883 139904526645120 model_training_utils.py:450] Train Step: 21778/21936  / loss = 0.7464839220046997\n",
            "I0420 22:42:41.534985 139904526645120 model_training_utils.py:450] Train Step: 21779/21936  / loss = 0.17123842239379883\n",
            "I0420 22:42:42.075042 139904526645120 model_training_utils.py:450] Train Step: 21780/21936  / loss = 0.5099392533302307\n",
            "I0420 22:42:42.614305 139904526645120 model_training_utils.py:450] Train Step: 21781/21936  / loss = 2.202465057373047\n",
            "I0420 22:42:43.152021 139904526645120 model_training_utils.py:450] Train Step: 21782/21936  / loss = 0.27472758293151855\n",
            "I0420 22:42:43.691731 139904526645120 model_training_utils.py:450] Train Step: 21783/21936  / loss = 0.7134994268417358\n",
            "I0420 22:42:44.231201 139904526645120 model_training_utils.py:450] Train Step: 21784/21936  / loss = 0.4409072995185852\n",
            "I0420 22:42:44.767218 139904526645120 model_training_utils.py:450] Train Step: 21785/21936  / loss = 0.15083305537700653\n",
            "I0420 22:42:45.304743 139904526645120 model_training_utils.py:450] Train Step: 21786/21936  / loss = 0.38178253173828125\n",
            "I0420 22:42:45.840665 139904526645120 model_training_utils.py:450] Train Step: 21787/21936  / loss = 1.0825080871582031\n",
            "I0420 22:42:46.378610 139904526645120 model_training_utils.py:450] Train Step: 21788/21936  / loss = 0.6293919086456299\n",
            "I0420 22:42:46.916219 139904526645120 model_training_utils.py:450] Train Step: 21789/21936  / loss = 0.2088286280632019\n",
            "I0420 22:42:47.465247 139904526645120 model_training_utils.py:450] Train Step: 21790/21936  / loss = 0.6460320949554443\n",
            "I0420 22:42:48.003072 139904526645120 model_training_utils.py:450] Train Step: 21791/21936  / loss = 0.19031119346618652\n",
            "I0420 22:42:48.539669 139904526645120 model_training_utils.py:450] Train Step: 21792/21936  / loss = 0.6599553823471069\n",
            "I0420 22:42:49.079787 139904526645120 model_training_utils.py:450] Train Step: 21793/21936  / loss = 0.2697638273239136\n",
            "I0420 22:42:49.617339 139904526645120 model_training_utils.py:450] Train Step: 21794/21936  / loss = 1.0861821174621582\n",
            "I0420 22:42:50.159244 139904526645120 model_training_utils.py:450] Train Step: 21795/21936  / loss = 0.23093031346797943\n",
            "I0420 22:42:50.697109 139904526645120 model_training_utils.py:450] Train Step: 21796/21936  / loss = 0.270946741104126\n",
            "I0420 22:42:51.237868 139904526645120 model_training_utils.py:450] Train Step: 21797/21936  / loss = 0.3336178660392761\n",
            "I0420 22:42:51.778156 139904526645120 model_training_utils.py:450] Train Step: 21798/21936  / loss = 0.41596272587776184\n",
            "I0420 22:42:52.315156 139904526645120 model_training_utils.py:450] Train Step: 21799/21936  / loss = 0.08399740606546402\n",
            "I0420 22:42:52.853620 139904526645120 model_training_utils.py:450] Train Step: 21800/21936  / loss = 1.1516958475112915\n",
            "I0420 22:42:53.393943 139904526645120 model_training_utils.py:450] Train Step: 21801/21936  / loss = 0.18421725928783417\n",
            "I0420 22:42:53.931045 139904526645120 model_training_utils.py:450] Train Step: 21802/21936  / loss = 0.38377588987350464\n",
            "I0420 22:42:54.468420 139904526645120 model_training_utils.py:450] Train Step: 21803/21936  / loss = 0.3003462553024292\n",
            "I0420 22:42:55.004945 139904526645120 model_training_utils.py:450] Train Step: 21804/21936  / loss = 0.8607640266418457\n",
            "I0420 22:42:55.541681 139904526645120 model_training_utils.py:450] Train Step: 21805/21936  / loss = 0.2748801112174988\n",
            "I0420 22:42:56.079692 139904526645120 model_training_utils.py:450] Train Step: 21806/21936  / loss = 0.3085438907146454\n",
            "I0420 22:42:56.617432 139904526645120 model_training_utils.py:450] Train Step: 21807/21936  / loss = 0.6579694747924805\n",
            "I0420 22:42:57.155677 139904526645120 model_training_utils.py:450] Train Step: 21808/21936  / loss = 0.28311705589294434\n",
            "I0420 22:42:57.694245 139904526645120 model_training_utils.py:450] Train Step: 21809/21936  / loss = 0.5068312287330627\n",
            "I0420 22:42:58.231522 139904526645120 model_training_utils.py:450] Train Step: 21810/21936  / loss = 0.4494829773902893\n",
            "I0420 22:42:58.769534 139904526645120 model_training_utils.py:450] Train Step: 21811/21936  / loss = 1.0016134977340698\n",
            "I0420 22:42:59.305531 139904526645120 model_training_utils.py:450] Train Step: 21812/21936  / loss = 0.29498761892318726\n",
            "I0420 22:42:59.840736 139904526645120 model_training_utils.py:450] Train Step: 21813/21936  / loss = 0.6281684041023254\n",
            "I0420 22:43:00.376766 139904526645120 model_training_utils.py:450] Train Step: 21814/21936  / loss = 0.3729098439216614\n",
            "I0420 22:43:00.914597 139904526645120 model_training_utils.py:450] Train Step: 21815/21936  / loss = 0.16005933284759521\n",
            "I0420 22:43:01.454720 139904526645120 model_training_utils.py:450] Train Step: 21816/21936  / loss = 0.41171643137931824\n",
            "I0420 22:43:01.990554 139904526645120 model_training_utils.py:450] Train Step: 21817/21936  / loss = 0.5236077904701233\n",
            "I0420 22:43:02.529960 139904526645120 model_training_utils.py:450] Train Step: 21818/21936  / loss = 0.3199421763420105\n",
            "I0420 22:43:03.066657 139904526645120 keras_utils.py:122] TimeHistory: 26.90 seconds, 14.87 examples/second between steps 32737 and 32787\n",
            "I0420 22:43:03.069389 139904526645120 model_training_utils.py:450] Train Step: 21819/21936  / loss = 0.18270240724086761\n",
            "I0420 22:43:03.606225 139904526645120 model_training_utils.py:450] Train Step: 21820/21936  / loss = 0.472380131483078\n",
            "I0420 22:43:04.144639 139904526645120 model_training_utils.py:450] Train Step: 21821/21936  / loss = 0.4278610646724701\n",
            "I0420 22:43:04.683974 139904526645120 model_training_utils.py:450] Train Step: 21822/21936  / loss = 0.07705347239971161\n",
            "I0420 22:43:05.221971 139904526645120 model_training_utils.py:450] Train Step: 21823/21936  / loss = 0.1495908796787262\n",
            "I0420 22:43:05.760794 139904526645120 model_training_utils.py:450] Train Step: 21824/21936  / loss = 0.4924487769603729\n",
            "I0420 22:43:06.298387 139904526645120 model_training_utils.py:450] Train Step: 21825/21936  / loss = 0.09031806141138077\n",
            "I0420 22:43:06.833719 139904526645120 model_training_utils.py:450] Train Step: 21826/21936  / loss = 0.03499656915664673\n",
            "I0420 22:43:07.374645 139904526645120 model_training_utils.py:450] Train Step: 21827/21936  / loss = 0.3969939947128296\n",
            "I0420 22:43:07.914431 139904526645120 model_training_utils.py:450] Train Step: 21828/21936  / loss = 0.3392695486545563\n",
            "I0420 22:43:08.450752 139904526645120 model_training_utils.py:450] Train Step: 21829/21936  / loss = 0.5531320571899414\n",
            "I0420 22:43:08.988816 139904526645120 model_training_utils.py:450] Train Step: 21830/21936  / loss = 0.20316436886787415\n",
            "I0420 22:43:09.528658 139904526645120 model_training_utils.py:450] Train Step: 21831/21936  / loss = 0.5121448040008545\n",
            "I0420 22:43:10.067712 139904526645120 model_training_utils.py:450] Train Step: 21832/21936  / loss = 0.26572033762931824\n",
            "I0420 22:43:10.607037 139904526645120 model_training_utils.py:450] Train Step: 21833/21936  / loss = 0.16644780337810516\n",
            "I0420 22:43:11.146169 139904526645120 model_training_utils.py:450] Train Step: 21834/21936  / loss = 0.8281200528144836\n",
            "I0420 22:43:11.683495 139904526645120 model_training_utils.py:450] Train Step: 21835/21936  / loss = 0.4330503046512604\n",
            "I0420 22:43:12.224379 139904526645120 model_training_utils.py:450] Train Step: 21836/21936  / loss = 0.2834222912788391\n",
            "I0420 22:43:12.765205 139904526645120 model_training_utils.py:450] Train Step: 21837/21936  / loss = 0.6383749842643738\n",
            "I0420 22:43:13.303635 139904526645120 model_training_utils.py:450] Train Step: 21838/21936  / loss = 0.2180459350347519\n",
            "I0420 22:43:13.840154 139904526645120 model_training_utils.py:450] Train Step: 21839/21936  / loss = 0.20241840183734894\n",
            "I0420 22:43:14.377979 139904526645120 model_training_utils.py:450] Train Step: 21840/21936  / loss = 1.0764102935791016\n",
            "I0420 22:43:14.914248 139904526645120 model_training_utils.py:450] Train Step: 21841/21936  / loss = 0.39211493730545044\n",
            "I0420 22:43:15.451418 139904526645120 model_training_utils.py:450] Train Step: 21842/21936  / loss = 0.3399180769920349\n",
            "I0420 22:43:15.988222 139904526645120 model_training_utils.py:450] Train Step: 21843/21936  / loss = 0.3816283643245697\n",
            "I0420 22:43:16.525942 139904526645120 model_training_utils.py:450] Train Step: 21844/21936  / loss = 0.3811088800430298\n",
            "I0420 22:43:17.062895 139904526645120 model_training_utils.py:450] Train Step: 21845/21936  / loss = 0.19430488348007202\n",
            "I0420 22:43:17.601302 139904526645120 model_training_utils.py:450] Train Step: 21846/21936  / loss = 0.3063417971134186\n",
            "I0420 22:43:18.137317 139904526645120 model_training_utils.py:450] Train Step: 21847/21936  / loss = 0.4121042788028717\n",
            "I0420 22:43:18.675764 139904526645120 model_training_utils.py:450] Train Step: 21848/21936  / loss = 0.8620574474334717\n",
            "I0420 22:43:19.215744 139904526645120 model_training_utils.py:450] Train Step: 21849/21936  / loss = 0.32364311814308167\n",
            "I0420 22:43:19.753341 139904526645120 model_training_utils.py:450] Train Step: 21850/21936  / loss = 0.3710331320762634\n",
            "I0420 22:43:20.291269 139904526645120 model_training_utils.py:450] Train Step: 21851/21936  / loss = 0.3840514123439789\n",
            "I0420 22:43:20.833261 139904526645120 model_training_utils.py:450] Train Step: 21852/21936  / loss = 0.08333577960729599\n",
            "I0420 22:43:21.372411 139904526645120 model_training_utils.py:450] Train Step: 21853/21936  / loss = 0.20296630263328552\n",
            "I0420 22:43:21.911233 139904526645120 model_training_utils.py:450] Train Step: 21854/21936  / loss = 0.11270873248577118\n",
            "I0420 22:43:22.447434 139904526645120 model_training_utils.py:450] Train Step: 21855/21936  / loss = 0.2944864332675934\n",
            "I0420 22:43:22.993366 139904526645120 model_training_utils.py:450] Train Step: 21856/21936  / loss = 0.3378210961818695\n",
            "I0420 22:43:23.535126 139904526645120 model_training_utils.py:450] Train Step: 21857/21936  / loss = 0.15955866873264313\n",
            "I0420 22:43:24.073896 139904526645120 model_training_utils.py:450] Train Step: 21858/21936  / loss = 0.244700625538826\n",
            "I0420 22:43:24.613048 139904526645120 model_training_utils.py:450] Train Step: 21859/21936  / loss = 0.39707088470458984\n",
            "I0420 22:43:25.150204 139904526645120 model_training_utils.py:450] Train Step: 21860/21936  / loss = 1.0216083526611328\n",
            "I0420 22:43:25.697598 139904526645120 model_training_utils.py:450] Train Step: 21861/21936  / loss = 0.06978729367256165\n",
            "I0420 22:43:26.241878 139904526645120 model_training_utils.py:450] Train Step: 21862/21936  / loss = 0.6740673184394836\n",
            "I0420 22:43:26.782261 139904526645120 model_training_utils.py:450] Train Step: 21863/21936  / loss = 0.3631284236907959\n",
            "I0420 22:43:27.320890 139904526645120 model_training_utils.py:450] Train Step: 21864/21936  / loss = 0.13855905830860138\n",
            "I0420 22:43:27.855872 139904526645120 model_training_utils.py:450] Train Step: 21865/21936  / loss = 0.08665769547224045\n",
            "I0420 22:43:28.391312 139904526645120 model_training_utils.py:450] Train Step: 21866/21936  / loss = 0.18962880969047546\n",
            "I0420 22:43:28.931166 139904526645120 model_training_utils.py:450] Train Step: 21867/21936  / loss = 0.6542121171951294\n",
            "I0420 22:43:29.470555 139904526645120 model_training_utils.py:450] Train Step: 21868/21936  / loss = 0.09915654361248016\n",
            "I0420 22:43:30.012405 139904526645120 keras_utils.py:122] TimeHistory: 26.94 seconds, 14.85 examples/second between steps 32787 and 32837\n",
            "I0420 22:43:30.015112 139904526645120 model_training_utils.py:450] Train Step: 21869/21936  / loss = 0.16112110018730164\n",
            "I0420 22:43:30.551986 139904526645120 model_training_utils.py:450] Train Step: 21870/21936  / loss = 0.41104844212532043\n",
            "I0420 22:43:31.089164 139904526645120 model_training_utils.py:450] Train Step: 21871/21936  / loss = 0.17969012260437012\n",
            "I0420 22:43:31.625926 139904526645120 model_training_utils.py:450] Train Step: 21872/21936  / loss = 0.2896769940853119\n",
            "I0420 22:43:32.164000 139904526645120 model_training_utils.py:450] Train Step: 21873/21936  / loss = 0.1695803552865982\n",
            "I0420 22:43:32.700290 139904526645120 model_training_utils.py:450] Train Step: 21874/21936  / loss = 0.20555609464645386\n",
            "I0420 22:43:33.244736 139904526645120 model_training_utils.py:450] Train Step: 21875/21936  / loss = 0.3067101240158081\n",
            "I0420 22:43:33.780971 139904526645120 model_training_utils.py:450] Train Step: 21876/21936  / loss = 0.8796206712722778\n",
            "I0420 22:43:34.317636 139904526645120 model_training_utils.py:450] Train Step: 21877/21936  / loss = 0.7634483575820923\n",
            "I0420 22:43:34.854907 139904526645120 model_training_utils.py:450] Train Step: 21878/21936  / loss = 0.33209896087646484\n",
            "I0420 22:43:35.389226 139904526645120 model_training_utils.py:450] Train Step: 21879/21936  / loss = 0.16290442645549774\n",
            "I0420 22:43:35.927534 139904526645120 model_training_utils.py:450] Train Step: 21880/21936  / loss = 0.155931755900383\n",
            "I0420 22:43:36.465786 139904526645120 model_training_utils.py:450] Train Step: 21881/21936  / loss = 0.3558166027069092\n",
            "I0420 22:43:37.005279 139904526645120 model_training_utils.py:450] Train Step: 21882/21936  / loss = 0.10077500343322754\n",
            "I0420 22:43:37.543388 139904526645120 model_training_utils.py:450] Train Step: 21883/21936  / loss = 0.36576640605926514\n",
            "I0420 22:43:38.080931 139904526645120 model_training_utils.py:450] Train Step: 21884/21936  / loss = 0.11426398903131485\n",
            "I0420 22:43:38.619839 139904526645120 model_training_utils.py:450] Train Step: 21885/21936  / loss = 0.4749331474304199\n",
            "I0420 22:43:39.155918 139904526645120 model_training_utils.py:450] Train Step: 21886/21936  / loss = 0.3883669078350067\n",
            "I0420 22:43:39.692211 139904526645120 model_training_utils.py:450] Train Step: 21887/21936  / loss = 0.5534162521362305\n",
            "I0420 22:43:40.227597 139904526645120 model_training_utils.py:450] Train Step: 21888/21936  / loss = 0.3716143071651459\n",
            "I0420 22:43:40.773011 139904526645120 model_training_utils.py:450] Train Step: 21889/21936  / loss = 0.31608882546424866\n",
            "I0420 22:43:41.311992 139904526645120 model_training_utils.py:450] Train Step: 21890/21936  / loss = 0.18988744914531708\n",
            "I0420 22:43:41.857536 139904526645120 model_training_utils.py:450] Train Step: 21891/21936  / loss = 0.45485949516296387\n",
            "I0420 22:43:42.396967 139904526645120 model_training_utils.py:450] Train Step: 21892/21936  / loss = 0.35539010167121887\n",
            "I0420 22:43:42.936135 139904526645120 model_training_utils.py:450] Train Step: 21893/21936  / loss = 0.28687214851379395\n",
            "I0420 22:43:43.474283 139904526645120 model_training_utils.py:450] Train Step: 21894/21936  / loss = 0.3601243495941162\n",
            "I0420 22:43:44.012560 139904526645120 model_training_utils.py:450] Train Step: 21895/21936  / loss = 0.1866842359304428\n",
            "I0420 22:43:44.549713 139904526645120 model_training_utils.py:450] Train Step: 21896/21936  / loss = 0.15322887897491455\n",
            "I0420 22:43:45.088190 139904526645120 model_training_utils.py:450] Train Step: 21897/21936  / loss = 0.6806752681732178\n",
            "I0420 22:43:45.625695 139904526645120 model_training_utils.py:450] Train Step: 21898/21936  / loss = 0.3318423628807068\n",
            "I0420 22:43:46.164178 139904526645120 model_training_utils.py:450] Train Step: 21899/21936  / loss = 1.140694499015808\n",
            "I0420 22:43:46.703399 139904526645120 model_training_utils.py:450] Train Step: 21900/21936  / loss = 0.07702889293432236\n",
            "I0420 22:43:47.238700 139904526645120 model_training_utils.py:450] Train Step: 21901/21936  / loss = 0.20831558108329773\n",
            "I0420 22:43:47.775927 139904526645120 model_training_utils.py:450] Train Step: 21902/21936  / loss = 0.3227086067199707\n",
            "I0420 22:43:48.315901 139904526645120 model_training_utils.py:450] Train Step: 21903/21936  / loss = 0.25163403153419495\n",
            "I0420 22:43:48.855233 139904526645120 model_training_utils.py:450] Train Step: 21904/21936  / loss = 0.33629608154296875\n",
            "I0420 22:43:49.391674 139904526645120 model_training_utils.py:450] Train Step: 21905/21936  / loss = 0.1811893880367279\n",
            "I0420 22:43:49.929806 139904526645120 model_training_utils.py:450] Train Step: 21906/21936  / loss = 0.2306806594133377\n",
            "I0420 22:43:50.466169 139904526645120 model_training_utils.py:450] Train Step: 21907/21936  / loss = 0.242437943816185\n",
            "I0420 22:43:51.007283 139904526645120 model_training_utils.py:450] Train Step: 21908/21936  / loss = 0.14512789249420166\n",
            "I0420 22:43:51.542717 139904526645120 model_training_utils.py:450] Train Step: 21909/21936  / loss = 0.40053439140319824\n",
            "I0420 22:43:52.078925 139904526645120 model_training_utils.py:450] Train Step: 21910/21936  / loss = 0.11609867960214615\n",
            "I0420 22:43:52.614655 139904526645120 model_training_utils.py:450] Train Step: 21911/21936  / loss = 0.1896096169948578\n",
            "I0420 22:43:53.155122 139904526645120 model_training_utils.py:450] Train Step: 21912/21936  / loss = 0.16491267085075378\n",
            "I0420 22:43:53.693394 139904526645120 model_training_utils.py:450] Train Step: 21913/21936  / loss = 0.7137491106987\n",
            "I0420 22:43:54.229427 139904526645120 model_training_utils.py:450] Train Step: 21914/21936  / loss = 0.31794190406799316\n",
            "I0420 22:43:54.769969 139904526645120 model_training_utils.py:450] Train Step: 21915/21936  / loss = 0.592186450958252\n",
            "I0420 22:43:55.306955 139904526645120 model_training_utils.py:450] Train Step: 21916/21936  / loss = 0.20160508155822754\n",
            "I0420 22:43:55.845609 139904526645120 model_training_utils.py:450] Train Step: 21917/21936  / loss = 0.3859502673149109\n",
            "I0420 22:43:56.382037 139904526645120 model_training_utils.py:450] Train Step: 21918/21936  / loss = 0.08439203351736069\n",
            "I0420 22:43:56.919267 139904526645120 keras_utils.py:122] TimeHistory: 26.90 seconds, 14.87 examples/second between steps 32837 and 32887\n",
            "I0420 22:43:56.921970 139904526645120 model_training_utils.py:450] Train Step: 21919/21936  / loss = 0.06729552149772644\n",
            "I0420 22:43:57.456696 139904526645120 model_training_utils.py:450] Train Step: 21920/21936  / loss = 0.20371946692466736\n",
            "I0420 22:43:57.994138 139904526645120 model_training_utils.py:450] Train Step: 21921/21936  / loss = 0.1160031259059906\n",
            "I0420 22:43:58.530807 139904526645120 model_training_utils.py:450] Train Step: 21922/21936  / loss = 0.4172218441963196\n",
            "I0420 22:43:59.067182 139904526645120 model_training_utils.py:450] Train Step: 21923/21936  / loss = 0.1376342624425888\n",
            "I0420 22:43:59.601796 139904526645120 model_training_utils.py:450] Train Step: 21924/21936  / loss = 0.3960978388786316\n",
            "I0420 22:44:00.142722 139904526645120 model_training_utils.py:450] Train Step: 21925/21936  / loss = 0.16549226641654968\n",
            "I0420 22:44:00.677111 139904526645120 model_training_utils.py:450] Train Step: 21926/21936  / loss = 0.06395242363214493\n",
            "I0420 22:44:01.215363 139904526645120 model_training_utils.py:450] Train Step: 21927/21936  / loss = 0.3222402036190033\n",
            "I0420 22:44:01.750644 139904526645120 model_training_utils.py:450] Train Step: 21928/21936  / loss = 0.03169260174036026\n",
            "I0420 22:44:02.287901 139904526645120 model_training_utils.py:450] Train Step: 21929/21936  / loss = 0.17795701324939728\n",
            "I0420 22:44:02.824224 139904526645120 model_training_utils.py:450] Train Step: 21930/21936  / loss = 0.20931470394134521\n",
            "I0420 22:44:03.360619 139904526645120 model_training_utils.py:450] Train Step: 21931/21936  / loss = 0.16165713965892792\n",
            "I0420 22:44:03.905596 139904526645120 model_training_utils.py:450] Train Step: 21932/21936  / loss = 0.47096726298332214\n",
            "I0420 22:44:04.441648 139904526645120 model_training_utils.py:450] Train Step: 21933/21936  / loss = 0.16982093453407288\n",
            "I0420 22:44:04.978079 139904526645120 model_training_utils.py:450] Train Step: 21934/21936  / loss = 0.3470368981361389\n",
            "I0420 22:44:05.514161 139904526645120 model_training_utils.py:450] Train Step: 21935/21936  / loss = 0.2395111322402954\n",
            "I0420 22:44:06.049410 139904526645120 model_training_utils.py:450] Train Step: 21936/21936  / loss = 0.8369525074958801\n",
            "I0420 22:44:11.898643 139904526645120 model_training_utils.py:48] Saving model as TF checkpoint: /mydrive/bert_finetuning_outputs/ctl_step_21936.ckpt-2\n",
            "I0420 22:44:11.901288 139904526645120 model_training_utils.py:94] Training Summary: \n",
            "{'total_training_steps': 21936, 'train_loss': 0.8369525074958801}\n",
            "I0420 22:44:13.440128 139904526645120 squad_lib.py:353] *** Example ***\n",
            "I0420 22:44:13.440286 139904526645120 squad_lib.py:354] unique_id: 1000000000\n",
            "I0420 22:44:13.440365 139904526645120 squad_lib.py:355] example_index: 0\n",
            "I0420 22:44:13.440423 139904526645120 squad_lib.py:356] doc_span_index: 0\n",
            "I0420 22:44:13.440550 139904526645120 squad_lib.py:358] tokens: [CLS] which nfl team represented the afc at super bowl 50 ? [SEP] super bowl 50 was an american football game to determine the champion of the national football league ( nfl ) for the 2015 season . the american football conference ( afc ) champion denver broncos defeated the national football conference ( nfc ) champion carolina panthers 24 – 10 to earn their third super bowl title . the game was played on february 7 , 2016 , at levi ' s stadium in the san francisco bay area at santa clara , california . as this was the 50th super bowl , the league emphasized the \" golden anniversary \" with various gold - themed initiatives , as well as temporarily suspend ##ing the tradition of naming each super bowl game with roman nu ##meral ##s ( under which the game would have been known as \" super bowl l \" ) , so that the logo could prominently feature the arabic nu ##meral ##s 50 . [SEP]\n",
            "I0420 22:44:13.440718 139904526645120 squad_lib.py:361] token_to_orig_map: 13:0 14:1 15:2 16:3 17:4 18:5 19:6 20:7 21:8 22:9 23:10 24:11 25:12 26:13 27:14 28:15 29:16 30:17 31:17 32:17 33:18 34:19 35:20 36:21 37:21 38:22 39:23 40:24 41:25 42:26 43:26 44:26 45:27 46:28 47:29 48:30 49:31 50:32 51:33 52:34 53:35 54:35 55:35 56:36 57:37 58:38 59:39 60:39 61:39 62:40 63:41 64:42 65:43 66:44 67:45 68:46 69:46 70:47 71:48 72:49 73:50 74:51 75:52 76:53 77:53 78:54 79:54 80:55 81:56 82:56 83:56 84:57 85:58 86:59 87:60 88:61 89:62 90:63 91:64 92:65 93:66 94:66 95:67 96:67 97:68 98:69 99:70 100:71 101:72 102:73 103:74 104:74 105:75 106:76 107:77 108:78 109:79 110:79 111:80 112:80 113:81 114:82 115:83 116:83 117:83 118:84 119:84 120:85 121:86 122:87 123:88 124:89 125:89 126:90 127:91 128:92 129:93 130:94 131:95 132:96 133:97 134:98 135:99 136:100 137:100 138:100 139:101 140:101 141:102 142:103 143:104 144:105 145:106 146:107 147:108 148:109 149:110 150:110 151:111 152:112 153:112 154:112 155:112 156:113 157:114 158:115 159:116 160:117 161:118 162:119 163:120 164:121 165:122 166:122 167:122 168:123 169:123\n",
            "I0420 22:44:13.440844 139904526645120 squad_lib.py:366] token_is_max_context: 13:True 14:True 15:True 16:True 17:True 18:True 19:True 20:True 21:True 22:True 23:True 24:True 25:True 26:True 27:True 28:True 29:True 30:True 31:True 32:True 33:True 34:True 35:True 36:True 37:True 38:True 39:True 40:True 41:True 42:True 43:True 44:True 45:True 46:True 47:True 48:True 49:True 50:True 51:True 52:True 53:True 54:True 55:True 56:True 57:True 58:True 59:True 60:True 61:True 62:True 63:True 64:True 65:True 66:True 67:True 68:True 69:True 70:True 71:True 72:True 73:True 74:True 75:True 76:True 77:True 78:True 79:True 80:True 81:True 82:True 83:True 84:True 85:True 86:True 87:True 88:True 89:True 90:True 91:True 92:True 93:True 94:True 95:True 96:True 97:True 98:True 99:True 100:True 101:True 102:True 103:True 104:True 105:True 106:True 107:True 108:True 109:True 110:True 111:True 112:True 113:True 114:True 115:True 116:True 117:True 118:True 119:True 120:True 121:True 122:True 123:True 124:True 125:True 126:True 127:True 128:True 129:True 130:True 131:True 132:True 133:True 134:True 135:True 136:True 137:True 138:True 139:True 140:True 141:True 142:True 143:True 144:True 145:True 146:True 147:True 148:True 149:True 150:True 151:True 152:True 153:True 154:True 155:True 156:True 157:True 158:True 159:True 160:True 161:True 162:True 163:True 164:True 165:True 166:True 167:True 168:True 169:True\n",
            "I0420 22:44:13.441021 139904526645120 squad_lib.py:368] input_ids: 101 2029 5088 2136 3421 1996 10511 2012 3565 4605 2753 1029 102 3565 4605 2753 2001 2019 2137 2374 2208 2000 5646 1996 3410 1997 1996 2120 2374 2223 1006 5088 1007 2005 1996 2325 2161 1012 1996 2137 2374 3034 1006 10511 1007 3410 7573 14169 3249 1996 2120 2374 3034 1006 22309 1007 3410 3792 12915 2484 1516 2184 2000 7796 2037 2353 3565 4605 2516 1012 1996 2208 2001 2209 2006 2337 1021 1010 2355 1010 2012 11902 1005 1055 3346 1999 1996 2624 3799 3016 2181 2012 4203 10254 1010 2662 1012 2004 2023 2001 1996 12951 3565 4605 1010 1996 2223 13155 1996 1000 3585 5315 1000 2007 2536 2751 1011 11773 11107 1010 2004 2092 2004 8184 28324 2075 1996 4535 1997 10324 2169 3565 4605 2208 2007 3142 16371 28990 2015 1006 2104 2029 1996 2208 2052 2031 2042 2124 2004 1000 3565 4605 1048 1000 1007 1010 2061 2008 1996 8154 2071 14500 3444 1996 5640 16371 28990 2015 2753 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0420 22:44:13.441182 139904526645120 squad_lib.py:369] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0420 22:44:13.441337 139904526645120 squad_lib.py:370] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0420 22:44:13.444829 139904526645120 squad_lib.py:353] *** Example ***\n",
            "I0420 22:44:13.444944 139904526645120 squad_lib.py:354] unique_id: 1000000001\n",
            "I0420 22:44:13.445016 139904526645120 squad_lib.py:355] example_index: 1\n",
            "I0420 22:44:13.445070 139904526645120 squad_lib.py:356] doc_span_index: 0\n",
            "I0420 22:44:13.445186 139904526645120 squad_lib.py:358] tokens: [CLS] which nfl team represented the nfc at super bowl 50 ? [SEP] super bowl 50 was an american football game to determine the champion of the national football league ( nfl ) for the 2015 season . the american football conference ( afc ) champion denver broncos defeated the national football conference ( nfc ) champion carolina panthers 24 – 10 to earn their third super bowl title . the game was played on february 7 , 2016 , at levi ' s stadium in the san francisco bay area at santa clara , california . as this was the 50th super bowl , the league emphasized the \" golden anniversary \" with various gold - themed initiatives , as well as temporarily suspend ##ing the tradition of naming each super bowl game with roman nu ##meral ##s ( under which the game would have been known as \" super bowl l \" ) , so that the logo could prominently feature the arabic nu ##meral ##s 50 . [SEP]\n",
            "I0420 22:44:13.445304 139904526645120 squad_lib.py:361] token_to_orig_map: 13:0 14:1 15:2 16:3 17:4 18:5 19:6 20:7 21:8 22:9 23:10 24:11 25:12 26:13 27:14 28:15 29:16 30:17 31:17 32:17 33:18 34:19 35:20 36:21 37:21 38:22 39:23 40:24 41:25 42:26 43:26 44:26 45:27 46:28 47:29 48:30 49:31 50:32 51:33 52:34 53:35 54:35 55:35 56:36 57:37 58:38 59:39 60:39 61:39 62:40 63:41 64:42 65:43 66:44 67:45 68:46 69:46 70:47 71:48 72:49 73:50 74:51 75:52 76:53 77:53 78:54 79:54 80:55 81:56 82:56 83:56 84:57 85:58 86:59 87:60 88:61 89:62 90:63 91:64 92:65 93:66 94:66 95:67 96:67 97:68 98:69 99:70 100:71 101:72 102:73 103:74 104:74 105:75 106:76 107:77 108:78 109:79 110:79 111:80 112:80 113:81 114:82 115:83 116:83 117:83 118:84 119:84 120:85 121:86 122:87 123:88 124:89 125:89 126:90 127:91 128:92 129:93 130:94 131:95 132:96 133:97 134:98 135:99 136:100 137:100 138:100 139:101 140:101 141:102 142:103 143:104 144:105 145:106 146:107 147:108 148:109 149:110 150:110 151:111 152:112 153:112 154:112 155:112 156:113 157:114 158:115 159:116 160:117 161:118 162:119 163:120 164:121 165:122 166:122 167:122 168:123 169:123\n",
            "I0420 22:44:13.445415 139904526645120 squad_lib.py:366] token_is_max_context: 13:True 14:True 15:True 16:True 17:True 18:True 19:True 20:True 21:True 22:True 23:True 24:True 25:True 26:True 27:True 28:True 29:True 30:True 31:True 32:True 33:True 34:True 35:True 36:True 37:True 38:True 39:True 40:True 41:True 42:True 43:True 44:True 45:True 46:True 47:True 48:True 49:True 50:True 51:True 52:True 53:True 54:True 55:True 56:True 57:True 58:True 59:True 60:True 61:True 62:True 63:True 64:True 65:True 66:True 67:True 68:True 69:True 70:True 71:True 72:True 73:True 74:True 75:True 76:True 77:True 78:True 79:True 80:True 81:True 82:True 83:True 84:True 85:True 86:True 87:True 88:True 89:True 90:True 91:True 92:True 93:True 94:True 95:True 96:True 97:True 98:True 99:True 100:True 101:True 102:True 103:True 104:True 105:True 106:True 107:True 108:True 109:True 110:True 111:True 112:True 113:True 114:True 115:True 116:True 117:True 118:True 119:True 120:True 121:True 122:True 123:True 124:True 125:True 126:True 127:True 128:True 129:True 130:True 131:True 132:True 133:True 134:True 135:True 136:True 137:True 138:True 139:True 140:True 141:True 142:True 143:True 144:True 145:True 146:True 147:True 148:True 149:True 150:True 151:True 152:True 153:True 154:True 155:True 156:True 157:True 158:True 159:True 160:True 161:True 162:True 163:True 164:True 165:True 166:True 167:True 168:True 169:True\n",
            "I0420 22:44:13.445599 139904526645120 squad_lib.py:368] input_ids: 101 2029 5088 2136 3421 1996 22309 2012 3565 4605 2753 1029 102 3565 4605 2753 2001 2019 2137 2374 2208 2000 5646 1996 3410 1997 1996 2120 2374 2223 1006 5088 1007 2005 1996 2325 2161 1012 1996 2137 2374 3034 1006 10511 1007 3410 7573 14169 3249 1996 2120 2374 3034 1006 22309 1007 3410 3792 12915 2484 1516 2184 2000 7796 2037 2353 3565 4605 2516 1012 1996 2208 2001 2209 2006 2337 1021 1010 2355 1010 2012 11902 1005 1055 3346 1999 1996 2624 3799 3016 2181 2012 4203 10254 1010 2662 1012 2004 2023 2001 1996 12951 3565 4605 1010 1996 2223 13155 1996 1000 3585 5315 1000 2007 2536 2751 1011 11773 11107 1010 2004 2092 2004 8184 28324 2075 1996 4535 1997 10324 2169 3565 4605 2208 2007 3142 16371 28990 2015 1006 2104 2029 1996 2208 2052 2031 2042 2124 2004 1000 3565 4605 1048 1000 1007 1010 2061 2008 1996 8154 2071 14500 3444 1996 5640 16371 28990 2015 2753 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0420 22:44:13.445769 139904526645120 squad_lib.py:369] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0420 22:44:13.445920 139904526645120 squad_lib.py:370] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0420 22:44:13.449075 139904526645120 squad_lib.py:353] *** Example ***\n",
            "I0420 22:44:13.449164 139904526645120 squad_lib.py:354] unique_id: 1000000002\n",
            "I0420 22:44:13.449227 139904526645120 squad_lib.py:355] example_index: 2\n",
            "I0420 22:44:13.449280 139904526645120 squad_lib.py:356] doc_span_index: 0\n",
            "I0420 22:44:13.449395 139904526645120 squad_lib.py:358] tokens: [CLS] where did super bowl 50 take place ? [SEP] super bowl 50 was an american football game to determine the champion of the national football league ( nfl ) for the 2015 season . the american football conference ( afc ) champion denver broncos defeated the national football conference ( nfc ) champion carolina panthers 24 – 10 to earn their third super bowl title . the game was played on february 7 , 2016 , at levi ' s stadium in the san francisco bay area at santa clara , california . as this was the 50th super bowl , the league emphasized the \" golden anniversary \" with various gold - themed initiatives , as well as temporarily suspend ##ing the tradition of naming each super bowl game with roman nu ##meral ##s ( under which the game would have been known as \" super bowl l \" ) , so that the logo could prominently feature the arabic nu ##meral ##s 50 . [SEP]\n",
            "I0420 22:44:13.449512 139904526645120 squad_lib.py:361] token_to_orig_map: 10:0 11:1 12:2 13:3 14:4 15:5 16:6 17:7 18:8 19:9 20:10 21:11 22:12 23:13 24:14 25:15 26:16 27:17 28:17 29:17 30:18 31:19 32:20 33:21 34:21 35:22 36:23 37:24 38:25 39:26 40:26 41:26 42:27 43:28 44:29 45:30 46:31 47:32 48:33 49:34 50:35 51:35 52:35 53:36 54:37 55:38 56:39 57:39 58:39 59:40 60:41 61:42 62:43 63:44 64:45 65:46 66:46 67:47 68:48 69:49 70:50 71:51 72:52 73:53 74:53 75:54 76:54 77:55 78:56 79:56 80:56 81:57 82:58 83:59 84:60 85:61 86:62 87:63 88:64 89:65 90:66 91:66 92:67 93:67 94:68 95:69 96:70 97:71 98:72 99:73 100:74 101:74 102:75 103:76 104:77 105:78 106:79 107:79 108:80 109:80 110:81 111:82 112:83 113:83 114:83 115:84 116:84 117:85 118:86 119:87 120:88 121:89 122:89 123:90 124:91 125:92 126:93 127:94 128:95 129:96 130:97 131:98 132:99 133:100 134:100 135:100 136:101 137:101 138:102 139:103 140:104 141:105 142:106 143:107 144:108 145:109 146:110 147:110 148:111 149:112 150:112 151:112 152:112 153:113 154:114 155:115 156:116 157:117 158:118 159:119 160:120 161:121 162:122 163:122 164:122 165:123 166:123\n",
            "I0420 22:44:13.506184 139904526645120 squad_lib.py:366] token_is_max_context: 10:True 11:True 12:True 13:True 14:True 15:True 16:True 17:True 18:True 19:True 20:True 21:True 22:True 23:True 24:True 25:True 26:True 27:True 28:True 29:True 30:True 31:True 32:True 33:True 34:True 35:True 36:True 37:True 38:True 39:True 40:True 41:True 42:True 43:True 44:True 45:True 46:True 47:True 48:True 49:True 50:True 51:True 52:True 53:True 54:True 55:True 56:True 57:True 58:True 59:True 60:True 61:True 62:True 63:True 64:True 65:True 66:True 67:True 68:True 69:True 70:True 71:True 72:True 73:True 74:True 75:True 76:True 77:True 78:True 79:True 80:True 81:True 82:True 83:True 84:True 85:True 86:True 87:True 88:True 89:True 90:True 91:True 92:True 93:True 94:True 95:True 96:True 97:True 98:True 99:True 100:True 101:True 102:True 103:True 104:True 105:True 106:True 107:True 108:True 109:True 110:True 111:True 112:True 113:True 114:True 115:True 116:True 117:True 118:True 119:True 120:True 121:True 122:True 123:True 124:True 125:True 126:True 127:True 128:True 129:True 130:True 131:True 132:True 133:True 134:True 135:True 136:True 137:True 138:True 139:True 140:True 141:True 142:True 143:True 144:True 145:True 146:True 147:True 148:True 149:True 150:True 151:True 152:True 153:True 154:True 155:True 156:True 157:True 158:True 159:True 160:True 161:True 162:True 163:True 164:True 165:True 166:True\n",
            "I0420 22:44:13.506465 139904526645120 squad_lib.py:368] input_ids: 101 2073 2106 3565 4605 2753 2202 2173 1029 102 3565 4605 2753 2001 2019 2137 2374 2208 2000 5646 1996 3410 1997 1996 2120 2374 2223 1006 5088 1007 2005 1996 2325 2161 1012 1996 2137 2374 3034 1006 10511 1007 3410 7573 14169 3249 1996 2120 2374 3034 1006 22309 1007 3410 3792 12915 2484 1516 2184 2000 7796 2037 2353 3565 4605 2516 1012 1996 2208 2001 2209 2006 2337 1021 1010 2355 1010 2012 11902 1005 1055 3346 1999 1996 2624 3799 3016 2181 2012 4203 10254 1010 2662 1012 2004 2023 2001 1996 12951 3565 4605 1010 1996 2223 13155 1996 1000 3585 5315 1000 2007 2536 2751 1011 11773 11107 1010 2004 2092 2004 8184 28324 2075 1996 4535 1997 10324 2169 3565 4605 2208 2007 3142 16371 28990 2015 1006 2104 2029 1996 2208 2052 2031 2042 2124 2004 1000 3565 4605 1048 1000 1007 1010 2061 2008 1996 8154 2071 14500 3444 1996 5640 16371 28990 2015 2753 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0420 22:44:13.506731 139904526645120 squad_lib.py:369] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0420 22:44:13.506985 139904526645120 squad_lib.py:370] segment_ids: 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0420 22:44:13.510120 139904526645120 squad_lib.py:353] *** Example ***\n",
            "I0420 22:44:13.510209 139904526645120 squad_lib.py:354] unique_id: 1000000003\n",
            "I0420 22:44:13.510276 139904526645120 squad_lib.py:355] example_index: 3\n",
            "I0420 22:44:13.510332 139904526645120 squad_lib.py:356] doc_span_index: 0\n",
            "I0420 22:44:13.510452 139904526645120 squad_lib.py:358] tokens: [CLS] which nfl team won super bowl 50 ? [SEP] super bowl 50 was an american football game to determine the champion of the national football league ( nfl ) for the 2015 season . the american football conference ( afc ) champion denver broncos defeated the national football conference ( nfc ) champion carolina panthers 24 – 10 to earn their third super bowl title . the game was played on february 7 , 2016 , at levi ' s stadium in the san francisco bay area at santa clara , california . as this was the 50th super bowl , the league emphasized the \" golden anniversary \" with various gold - themed initiatives , as well as temporarily suspend ##ing the tradition of naming each super bowl game with roman nu ##meral ##s ( under which the game would have been known as \" super bowl l \" ) , so that the logo could prominently feature the arabic nu ##meral ##s 50 . [SEP]\n",
            "I0420 22:44:13.510584 139904526645120 squad_lib.py:361] token_to_orig_map: 10:0 11:1 12:2 13:3 14:4 15:5 16:6 17:7 18:8 19:9 20:10 21:11 22:12 23:13 24:14 25:15 26:16 27:17 28:17 29:17 30:18 31:19 32:20 33:21 34:21 35:22 36:23 37:24 38:25 39:26 40:26 41:26 42:27 43:28 44:29 45:30 46:31 47:32 48:33 49:34 50:35 51:35 52:35 53:36 54:37 55:38 56:39 57:39 58:39 59:40 60:41 61:42 62:43 63:44 64:45 65:46 66:46 67:47 68:48 69:49 70:50 71:51 72:52 73:53 74:53 75:54 76:54 77:55 78:56 79:56 80:56 81:57 82:58 83:59 84:60 85:61 86:62 87:63 88:64 89:65 90:66 91:66 92:67 93:67 94:68 95:69 96:70 97:71 98:72 99:73 100:74 101:74 102:75 103:76 104:77 105:78 106:79 107:79 108:80 109:80 110:81 111:82 112:83 113:83 114:83 115:84 116:84 117:85 118:86 119:87 120:88 121:89 122:89 123:90 124:91 125:92 126:93 127:94 128:95 129:96 130:97 131:98 132:99 133:100 134:100 135:100 136:101 137:101 138:102 139:103 140:104 141:105 142:106 143:107 144:108 145:109 146:110 147:110 148:111 149:112 150:112 151:112 152:112 153:113 154:114 155:115 156:116 157:117 158:118 159:119 160:120 161:121 162:122 163:122 164:122 165:123 166:123\n",
            "I0420 22:44:13.510702 139904526645120 squad_lib.py:366] token_is_max_context: 10:True 11:True 12:True 13:True 14:True 15:True 16:True 17:True 18:True 19:True 20:True 21:True 22:True 23:True 24:True 25:True 26:True 27:True 28:True 29:True 30:True 31:True 32:True 33:True 34:True 35:True 36:True 37:True 38:True 39:True 40:True 41:True 42:True 43:True 44:True 45:True 46:True 47:True 48:True 49:True 50:True 51:True 52:True 53:True 54:True 55:True 56:True 57:True 58:True 59:True 60:True 61:True 62:True 63:True 64:True 65:True 66:True 67:True 68:True 69:True 70:True 71:True 72:True 73:True 74:True 75:True 76:True 77:True 78:True 79:True 80:True 81:True 82:True 83:True 84:True 85:True 86:True 87:True 88:True 89:True 90:True 91:True 92:True 93:True 94:True 95:True 96:True 97:True 98:True 99:True 100:True 101:True 102:True 103:True 104:True 105:True 106:True 107:True 108:True 109:True 110:True 111:True 112:True 113:True 114:True 115:True 116:True 117:True 118:True 119:True 120:True 121:True 122:True 123:True 124:True 125:True 126:True 127:True 128:True 129:True 130:True 131:True 132:True 133:True 134:True 135:True 136:True 137:True 138:True 139:True 140:True 141:True 142:True 143:True 144:True 145:True 146:True 147:True 148:True 149:True 150:True 151:True 152:True 153:True 154:True 155:True 156:True 157:True 158:True 159:True 160:True 161:True 162:True 163:True 164:True 165:True 166:True\n",
            "I0420 22:44:13.510873 139904526645120 squad_lib.py:368] input_ids: 101 2029 5088 2136 2180 3565 4605 2753 1029 102 3565 4605 2753 2001 2019 2137 2374 2208 2000 5646 1996 3410 1997 1996 2120 2374 2223 1006 5088 1007 2005 1996 2325 2161 1012 1996 2137 2374 3034 1006 10511 1007 3410 7573 14169 3249 1996 2120 2374 3034 1006 22309 1007 3410 3792 12915 2484 1516 2184 2000 7796 2037 2353 3565 4605 2516 1012 1996 2208 2001 2209 2006 2337 1021 1010 2355 1010 2012 11902 1005 1055 3346 1999 1996 2624 3799 3016 2181 2012 4203 10254 1010 2662 1012 2004 2023 2001 1996 12951 3565 4605 1010 1996 2223 13155 1996 1000 3585 5315 1000 2007 2536 2751 1011 11773 11107 1010 2004 2092 2004 8184 28324 2075 1996 4535 1997 10324 2169 3565 4605 2208 2007 3142 16371 28990 2015 1006 2104 2029 1996 2208 2052 2031 2042 2124 2004 1000 3565 4605 1048 1000 1007 1010 2061 2008 1996 8154 2071 14500 3444 1996 5640 16371 28990 2015 2753 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0420 22:44:13.511035 139904526645120 squad_lib.py:369] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0420 22:44:13.511191 139904526645120 squad_lib.py:370] segment_ids: 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0420 22:44:13.514626 139904526645120 squad_lib.py:353] *** Example ***\n",
            "I0420 22:44:13.514714 139904526645120 squad_lib.py:354] unique_id: 1000000004\n",
            "I0420 22:44:13.514781 139904526645120 squad_lib.py:355] example_index: 4\n",
            "I0420 22:44:13.514837 139904526645120 squad_lib.py:356] doc_span_index: 0\n",
            "I0420 22:44:13.514957 139904526645120 squad_lib.py:358] tokens: [CLS] what color was used to emphasize the 50th anniversary of the super bowl ? [SEP] super bowl 50 was an american football game to determine the champion of the national football league ( nfl ) for the 2015 season . the american football conference ( afc ) champion denver broncos defeated the national football conference ( nfc ) champion carolina panthers 24 – 10 to earn their third super bowl title . the game was played on february 7 , 2016 , at levi ' s stadium in the san francisco bay area at santa clara , california . as this was the 50th super bowl , the league emphasized the \" golden anniversary \" with various gold - themed initiatives , as well as temporarily suspend ##ing the tradition of naming each super bowl game with roman nu ##meral ##s ( under which the game would have been known as \" super bowl l \" ) , so that the logo could prominently feature the arabic nu ##meral ##s 50 . [SEP]\n",
            "I0420 22:44:13.515090 139904526645120 squad_lib.py:361] token_to_orig_map: 16:0 17:1 18:2 19:3 20:4 21:5 22:6 23:7 24:8 25:9 26:10 27:11 28:12 29:13 30:14 31:15 32:16 33:17 34:17 35:17 36:18 37:19 38:20 39:21 40:21 41:22 42:23 43:24 44:25 45:26 46:26 47:26 48:27 49:28 50:29 51:30 52:31 53:32 54:33 55:34 56:35 57:35 58:35 59:36 60:37 61:38 62:39 63:39 64:39 65:40 66:41 67:42 68:43 69:44 70:45 71:46 72:46 73:47 74:48 75:49 76:50 77:51 78:52 79:53 80:53 81:54 82:54 83:55 84:56 85:56 86:56 87:57 88:58 89:59 90:60 91:61 92:62 93:63 94:64 95:65 96:66 97:66 98:67 99:67 100:68 101:69 102:70 103:71 104:72 105:73 106:74 107:74 108:75 109:76 110:77 111:78 112:79 113:79 114:80 115:80 116:81 117:82 118:83 119:83 120:83 121:84 122:84 123:85 124:86 125:87 126:88 127:89 128:89 129:90 130:91 131:92 132:93 133:94 134:95 135:96 136:97 137:98 138:99 139:100 140:100 141:100 142:101 143:101 144:102 145:103 146:104 147:105 148:106 149:107 150:108 151:109 152:110 153:110 154:111 155:112 156:112 157:112 158:112 159:113 160:114 161:115 162:116 163:117 164:118 165:119 166:120 167:121 168:122 169:122 170:122 171:123 172:123\n",
            "I0420 22:44:13.515204 139904526645120 squad_lib.py:366] token_is_max_context: 16:True 17:True 18:True 19:True 20:True 21:True 22:True 23:True 24:True 25:True 26:True 27:True 28:True 29:True 30:True 31:True 32:True 33:True 34:True 35:True 36:True 37:True 38:True 39:True 40:True 41:True 42:True 43:True 44:True 45:True 46:True 47:True 48:True 49:True 50:True 51:True 52:True 53:True 54:True 55:True 56:True 57:True 58:True 59:True 60:True 61:True 62:True 63:True 64:True 65:True 66:True 67:True 68:True 69:True 70:True 71:True 72:True 73:True 74:True 75:True 76:True 77:True 78:True 79:True 80:True 81:True 82:True 83:True 84:True 85:True 86:True 87:True 88:True 89:True 90:True 91:True 92:True 93:True 94:True 95:True 96:True 97:True 98:True 99:True 100:True 101:True 102:True 103:True 104:True 105:True 106:True 107:True 108:True 109:True 110:True 111:True 112:True 113:True 114:True 115:True 116:True 117:True 118:True 119:True 120:True 121:True 122:True 123:True 124:True 125:True 126:True 127:True 128:True 129:True 130:True 131:True 132:True 133:True 134:True 135:True 136:True 137:True 138:True 139:True 140:True 141:True 142:True 143:True 144:True 145:True 146:True 147:True 148:True 149:True 150:True 151:True 152:True 153:True 154:True 155:True 156:True 157:True 158:True 159:True 160:True 161:True 162:True 163:True 164:True 165:True 166:True 167:True 168:True 169:True 170:True 171:True 172:True\n",
            "I0420 22:44:13.515366 139904526645120 squad_lib.py:368] input_ids: 101 2054 3609 2001 2109 2000 17902 1996 12951 5315 1997 1996 3565 4605 1029 102 3565 4605 2753 2001 2019 2137 2374 2208 2000 5646 1996 3410 1997 1996 2120 2374 2223 1006 5088 1007 2005 1996 2325 2161 1012 1996 2137 2374 3034 1006 10511 1007 3410 7573 14169 3249 1996 2120 2374 3034 1006 22309 1007 3410 3792 12915 2484 1516 2184 2000 7796 2037 2353 3565 4605 2516 1012 1996 2208 2001 2209 2006 2337 1021 1010 2355 1010 2012 11902 1005 1055 3346 1999 1996 2624 3799 3016 2181 2012 4203 10254 1010 2662 1012 2004 2023 2001 1996 12951 3565 4605 1010 1996 2223 13155 1996 1000 3585 5315 1000 2007 2536 2751 1011 11773 11107 1010 2004 2092 2004 8184 28324 2075 1996 4535 1997 10324 2169 3565 4605 2208 2007 3142 16371 28990 2015 1006 2104 2029 1996 2208 2052 2031 2042 2124 2004 1000 3565 4605 1048 1000 1007 1010 2061 2008 1996 8154 2071 14500 3444 1996 5640 16371 28990 2015 2753 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0420 22:44:13.515526 139904526645120 squad_lib.py:369] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0420 22:44:13.515691 139904526645120 squad_lib.py:370] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0420 22:44:13.803656 139904526645120 squad_lib.py:353] *** Example ***\n",
            "I0420 22:44:13.803828 139904526645120 squad_lib.py:354] unique_id: 1000000005\n",
            "I0420 22:44:13.803914 139904526645120 squad_lib.py:355] example_index: 5\n",
            "I0420 22:44:13.803976 139904526645120 squad_lib.py:356] doc_span_index: 0\n",
            "I0420 22:44:13.804112 139904526645120 squad_lib.py:358] tokens: [CLS] what was the theme of super bowl 50 ? [SEP] super bowl 50 was an american football game to determine the champion of the national football league ( nfl ) for the 2015 season . the american football conference ( afc ) champion denver broncos defeated the national football conference ( nfc ) champion carolina panthers 24 – 10 to earn their third super bowl title . the game was played on february 7 , 2016 , at levi ' s stadium in the san francisco bay area at santa clara , california . as this was the 50th super bowl , the league emphasized the \" golden anniversary \" with various gold - themed initiatives , as well as temporarily suspend ##ing the tradition of naming each super bowl game with roman nu ##meral ##s ( under which the game would have been known as \" super bowl l \" ) , so that the logo could prominently feature the arabic nu ##meral ##s 50 . [SEP]\n",
            "I0420 22:44:13.804251 139904526645120 squad_lib.py:361] token_to_orig_map: 11:0 12:1 13:2 14:3 15:4 16:5 17:6 18:7 19:8 20:9 21:10 22:11 23:12 24:13 25:14 26:15 27:16 28:17 29:17 30:17 31:18 32:19 33:20 34:21 35:21 36:22 37:23 38:24 39:25 40:26 41:26 42:26 43:27 44:28 45:29 46:30 47:31 48:32 49:33 50:34 51:35 52:35 53:35 54:36 55:37 56:38 57:39 58:39 59:39 60:40 61:41 62:42 63:43 64:44 65:45 66:46 67:46 68:47 69:48 70:49 71:50 72:51 73:52 74:53 75:53 76:54 77:54 78:55 79:56 80:56 81:56 82:57 83:58 84:59 85:60 86:61 87:62 88:63 89:64 90:65 91:66 92:66 93:67 94:67 95:68 96:69 97:70 98:71 99:72 100:73 101:74 102:74 103:75 104:76 105:77 106:78 107:79 108:79 109:80 110:80 111:81 112:82 113:83 114:83 115:83 116:84 117:84 118:85 119:86 120:87 121:88 122:89 123:89 124:90 125:91 126:92 127:93 128:94 129:95 130:96 131:97 132:98 133:99 134:100 135:100 136:100 137:101 138:101 139:102 140:103 141:104 142:105 143:106 144:107 145:108 146:109 147:110 148:110 149:111 150:112 151:112 152:112 153:112 154:113 155:114 156:115 157:116 158:117 159:118 160:119 161:120 162:121 163:122 164:122 165:122 166:123 167:123\n",
            "I0420 22:44:13.804379 139904526645120 squad_lib.py:366] token_is_max_context: 11:True 12:True 13:True 14:True 15:True 16:True 17:True 18:True 19:True 20:True 21:True 22:True 23:True 24:True 25:True 26:True 27:True 28:True 29:True 30:True 31:True 32:True 33:True 34:True 35:True 36:True 37:True 38:True 39:True 40:True 41:True 42:True 43:True 44:True 45:True 46:True 47:True 48:True 49:True 50:True 51:True 52:True 53:True 54:True 55:True 56:True 57:True 58:True 59:True 60:True 61:True 62:True 63:True 64:True 65:True 66:True 67:True 68:True 69:True 70:True 71:True 72:True 73:True 74:True 75:True 76:True 77:True 78:True 79:True 80:True 81:True 82:True 83:True 84:True 85:True 86:True 87:True 88:True 89:True 90:True 91:True 92:True 93:True 94:True 95:True 96:True 97:True 98:True 99:True 100:True 101:True 102:True 103:True 104:True 105:True 106:True 107:True 108:True 109:True 110:True 111:True 112:True 113:True 114:True 115:True 116:True 117:True 118:True 119:True 120:True 121:True 122:True 123:True 124:True 125:True 126:True 127:True 128:True 129:True 130:True 131:True 132:True 133:True 134:True 135:True 136:True 137:True 138:True 139:True 140:True 141:True 142:True 143:True 144:True 145:True 146:True 147:True 148:True 149:True 150:True 151:True 152:True 153:True 154:True 155:True 156:True 157:True 158:True 159:True 160:True 161:True 162:True 163:True 164:True 165:True 166:True 167:True\n",
            "I0420 22:44:13.804559 139904526645120 squad_lib.py:368] input_ids: 101 2054 2001 1996 4323 1997 3565 4605 2753 1029 102 3565 4605 2753 2001 2019 2137 2374 2208 2000 5646 1996 3410 1997 1996 2120 2374 2223 1006 5088 1007 2005 1996 2325 2161 1012 1996 2137 2374 3034 1006 10511 1007 3410 7573 14169 3249 1996 2120 2374 3034 1006 22309 1007 3410 3792 12915 2484 1516 2184 2000 7796 2037 2353 3565 4605 2516 1012 1996 2208 2001 2209 2006 2337 1021 1010 2355 1010 2012 11902 1005 1055 3346 1999 1996 2624 3799 3016 2181 2012 4203 10254 1010 2662 1012 2004 2023 2001 1996 12951 3565 4605 1010 1996 2223 13155 1996 1000 3585 5315 1000 2007 2536 2751 1011 11773 11107 1010 2004 2092 2004 8184 28324 2075 1996 4535 1997 10324 2169 3565 4605 2208 2007 3142 16371 28990 2015 1006 2104 2029 1996 2208 2052 2031 2042 2124 2004 1000 3565 4605 1048 1000 1007 1010 2061 2008 1996 8154 2071 14500 3444 1996 5640 16371 28990 2015 2753 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0420 22:44:13.804748 139904526645120 squad_lib.py:369] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0420 22:44:13.804913 139904526645120 squad_lib.py:370] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0420 22:44:13.808435 139904526645120 squad_lib.py:353] *** Example ***\n",
            "I0420 22:44:13.808553 139904526645120 squad_lib.py:354] unique_id: 1000000006\n",
            "I0420 22:44:13.808647 139904526645120 squad_lib.py:355] example_index: 6\n",
            "I0420 22:44:13.808716 139904526645120 squad_lib.py:356] doc_span_index: 0\n",
            "I0420 22:44:13.808842 139904526645120 squad_lib.py:358] tokens: [CLS] what day was the game played on ? [SEP] super bowl 50 was an american football game to determine the champion of the national football league ( nfl ) for the 2015 season . the american football conference ( afc ) champion denver broncos defeated the national football conference ( nfc ) champion carolina panthers 24 – 10 to earn their third super bowl title . the game was played on february 7 , 2016 , at levi ' s stadium in the san francisco bay area at santa clara , california . as this was the 50th super bowl , the league emphasized the \" golden anniversary \" with various gold - themed initiatives , as well as temporarily suspend ##ing the tradition of naming each super bowl game with roman nu ##meral ##s ( under which the game would have been known as \" super bowl l \" ) , so that the logo could prominently feature the arabic nu ##meral ##s 50 . [SEP]\n",
            "I0420 22:44:13.808972 139904526645120 squad_lib.py:361] token_to_orig_map: 10:0 11:1 12:2 13:3 14:4 15:5 16:6 17:7 18:8 19:9 20:10 21:11 22:12 23:13 24:14 25:15 26:16 27:17 28:17 29:17 30:18 31:19 32:20 33:21 34:21 35:22 36:23 37:24 38:25 39:26 40:26 41:26 42:27 43:28 44:29 45:30 46:31 47:32 48:33 49:34 50:35 51:35 52:35 53:36 54:37 55:38 56:39 57:39 58:39 59:40 60:41 61:42 62:43 63:44 64:45 65:46 66:46 67:47 68:48 69:49 70:50 71:51 72:52 73:53 74:53 75:54 76:54 77:55 78:56 79:56 80:56 81:57 82:58 83:59 84:60 85:61 86:62 87:63 88:64 89:65 90:66 91:66 92:67 93:67 94:68 95:69 96:70 97:71 98:72 99:73 100:74 101:74 102:75 103:76 104:77 105:78 106:79 107:79 108:80 109:80 110:81 111:82 112:83 113:83 114:83 115:84 116:84 117:85 118:86 119:87 120:88 121:89 122:89 123:90 124:91 125:92 126:93 127:94 128:95 129:96 130:97 131:98 132:99 133:100 134:100 135:100 136:101 137:101 138:102 139:103 140:104 141:105 142:106 143:107 144:108 145:109 146:110 147:110 148:111 149:112 150:112 151:112 152:112 153:113 154:114 155:115 156:116 157:117 158:118 159:119 160:120 161:121 162:122 163:122 164:122 165:123 166:123\n",
            "I0420 22:44:13.809110 139904526645120 squad_lib.py:366] token_is_max_context: 10:True 11:True 12:True 13:True 14:True 15:True 16:True 17:True 18:True 19:True 20:True 21:True 22:True 23:True 24:True 25:True 26:True 27:True 28:True 29:True 30:True 31:True 32:True 33:True 34:True 35:True 36:True 37:True 38:True 39:True 40:True 41:True 42:True 43:True 44:True 45:True 46:True 47:True 48:True 49:True 50:True 51:True 52:True 53:True 54:True 55:True 56:True 57:True 58:True 59:True 60:True 61:True 62:True 63:True 64:True 65:True 66:True 67:True 68:True 69:True 70:True 71:True 72:True 73:True 74:True 75:True 76:True 77:True 78:True 79:True 80:True 81:True 82:True 83:True 84:True 85:True 86:True 87:True 88:True 89:True 90:True 91:True 92:True 93:True 94:True 95:True 96:True 97:True 98:True 99:True 100:True 101:True 102:True 103:True 104:True 105:True 106:True 107:True 108:True 109:True 110:True 111:True 112:True 113:True 114:True 115:True 116:True 117:True 118:True 119:True 120:True 121:True 122:True 123:True 124:True 125:True 126:True 127:True 128:True 129:True 130:True 131:True 132:True 133:True 134:True 135:True 136:True 137:True 138:True 139:True 140:True 141:True 142:True 143:True 144:True 145:True 146:True 147:True 148:True 149:True 150:True 151:True 152:True 153:True 154:True 155:True 156:True 157:True 158:True 159:True 160:True 161:True 162:True 163:True 164:True 165:True 166:True\n",
            "I0420 22:44:13.809286 139904526645120 squad_lib.py:368] input_ids: 101 2054 2154 2001 1996 2208 2209 2006 1029 102 3565 4605 2753 2001 2019 2137 2374 2208 2000 5646 1996 3410 1997 1996 2120 2374 2223 1006 5088 1007 2005 1996 2325 2161 1012 1996 2137 2374 3034 1006 10511 1007 3410 7573 14169 3249 1996 2120 2374 3034 1006 22309 1007 3410 3792 12915 2484 1516 2184 2000 7796 2037 2353 3565 4605 2516 1012 1996 2208 2001 2209 2006 2337 1021 1010 2355 1010 2012 11902 1005 1055 3346 1999 1996 2624 3799 3016 2181 2012 4203 10254 1010 2662 1012 2004 2023 2001 1996 12951 3565 4605 1010 1996 2223 13155 1996 1000 3585 5315 1000 2007 2536 2751 1011 11773 11107 1010 2004 2092 2004 8184 28324 2075 1996 4535 1997 10324 2169 3565 4605 2208 2007 3142 16371 28990 2015 1006 2104 2029 1996 2208 2052 2031 2042 2124 2004 1000 3565 4605 1048 1000 1007 1010 2061 2008 1996 8154 2071 14500 3444 1996 5640 16371 28990 2015 2753 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0420 22:44:13.809464 139904526645120 squad_lib.py:369] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0420 22:44:13.809660 139904526645120 squad_lib.py:370] segment_ids: 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0420 22:44:13.812810 139904526645120 squad_lib.py:353] *** Example ***\n",
            "I0420 22:44:13.812905 139904526645120 squad_lib.py:354] unique_id: 1000000007\n",
            "I0420 22:44:13.812971 139904526645120 squad_lib.py:355] example_index: 7\n",
            "I0420 22:44:13.813031 139904526645120 squad_lib.py:356] doc_span_index: 0\n",
            "I0420 22:44:13.813150 139904526645120 squad_lib.py:358] tokens: [CLS] what is the afc short for ? [SEP] super bowl 50 was an american football game to determine the champion of the national football league ( nfl ) for the 2015 season . the american football conference ( afc ) champion denver broncos defeated the national football conference ( nfc ) champion carolina panthers 24 – 10 to earn their third super bowl title . the game was played on february 7 , 2016 , at levi ' s stadium in the san francisco bay area at santa clara , california . as this was the 50th super bowl , the league emphasized the \" golden anniversary \" with various gold - themed initiatives , as well as temporarily suspend ##ing the tradition of naming each super bowl game with roman nu ##meral ##s ( under which the game would have been known as \" super bowl l \" ) , so that the logo could prominently feature the arabic nu ##meral ##s 50 . [SEP]\n",
            "I0420 22:44:13.813269 139904526645120 squad_lib.py:361] token_to_orig_map: 9:0 10:1 11:2 12:3 13:4 14:5 15:6 16:7 17:8 18:9 19:10 20:11 21:12 22:13 23:14 24:15 25:16 26:17 27:17 28:17 29:18 30:19 31:20 32:21 33:21 34:22 35:23 36:24 37:25 38:26 39:26 40:26 41:27 42:28 43:29 44:30 45:31 46:32 47:33 48:34 49:35 50:35 51:35 52:36 53:37 54:38 55:39 56:39 57:39 58:40 59:41 60:42 61:43 62:44 63:45 64:46 65:46 66:47 67:48 68:49 69:50 70:51 71:52 72:53 73:53 74:54 75:54 76:55 77:56 78:56 79:56 80:57 81:58 82:59 83:60 84:61 85:62 86:63 87:64 88:65 89:66 90:66 91:67 92:67 93:68 94:69 95:70 96:71 97:72 98:73 99:74 100:74 101:75 102:76 103:77 104:78 105:79 106:79 107:80 108:80 109:81 110:82 111:83 112:83 113:83 114:84 115:84 116:85 117:86 118:87 119:88 120:89 121:89 122:90 123:91 124:92 125:93 126:94 127:95 128:96 129:97 130:98 131:99 132:100 133:100 134:100 135:101 136:101 137:102 138:103 139:104 140:105 141:106 142:107 143:108 144:109 145:110 146:110 147:111 148:112 149:112 150:112 151:112 152:113 153:114 154:115 155:116 156:117 157:118 158:119 159:120 160:121 161:122 162:122 163:122 164:123 165:123\n",
            "I0420 22:44:13.813383 139904526645120 squad_lib.py:366] token_is_max_context: 9:True 10:True 11:True 12:True 13:True 14:True 15:True 16:True 17:True 18:True 19:True 20:True 21:True 22:True 23:True 24:True 25:True 26:True 27:True 28:True 29:True 30:True 31:True 32:True 33:True 34:True 35:True 36:True 37:True 38:True 39:True 40:True 41:True 42:True 43:True 44:True 45:True 46:True 47:True 48:True 49:True 50:True 51:True 52:True 53:True 54:True 55:True 56:True 57:True 58:True 59:True 60:True 61:True 62:True 63:True 64:True 65:True 66:True 67:True 68:True 69:True 70:True 71:True 72:True 73:True 74:True 75:True 76:True 77:True 78:True 79:True 80:True 81:True 82:True 83:True 84:True 85:True 86:True 87:True 88:True 89:True 90:True 91:True 92:True 93:True 94:True 95:True 96:True 97:True 98:True 99:True 100:True 101:True 102:True 103:True 104:True 105:True 106:True 107:True 108:True 109:True 110:True 111:True 112:True 113:True 114:True 115:True 116:True 117:True 118:True 119:True 120:True 121:True 122:True 123:True 124:True 125:True 126:True 127:True 128:True 129:True 130:True 131:True 132:True 133:True 134:True 135:True 136:True 137:True 138:True 139:True 140:True 141:True 142:True 143:True 144:True 145:True 146:True 147:True 148:True 149:True 150:True 151:True 152:True 153:True 154:True 155:True 156:True 157:True 158:True 159:True 160:True 161:True 162:True 163:True 164:True 165:True\n",
            "I0420 22:44:13.813542 139904526645120 squad_lib.py:368] input_ids: 101 2054 2003 1996 10511 2460 2005 1029 102 3565 4605 2753 2001 2019 2137 2374 2208 2000 5646 1996 3410 1997 1996 2120 2374 2223 1006 5088 1007 2005 1996 2325 2161 1012 1996 2137 2374 3034 1006 10511 1007 3410 7573 14169 3249 1996 2120 2374 3034 1006 22309 1007 3410 3792 12915 2484 1516 2184 2000 7796 2037 2353 3565 4605 2516 1012 1996 2208 2001 2209 2006 2337 1021 1010 2355 1010 2012 11902 1005 1055 3346 1999 1996 2624 3799 3016 2181 2012 4203 10254 1010 2662 1012 2004 2023 2001 1996 12951 3565 4605 1010 1996 2223 13155 1996 1000 3585 5315 1000 2007 2536 2751 1011 11773 11107 1010 2004 2092 2004 8184 28324 2075 1996 4535 1997 10324 2169 3565 4605 2208 2007 3142 16371 28990 2015 1006 2104 2029 1996 2208 2052 2031 2042 2124 2004 1000 3565 4605 1048 1000 1007 1010 2061 2008 1996 8154 2071 14500 3444 1996 5640 16371 28990 2015 2753 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0420 22:44:13.813719 139904526645120 squad_lib.py:369] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0420 22:44:13.813879 139904526645120 squad_lib.py:370] segment_ids: 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0420 22:44:13.817071 139904526645120 squad_lib.py:353] *** Example ***\n",
            "I0420 22:44:13.817165 139904526645120 squad_lib.py:354] unique_id: 1000000008\n",
            "I0420 22:44:13.817237 139904526645120 squad_lib.py:355] example_index: 8\n",
            "I0420 22:44:13.817299 139904526645120 squad_lib.py:356] doc_span_index: 0\n",
            "I0420 22:44:13.817423 139904526645120 squad_lib.py:358] tokens: [CLS] what was the theme of super bowl 50 ? [SEP] super bowl 50 was an american football game to determine the champion of the national football league ( nfl ) for the 2015 season . the american football conference ( afc ) champion denver broncos defeated the national football conference ( nfc ) champion carolina panthers 24 – 10 to earn their third super bowl title . the game was played on february 7 , 2016 , at levi ' s stadium in the san francisco bay area at santa clara , california . as this was the 50th super bowl , the league emphasized the \" golden anniversary \" with various gold - themed initiatives , as well as temporarily suspend ##ing the tradition of naming each super bowl game with roman nu ##meral ##s ( under which the game would have been known as \" super bowl l \" ) , so that the logo could prominently feature the arabic nu ##meral ##s 50 . [SEP]\n",
            "I0420 22:44:13.817542 139904526645120 squad_lib.py:361] token_to_orig_map: 11:0 12:1 13:2 14:3 15:4 16:5 17:6 18:7 19:8 20:9 21:10 22:11 23:12 24:13 25:14 26:15 27:16 28:17 29:17 30:17 31:18 32:19 33:20 34:21 35:21 36:22 37:23 38:24 39:25 40:26 41:26 42:26 43:27 44:28 45:29 46:30 47:31 48:32 49:33 50:34 51:35 52:35 53:35 54:36 55:37 56:38 57:39 58:39 59:39 60:40 61:41 62:42 63:43 64:44 65:45 66:46 67:46 68:47 69:48 70:49 71:50 72:51 73:52 74:53 75:53 76:54 77:54 78:55 79:56 80:56 81:56 82:57 83:58 84:59 85:60 86:61 87:62 88:63 89:64 90:65 91:66 92:66 93:67 94:67 95:68 96:69 97:70 98:71 99:72 100:73 101:74 102:74 103:75 104:76 105:77 106:78 107:79 108:79 109:80 110:80 111:81 112:82 113:83 114:83 115:83 116:84 117:84 118:85 119:86 120:87 121:88 122:89 123:89 124:90 125:91 126:92 127:93 128:94 129:95 130:96 131:97 132:98 133:99 134:100 135:100 136:100 137:101 138:101 139:102 140:103 141:104 142:105 143:106 144:107 145:108 146:109 147:110 148:110 149:111 150:112 151:112 152:112 153:112 154:113 155:114 156:115 157:116 158:117 159:118 160:119 161:120 162:121 163:122 164:122 165:122 166:123 167:123\n",
            "I0420 22:44:13.908818 139904526645120 squad_lib.py:366] token_is_max_context: 11:True 12:True 13:True 14:True 15:True 16:True 17:True 18:True 19:True 20:True 21:True 22:True 23:True 24:True 25:True 26:True 27:True 28:True 29:True 30:True 31:True 32:True 33:True 34:True 35:True 36:True 37:True 38:True 39:True 40:True 41:True 42:True 43:True 44:True 45:True 46:True 47:True 48:True 49:True 50:True 51:True 52:True 53:True 54:True 55:True 56:True 57:True 58:True 59:True 60:True 61:True 62:True 63:True 64:True 65:True 66:True 67:True 68:True 69:True 70:True 71:True 72:True 73:True 74:True 75:True 76:True 77:True 78:True 79:True 80:True 81:True 82:True 83:True 84:True 85:True 86:True 87:True 88:True 89:True 90:True 91:True 92:True 93:True 94:True 95:True 96:True 97:True 98:True 99:True 100:True 101:True 102:True 103:True 104:True 105:True 106:True 107:True 108:True 109:True 110:True 111:True 112:True 113:True 114:True 115:True 116:True 117:True 118:True 119:True 120:True 121:True 122:True 123:True 124:True 125:True 126:True 127:True 128:True 129:True 130:True 131:True 132:True 133:True 134:True 135:True 136:True 137:True 138:True 139:True 140:True 141:True 142:True 143:True 144:True 145:True 146:True 147:True 148:True 149:True 150:True 151:True 152:True 153:True 154:True 155:True 156:True 157:True 158:True 159:True 160:True 161:True 162:True 163:True 164:True 165:True 166:True 167:True\n",
            "I0420 22:44:13.909113 139904526645120 squad_lib.py:368] input_ids: 101 2054 2001 1996 4323 1997 3565 4605 2753 1029 102 3565 4605 2753 2001 2019 2137 2374 2208 2000 5646 1996 3410 1997 1996 2120 2374 2223 1006 5088 1007 2005 1996 2325 2161 1012 1996 2137 2374 3034 1006 10511 1007 3410 7573 14169 3249 1996 2120 2374 3034 1006 22309 1007 3410 3792 12915 2484 1516 2184 2000 7796 2037 2353 3565 4605 2516 1012 1996 2208 2001 2209 2006 2337 1021 1010 2355 1010 2012 11902 1005 1055 3346 1999 1996 2624 3799 3016 2181 2012 4203 10254 1010 2662 1012 2004 2023 2001 1996 12951 3565 4605 1010 1996 2223 13155 1996 1000 3585 5315 1000 2007 2536 2751 1011 11773 11107 1010 2004 2092 2004 8184 28324 2075 1996 4535 1997 10324 2169 3565 4605 2208 2007 3142 16371 28990 2015 1006 2104 2029 1996 2208 2052 2031 2042 2124 2004 1000 3565 4605 1048 1000 1007 1010 2061 2008 1996 8154 2071 14500 3444 1996 5640 16371 28990 2015 2753 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0420 22:44:13.909365 139904526645120 squad_lib.py:369] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0420 22:44:13.909552 139904526645120 squad_lib.py:370] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0420 22:44:13.912856 139904526645120 squad_lib.py:353] *** Example ***\n",
            "I0420 22:44:13.912946 139904526645120 squad_lib.py:354] unique_id: 1000000009\n",
            "I0420 22:44:13.913015 139904526645120 squad_lib.py:355] example_index: 9\n",
            "I0420 22:44:13.913071 139904526645120 squad_lib.py:356] doc_span_index: 0\n",
            "I0420 22:44:13.913192 139904526645120 squad_lib.py:358] tokens: [CLS] what does afc stand for ? [SEP] super bowl 50 was an american football game to determine the champion of the national football league ( nfl ) for the 2015 season . the american football conference ( afc ) champion denver broncos defeated the national football conference ( nfc ) champion carolina panthers 24 – 10 to earn their third super bowl title . the game was played on february 7 , 2016 , at levi ' s stadium in the san francisco bay area at santa clara , california . as this was the 50th super bowl , the league emphasized the \" golden anniversary \" with various gold - themed initiatives , as well as temporarily suspend ##ing the tradition of naming each super bowl game with roman nu ##meral ##s ( under which the game would have been known as \" super bowl l \" ) , so that the logo could prominently feature the arabic nu ##meral ##s 50 . [SEP]\n",
            "I0420 22:44:13.913314 139904526645120 squad_lib.py:361] token_to_orig_map: 8:0 9:1 10:2 11:3 12:4 13:5 14:6 15:7 16:8 17:9 18:10 19:11 20:12 21:13 22:14 23:15 24:16 25:17 26:17 27:17 28:18 29:19 30:20 31:21 32:21 33:22 34:23 35:24 36:25 37:26 38:26 39:26 40:27 41:28 42:29 43:30 44:31 45:32 46:33 47:34 48:35 49:35 50:35 51:36 52:37 53:38 54:39 55:39 56:39 57:40 58:41 59:42 60:43 61:44 62:45 63:46 64:46 65:47 66:48 67:49 68:50 69:51 70:52 71:53 72:53 73:54 74:54 75:55 76:56 77:56 78:56 79:57 80:58 81:59 82:60 83:61 84:62 85:63 86:64 87:65 88:66 89:66 90:67 91:67 92:68 93:69 94:70 95:71 96:72 97:73 98:74 99:74 100:75 101:76 102:77 103:78 104:79 105:79 106:80 107:80 108:81 109:82 110:83 111:83 112:83 113:84 114:84 115:85 116:86 117:87 118:88 119:89 120:89 121:90 122:91 123:92 124:93 125:94 126:95 127:96 128:97 129:98 130:99 131:100 132:100 133:100 134:101 135:101 136:102 137:103 138:104 139:105 140:106 141:107 142:108 143:109 144:110 145:110 146:111 147:112 148:112 149:112 150:112 151:113 152:114 153:115 154:116 155:117 156:118 157:119 158:120 159:121 160:122 161:122 162:122 163:123 164:123\n",
            "I0420 22:44:13.913429 139904526645120 squad_lib.py:366] token_is_max_context: 8:True 9:True 10:True 11:True 12:True 13:True 14:True 15:True 16:True 17:True 18:True 19:True 20:True 21:True 22:True 23:True 24:True 25:True 26:True 27:True 28:True 29:True 30:True 31:True 32:True 33:True 34:True 35:True 36:True 37:True 38:True 39:True 40:True 41:True 42:True 43:True 44:True 45:True 46:True 47:True 48:True 49:True 50:True 51:True 52:True 53:True 54:True 55:True 56:True 57:True 58:True 59:True 60:True 61:True 62:True 63:True 64:True 65:True 66:True 67:True 68:True 69:True 70:True 71:True 72:True 73:True 74:True 75:True 76:True 77:True 78:True 79:True 80:True 81:True 82:True 83:True 84:True 85:True 86:True 87:True 88:True 89:True 90:True 91:True 92:True 93:True 94:True 95:True 96:True 97:True 98:True 99:True 100:True 101:True 102:True 103:True 104:True 105:True 106:True 107:True 108:True 109:True 110:True 111:True 112:True 113:True 114:True 115:True 116:True 117:True 118:True 119:True 120:True 121:True 122:True 123:True 124:True 125:True 126:True 127:True 128:True 129:True 130:True 131:True 132:True 133:True 134:True 135:True 136:True 137:True 138:True 139:True 140:True 141:True 142:True 143:True 144:True 145:True 146:True 147:True 148:True 149:True 150:True 151:True 152:True 153:True 154:True 155:True 156:True 157:True 158:True 159:True 160:True 161:True 162:True 163:True 164:True\n",
            "I0420 22:44:13.913607 139904526645120 squad_lib.py:368] input_ids: 101 2054 2515 10511 3233 2005 1029 102 3565 4605 2753 2001 2019 2137 2374 2208 2000 5646 1996 3410 1997 1996 2120 2374 2223 1006 5088 1007 2005 1996 2325 2161 1012 1996 2137 2374 3034 1006 10511 1007 3410 7573 14169 3249 1996 2120 2374 3034 1006 22309 1007 3410 3792 12915 2484 1516 2184 2000 7796 2037 2353 3565 4605 2516 1012 1996 2208 2001 2209 2006 2337 1021 1010 2355 1010 2012 11902 1005 1055 3346 1999 1996 2624 3799 3016 2181 2012 4203 10254 1010 2662 1012 2004 2023 2001 1996 12951 3565 4605 1010 1996 2223 13155 1996 1000 3585 5315 1000 2007 2536 2751 1011 11773 11107 1010 2004 2092 2004 8184 28324 2075 1996 4535 1997 10324 2169 3565 4605 2208 2007 3142 16371 28990 2015 1006 2104 2029 1996 2208 2052 2031 2042 2124 2004 1000 3565 4605 1048 1000 1007 1010 2061 2008 1996 8154 2071 14500 3444 1996 5640 16371 28990 2015 2753 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0420 22:44:13.913772 139904526645120 squad_lib.py:369] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0420 22:44:13.913931 139904526645120 squad_lib.py:370] segment_ids: 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0420 22:44:13.917109 139904526645120 squad_lib.py:353] *** Example ***\n",
            "I0420 22:44:13.917202 139904526645120 squad_lib.py:354] unique_id: 1000000010\n",
            "I0420 22:44:13.917272 139904526645120 squad_lib.py:355] example_index: 10\n",
            "I0420 22:44:13.917339 139904526645120 squad_lib.py:356] doc_span_index: 0\n",
            "I0420 22:44:13.917464 139904526645120 squad_lib.py:358] tokens: [CLS] what day was the super bowl played on ? [SEP] super bowl 50 was an american football game to determine the champion of the national football league ( nfl ) for the 2015 season . the american football conference ( afc ) champion denver broncos defeated the national football conference ( nfc ) champion carolina panthers 24 – 10 to earn their third super bowl title . the game was played on february 7 , 2016 , at levi ' s stadium in the san francisco bay area at santa clara , california . as this was the 50th super bowl , the league emphasized the \" golden anniversary \" with various gold - themed initiatives , as well as temporarily suspend ##ing the tradition of naming each super bowl game with roman nu ##meral ##s ( under which the game would have been known as \" super bowl l \" ) , so that the logo could prominently feature the arabic nu ##meral ##s 50 . [SEP]\n",
            "I0420 22:44:13.917596 139904526645120 squad_lib.py:361] token_to_orig_map: 11:0 12:1 13:2 14:3 15:4 16:5 17:6 18:7 19:8 20:9 21:10 22:11 23:12 24:13 25:14 26:15 27:16 28:17 29:17 30:17 31:18 32:19 33:20 34:21 35:21 36:22 37:23 38:24 39:25 40:26 41:26 42:26 43:27 44:28 45:29 46:30 47:31 48:32 49:33 50:34 51:35 52:35 53:35 54:36 55:37 56:38 57:39 58:39 59:39 60:40 61:41 62:42 63:43 64:44 65:45 66:46 67:46 68:47 69:48 70:49 71:50 72:51 73:52 74:53 75:53 76:54 77:54 78:55 79:56 80:56 81:56 82:57 83:58 84:59 85:60 86:61 87:62 88:63 89:64 90:65 91:66 92:66 93:67 94:67 95:68 96:69 97:70 98:71 99:72 100:73 101:74 102:74 103:75 104:76 105:77 106:78 107:79 108:79 109:80 110:80 111:81 112:82 113:83 114:83 115:83 116:84 117:84 118:85 119:86 120:87 121:88 122:89 123:89 124:90 125:91 126:92 127:93 128:94 129:95 130:96 131:97 132:98 133:99 134:100 135:100 136:100 137:101 138:101 139:102 140:103 141:104 142:105 143:106 144:107 145:108 146:109 147:110 148:110 149:111 150:112 151:112 152:112 153:112 154:113 155:114 156:115 157:116 158:117 159:118 160:119 161:120 162:121 163:122 164:122 165:122 166:123 167:123\n",
            "I0420 22:44:13.917722 139904526645120 squad_lib.py:366] token_is_max_context: 11:True 12:True 13:True 14:True 15:True 16:True 17:True 18:True 19:True 20:True 21:True 22:True 23:True 24:True 25:True 26:True 27:True 28:True 29:True 30:True 31:True 32:True 33:True 34:True 35:True 36:True 37:True 38:True 39:True 40:True 41:True 42:True 43:True 44:True 45:True 46:True 47:True 48:True 49:True 50:True 51:True 52:True 53:True 54:True 55:True 56:True 57:True 58:True 59:True 60:True 61:True 62:True 63:True 64:True 65:True 66:True 67:True 68:True 69:True 70:True 71:True 72:True 73:True 74:True 75:True 76:True 77:True 78:True 79:True 80:True 81:True 82:True 83:True 84:True 85:True 86:True 87:True 88:True 89:True 90:True 91:True 92:True 93:True 94:True 95:True 96:True 97:True 98:True 99:True 100:True 101:True 102:True 103:True 104:True 105:True 106:True 107:True 108:True 109:True 110:True 111:True 112:True 113:True 114:True 115:True 116:True 117:True 118:True 119:True 120:True 121:True 122:True 123:True 124:True 125:True 126:True 127:True 128:True 129:True 130:True 131:True 132:True 133:True 134:True 135:True 136:True 137:True 138:True 139:True 140:True 141:True 142:True 143:True 144:True 145:True 146:True 147:True 148:True 149:True 150:True 151:True 152:True 153:True 154:True 155:True 156:True 157:True 158:True 159:True 160:True 161:True 162:True 163:True 164:True 165:True 166:True 167:True\n",
            "I0420 22:44:13.917891 139904526645120 squad_lib.py:368] input_ids: 101 2054 2154 2001 1996 3565 4605 2209 2006 1029 102 3565 4605 2753 2001 2019 2137 2374 2208 2000 5646 1996 3410 1997 1996 2120 2374 2223 1006 5088 1007 2005 1996 2325 2161 1012 1996 2137 2374 3034 1006 10511 1007 3410 7573 14169 3249 1996 2120 2374 3034 1006 22309 1007 3410 3792 12915 2484 1516 2184 2000 7796 2037 2353 3565 4605 2516 1012 1996 2208 2001 2209 2006 2337 1021 1010 2355 1010 2012 11902 1005 1055 3346 1999 1996 2624 3799 3016 2181 2012 4203 10254 1010 2662 1012 2004 2023 2001 1996 12951 3565 4605 1010 1996 2223 13155 1996 1000 3585 5315 1000 2007 2536 2751 1011 11773 11107 1010 2004 2092 2004 8184 28324 2075 1996 4535 1997 10324 2169 3565 4605 2208 2007 3142 16371 28990 2015 1006 2104 2029 1996 2208 2052 2031 2042 2124 2004 1000 3565 4605 1048 1000 1007 1010 2061 2008 1996 8154 2071 14500 3444 1996 5640 16371 28990 2015 2753 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0420 22:44:13.918056 139904526645120 squad_lib.py:369] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0420 22:44:13.918214 139904526645120 squad_lib.py:370] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0420 22:44:14.016716 139904526645120 squad_lib.py:353] *** Example ***\n",
            "I0420 22:44:14.016849 139904526645120 squad_lib.py:354] unique_id: 1000000011\n",
            "I0420 22:44:14.016937 139904526645120 squad_lib.py:355] example_index: 11\n",
            "I0420 22:44:14.017009 139904526645120 squad_lib.py:356] doc_span_index: 0\n",
            "I0420 22:44:14.017153 139904526645120 squad_lib.py:358] tokens: [CLS] who won super bowl 50 ? [SEP] super bowl 50 was an american football game to determine the champion of the national football league ( nfl ) for the 2015 season . the american football conference ( afc ) champion denver broncos defeated the national football conference ( nfc ) champion carolina panthers 24 – 10 to earn their third super bowl title . the game was played on february 7 , 2016 , at levi ' s stadium in the san francisco bay area at santa clara , california . as this was the 50th super bowl , the league emphasized the \" golden anniversary \" with various gold - themed initiatives , as well as temporarily suspend ##ing the tradition of naming each super bowl game with roman nu ##meral ##s ( under which the game would have been known as \" super bowl l \" ) , so that the logo could prominently feature the arabic nu ##meral ##s 50 . [SEP]\n",
            "I0420 22:44:14.017292 139904526645120 squad_lib.py:361] token_to_orig_map: 8:0 9:1 10:2 11:3 12:4 13:5 14:6 15:7 16:8 17:9 18:10 19:11 20:12 21:13 22:14 23:15 24:16 25:17 26:17 27:17 28:18 29:19 30:20 31:21 32:21 33:22 34:23 35:24 36:25 37:26 38:26 39:26 40:27 41:28 42:29 43:30 44:31 45:32 46:33 47:34 48:35 49:35 50:35 51:36 52:37 53:38 54:39 55:39 56:39 57:40 58:41 59:42 60:43 61:44 62:45 63:46 64:46 65:47 66:48 67:49 68:50 69:51 70:52 71:53 72:53 73:54 74:54 75:55 76:56 77:56 78:56 79:57 80:58 81:59 82:60 83:61 84:62 85:63 86:64 87:65 88:66 89:66 90:67 91:67 92:68 93:69 94:70 95:71 96:72 97:73 98:74 99:74 100:75 101:76 102:77 103:78 104:79 105:79 106:80 107:80 108:81 109:82 110:83 111:83 112:83 113:84 114:84 115:85 116:86 117:87 118:88 119:89 120:89 121:90 122:91 123:92 124:93 125:94 126:95 127:96 128:97 129:98 130:99 131:100 132:100 133:100 134:101 135:101 136:102 137:103 138:104 139:105 140:106 141:107 142:108 143:109 144:110 145:110 146:111 147:112 148:112 149:112 150:112 151:113 152:114 153:115 154:116 155:117 156:118 157:119 158:120 159:121 160:122 161:122 162:122 163:123 164:123\n",
            "I0420 22:44:14.017614 139904526645120 squad_lib.py:366] token_is_max_context: 8:True 9:True 10:True 11:True 12:True 13:True 14:True 15:True 16:True 17:True 18:True 19:True 20:True 21:True 22:True 23:True 24:True 25:True 26:True 27:True 28:True 29:True 30:True 31:True 32:True 33:True 34:True 35:True 36:True 37:True 38:True 39:True 40:True 41:True 42:True 43:True 44:True 45:True 46:True 47:True 48:True 49:True 50:True 51:True 52:True 53:True 54:True 55:True 56:True 57:True 58:True 59:True 60:True 61:True 62:True 63:True 64:True 65:True 66:True 67:True 68:True 69:True 70:True 71:True 72:True 73:True 74:True 75:True 76:True 77:True 78:True 79:True 80:True 81:True 82:True 83:True 84:True 85:True 86:True 87:True 88:True 89:True 90:True 91:True 92:True 93:True 94:True 95:True 96:True 97:True 98:True 99:True 100:True 101:True 102:True 103:True 104:True 105:True 106:True 107:True 108:True 109:True 110:True 111:True 112:True 113:True 114:True 115:True 116:True 117:True 118:True 119:True 120:True 121:True 122:True 123:True 124:True 125:True 126:True 127:True 128:True 129:True 130:True 131:True 132:True 133:True 134:True 135:True 136:True 137:True 138:True 139:True 140:True 141:True 142:True 143:True 144:True 145:True 146:True 147:True 148:True 149:True 150:True 151:True 152:True 153:True 154:True 155:True 156:True 157:True 158:True 159:True 160:True 161:True 162:True 163:True 164:True\n",
            "I0420 22:44:14.017870 139904526645120 squad_lib.py:368] input_ids: 101 2040 2180 3565 4605 2753 1029 102 3565 4605 2753 2001 2019 2137 2374 2208 2000 5646 1996 3410 1997 1996 2120 2374 2223 1006 5088 1007 2005 1996 2325 2161 1012 1996 2137 2374 3034 1006 10511 1007 3410 7573 14169 3249 1996 2120 2374 3034 1006 22309 1007 3410 3792 12915 2484 1516 2184 2000 7796 2037 2353 3565 4605 2516 1012 1996 2208 2001 2209 2006 2337 1021 1010 2355 1010 2012 11902 1005 1055 3346 1999 1996 2624 3799 3016 2181 2012 4203 10254 1010 2662 1012 2004 2023 2001 1996 12951 3565 4605 1010 1996 2223 13155 1996 1000 3585 5315 1000 2007 2536 2751 1011 11773 11107 1010 2004 2092 2004 8184 28324 2075 1996 4535 1997 10324 2169 3565 4605 2208 2007 3142 16371 28990 2015 1006 2104 2029 1996 2208 2052 2031 2042 2124 2004 1000 3565 4605 1048 1000 1007 1010 2061 2008 1996 8154 2071 14500 3444 1996 5640 16371 28990 2015 2753 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0420 22:44:14.018107 139904526645120 squad_lib.py:369] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0420 22:44:14.018320 139904526645120 squad_lib.py:370] segment_ids: 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0420 22:44:14.023202 139904526645120 squad_lib.py:353] *** Example ***\n",
            "I0420 22:44:14.023298 139904526645120 squad_lib.py:354] unique_id: 1000000012\n",
            "I0420 22:44:14.023362 139904526645120 squad_lib.py:355] example_index: 12\n",
            "I0420 22:44:14.023416 139904526645120 squad_lib.py:356] doc_span_index: 0\n",
            "I0420 22:44:14.023533 139904526645120 squad_lib.py:358] tokens: [CLS] what venue did super bowl 50 take place in ? [SEP] super bowl 50 was an american football game to determine the champion of the national football league ( nfl ) for the 2015 season . the american football conference ( afc ) champion denver broncos defeated the national football conference ( nfc ) champion carolina panthers 24 – 10 to earn their third super bowl title . the game was played on february 7 , 2016 , at levi ' s stadium in the san francisco bay area at santa clara , california . as this was the 50th super bowl , the league emphasized the \" golden anniversary \" with various gold - themed initiatives , as well as temporarily suspend ##ing the tradition of naming each super bowl game with roman nu ##meral ##s ( under which the game would have been known as \" super bowl l \" ) , so that the logo could prominently feature the arabic nu ##meral ##s 50 . [SEP]\n",
            "I0420 22:44:14.023694 139904526645120 squad_lib.py:361] token_to_orig_map: 12:0 13:1 14:2 15:3 16:4 17:5 18:6 19:7 20:8 21:9 22:10 23:11 24:12 25:13 26:14 27:15 28:16 29:17 30:17 31:17 32:18 33:19 34:20 35:21 36:21 37:22 38:23 39:24 40:25 41:26 42:26 43:26 44:27 45:28 46:29 47:30 48:31 49:32 50:33 51:34 52:35 53:35 54:35 55:36 56:37 57:38 58:39 59:39 60:39 61:40 62:41 63:42 64:43 65:44 66:45 67:46 68:46 69:47 70:48 71:49 72:50 73:51 74:52 75:53 76:53 77:54 78:54 79:55 80:56 81:56 82:56 83:57 84:58 85:59 86:60 87:61 88:62 89:63 90:64 91:65 92:66 93:66 94:67 95:67 96:68 97:69 98:70 99:71 100:72 101:73 102:74 103:74 104:75 105:76 106:77 107:78 108:79 109:79 110:80 111:80 112:81 113:82 114:83 115:83 116:83 117:84 118:84 119:85 120:86 121:87 122:88 123:89 124:89 125:90 126:91 127:92 128:93 129:94 130:95 131:96 132:97 133:98 134:99 135:100 136:100 137:100 138:101 139:101 140:102 141:103 142:104 143:105 144:106 145:107 146:108 147:109 148:110 149:110 150:111 151:112 152:112 153:112 154:112 155:113 156:114 157:115 158:116 159:117 160:118 161:119 162:120 163:121 164:122 165:122 166:122 167:123 168:123\n",
            "I0420 22:44:14.023814 139904526645120 squad_lib.py:366] token_is_max_context: 12:True 13:True 14:True 15:True 16:True 17:True 18:True 19:True 20:True 21:True 22:True 23:True 24:True 25:True 26:True 27:True 28:True 29:True 30:True 31:True 32:True 33:True 34:True 35:True 36:True 37:True 38:True 39:True 40:True 41:True 42:True 43:True 44:True 45:True 46:True 47:True 48:True 49:True 50:True 51:True 52:True 53:True 54:True 55:True 56:True 57:True 58:True 59:True 60:True 61:True 62:True 63:True 64:True 65:True 66:True 67:True 68:True 69:True 70:True 71:True 72:True 73:True 74:True 75:True 76:True 77:True 78:True 79:True 80:True 81:True 82:True 83:True 84:True 85:True 86:True 87:True 88:True 89:True 90:True 91:True 92:True 93:True 94:True 95:True 96:True 97:True 98:True 99:True 100:True 101:True 102:True 103:True 104:True 105:True 106:True 107:True 108:True 109:True 110:True 111:True 112:True 113:True 114:True 115:True 116:True 117:True 118:True 119:True 120:True 121:True 122:True 123:True 124:True 125:True 126:True 127:True 128:True 129:True 130:True 131:True 132:True 133:True 134:True 135:True 136:True 137:True 138:True 139:True 140:True 141:True 142:True 143:True 144:True 145:True 146:True 147:True 148:True 149:True 150:True 151:True 152:True 153:True 154:True 155:True 156:True 157:True 158:True 159:True 160:True 161:True 162:True 163:True 164:True 165:True 166:True 167:True 168:True\n",
            "I0420 22:44:14.023973 139904526645120 squad_lib.py:368] input_ids: 101 2054 6891 2106 3565 4605 2753 2202 2173 1999 1029 102 3565 4605 2753 2001 2019 2137 2374 2208 2000 5646 1996 3410 1997 1996 2120 2374 2223 1006 5088 1007 2005 1996 2325 2161 1012 1996 2137 2374 3034 1006 10511 1007 3410 7573 14169 3249 1996 2120 2374 3034 1006 22309 1007 3410 3792 12915 2484 1516 2184 2000 7796 2037 2353 3565 4605 2516 1012 1996 2208 2001 2209 2006 2337 1021 1010 2355 1010 2012 11902 1005 1055 3346 1999 1996 2624 3799 3016 2181 2012 4203 10254 1010 2662 1012 2004 2023 2001 1996 12951 3565 4605 1010 1996 2223 13155 1996 1000 3585 5315 1000 2007 2536 2751 1011 11773 11107 1010 2004 2092 2004 8184 28324 2075 1996 4535 1997 10324 2169 3565 4605 2208 2007 3142 16371 28990 2015 1006 2104 2029 1996 2208 2052 2031 2042 2124 2004 1000 3565 4605 1048 1000 1007 1010 2061 2008 1996 8154 2071 14500 3444 1996 5640 16371 28990 2015 2753 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0420 22:44:14.024129 139904526645120 squad_lib.py:369] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0420 22:44:14.024278 139904526645120 squad_lib.py:370] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0420 22:44:14.027451 139904526645120 squad_lib.py:353] *** Example ***\n",
            "I0420 22:44:14.110272 139904526645120 squad_lib.py:354] unique_id: 1000000013\n",
            "I0420 22:44:14.110389 139904526645120 squad_lib.py:355] example_index: 13\n",
            "I0420 22:44:14.110510 139904526645120 squad_lib.py:356] doc_span_index: 0\n",
            "I0420 22:44:14.110711 139904526645120 squad_lib.py:358] tokens: [CLS] what city did super bowl 50 take place in ? [SEP] super bowl 50 was an american football game to determine the champion of the national football league ( nfl ) for the 2015 season . the american football conference ( afc ) champion denver broncos defeated the national football conference ( nfc ) champion carolina panthers 24 – 10 to earn their third super bowl title . the game was played on february 7 , 2016 , at levi ' s stadium in the san francisco bay area at santa clara , california . as this was the 50th super bowl , the league emphasized the \" golden anniversary \" with various gold - themed initiatives , as well as temporarily suspend ##ing the tradition of naming each super bowl game with roman nu ##meral ##s ( under which the game would have been known as \" super bowl l \" ) , so that the logo could prominently feature the arabic nu ##meral ##s 50 . [SEP]\n",
            "I0420 22:44:14.110882 139904526645120 squad_lib.py:361] token_to_orig_map: 12:0 13:1 14:2 15:3 16:4 17:5 18:6 19:7 20:8 21:9 22:10 23:11 24:12 25:13 26:14 27:15 28:16 29:17 30:17 31:17 32:18 33:19 34:20 35:21 36:21 37:22 38:23 39:24 40:25 41:26 42:26 43:26 44:27 45:28 46:29 47:30 48:31 49:32 50:33 51:34 52:35 53:35 54:35 55:36 56:37 57:38 58:39 59:39 60:39 61:40 62:41 63:42 64:43 65:44 66:45 67:46 68:46 69:47 70:48 71:49 72:50 73:51 74:52 75:53 76:53 77:54 78:54 79:55 80:56 81:56 82:56 83:57 84:58 85:59 86:60 87:61 88:62 89:63 90:64 91:65 92:66 93:66 94:67 95:67 96:68 97:69 98:70 99:71 100:72 101:73 102:74 103:74 104:75 105:76 106:77 107:78 108:79 109:79 110:80 111:80 112:81 113:82 114:83 115:83 116:83 117:84 118:84 119:85 120:86 121:87 122:88 123:89 124:89 125:90 126:91 127:92 128:93 129:94 130:95 131:96 132:97 133:98 134:99 135:100 136:100 137:100 138:101 139:101 140:102 141:103 142:104 143:105 144:106 145:107 146:108 147:109 148:110 149:110 150:111 151:112 152:112 153:112 154:112 155:113 156:114 157:115 158:116 159:117 160:118 161:119 162:120 163:121 164:122 165:122 166:122 167:123 168:123\n",
            "I0420 22:44:14.111013 139904526645120 squad_lib.py:366] token_is_max_context: 12:True 13:True 14:True 15:True 16:True 17:True 18:True 19:True 20:True 21:True 22:True 23:True 24:True 25:True 26:True 27:True 28:True 29:True 30:True 31:True 32:True 33:True 34:True 35:True 36:True 37:True 38:True 39:True 40:True 41:True 42:True 43:True 44:True 45:True 46:True 47:True 48:True 49:True 50:True 51:True 52:True 53:True 54:True 55:True 56:True 57:True 58:True 59:True 60:True 61:True 62:True 63:True 64:True 65:True 66:True 67:True 68:True 69:True 70:True 71:True 72:True 73:True 74:True 75:True 76:True 77:True 78:True 79:True 80:True 81:True 82:True 83:True 84:True 85:True 86:True 87:True 88:True 89:True 90:True 91:True 92:True 93:True 94:True 95:True 96:True 97:True 98:True 99:True 100:True 101:True 102:True 103:True 104:True 105:True 106:True 107:True 108:True 109:True 110:True 111:True 112:True 113:True 114:True 115:True 116:True 117:True 118:True 119:True 120:True 121:True 122:True 123:True 124:True 125:True 126:True 127:True 128:True 129:True 130:True 131:True 132:True 133:True 134:True 135:True 136:True 137:True 138:True 139:True 140:True 141:True 142:True 143:True 144:True 145:True 146:True 147:True 148:True 149:True 150:True 151:True 152:True 153:True 154:True 155:True 156:True 157:True 158:True 159:True 160:True 161:True 162:True 163:True 164:True 165:True 166:True 167:True 168:True\n",
            "I0420 22:44:14.111181 139904526645120 squad_lib.py:368] input_ids: 101 2054 2103 2106 3565 4605 2753 2202 2173 1999 1029 102 3565 4605 2753 2001 2019 2137 2374 2208 2000 5646 1996 3410 1997 1996 2120 2374 2223 1006 5088 1007 2005 1996 2325 2161 1012 1996 2137 2374 3034 1006 10511 1007 3410 7573 14169 3249 1996 2120 2374 3034 1006 22309 1007 3410 3792 12915 2484 1516 2184 2000 7796 2037 2353 3565 4605 2516 1012 1996 2208 2001 2209 2006 2337 1021 1010 2355 1010 2012 11902 1005 1055 3346 1999 1996 2624 3799 3016 2181 2012 4203 10254 1010 2662 1012 2004 2023 2001 1996 12951 3565 4605 1010 1996 2223 13155 1996 1000 3585 5315 1000 2007 2536 2751 1011 11773 11107 1010 2004 2092 2004 8184 28324 2075 1996 4535 1997 10324 2169 3565 4605 2208 2007 3142 16371 28990 2015 1006 2104 2029 1996 2208 2052 2031 2042 2124 2004 1000 3565 4605 1048 1000 1007 1010 2061 2008 1996 8154 2071 14500 3444 1996 5640 16371 28990 2015 2753 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0420 22:44:14.111336 139904526645120 squad_lib.py:369] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0420 22:44:14.111499 139904526645120 squad_lib.py:370] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0420 22:44:14.114900 139904526645120 squad_lib.py:353] *** Example ***\n",
            "I0420 22:44:14.114999 139904526645120 squad_lib.py:354] unique_id: 1000000014\n",
            "I0420 22:44:14.115066 139904526645120 squad_lib.py:355] example_index: 14\n",
            "I0420 22:44:14.115123 139904526645120 squad_lib.py:356] doc_span_index: 0\n",
            "I0420 22:44:14.115244 139904526645120 squad_lib.py:358] tokens: [CLS] if roman nu ##meral ##s were used , what would super bowl 50 have been called ? [SEP] super bowl 50 was an american football game to determine the champion of the national football league ( nfl ) for the 2015 season . the american football conference ( afc ) champion denver broncos defeated the national football conference ( nfc ) champion carolina panthers 24 – 10 to earn their third super bowl title . the game was played on february 7 , 2016 , at levi ' s stadium in the san francisco bay area at santa clara , california . as this was the 50th super bowl , the league emphasized the \" golden anniversary \" with various gold - themed initiatives , as well as temporarily suspend ##ing the tradition of naming each super bowl game with roman nu ##meral ##s ( under which the game would have been known as \" super bowl l \" ) , so that the logo could prominently feature the arabic nu ##meral ##s 50 . [SEP]\n",
            "I0420 22:44:14.115363 139904526645120 squad_lib.py:361] token_to_orig_map: 19:0 20:1 21:2 22:3 23:4 24:5 25:6 26:7 27:8 28:9 29:10 30:11 31:12 32:13 33:14 34:15 35:16 36:17 37:17 38:17 39:18 40:19 41:20 42:21 43:21 44:22 45:23 46:24 47:25 48:26 49:26 50:26 51:27 52:28 53:29 54:30 55:31 56:32 57:33 58:34 59:35 60:35 61:35 62:36 63:37 64:38 65:39 66:39 67:39 68:40 69:41 70:42 71:43 72:44 73:45 74:46 75:46 76:47 77:48 78:49 79:50 80:51 81:52 82:53 83:53 84:54 85:54 86:55 87:56 88:56 89:56 90:57 91:58 92:59 93:60 94:61 95:62 96:63 97:64 98:65 99:66 100:66 101:67 102:67 103:68 104:69 105:70 106:71 107:72 108:73 109:74 110:74 111:75 112:76 113:77 114:78 115:79 116:79 117:80 118:80 119:81 120:82 121:83 122:83 123:83 124:84 125:84 126:85 127:86 128:87 129:88 130:89 131:89 132:90 133:91 134:92 135:93 136:94 137:95 138:96 139:97 140:98 141:99 142:100 143:100 144:100 145:101 146:101 147:102 148:103 149:104 150:105 151:106 152:107 153:108 154:109 155:110 156:110 157:111 158:112 159:112 160:112 161:112 162:113 163:114 164:115 165:116 166:117 167:118 168:119 169:120 170:121 171:122 172:122 173:122 174:123 175:123\n",
            "I0420 22:44:14.115483 139904526645120 squad_lib.py:366] token_is_max_context: 19:True 20:True 21:True 22:True 23:True 24:True 25:True 26:True 27:True 28:True 29:True 30:True 31:True 32:True 33:True 34:True 35:True 36:True 37:True 38:True 39:True 40:True 41:True 42:True 43:True 44:True 45:True 46:True 47:True 48:True 49:True 50:True 51:True 52:True 53:True 54:True 55:True 56:True 57:True 58:True 59:True 60:True 61:True 62:True 63:True 64:True 65:True 66:True 67:True 68:True 69:True 70:True 71:True 72:True 73:True 74:True 75:True 76:True 77:True 78:True 79:True 80:True 81:True 82:True 83:True 84:True 85:True 86:True 87:True 88:True 89:True 90:True 91:True 92:True 93:True 94:True 95:True 96:True 97:True 98:True 99:True 100:True 101:True 102:True 103:True 104:True 105:True 106:True 107:True 108:True 109:True 110:True 111:True 112:True 113:True 114:True 115:True 116:True 117:True 118:True 119:True 120:True 121:True 122:True 123:True 124:True 125:True 126:True 127:True 128:True 129:True 130:True 131:True 132:True 133:True 134:True 135:True 136:True 137:True 138:True 139:True 140:True 141:True 142:True 143:True 144:True 145:True 146:True 147:True 148:True 149:True 150:True 151:True 152:True 153:True 154:True 155:True 156:True 157:True 158:True 159:True 160:True 161:True 162:True 163:True 164:True 165:True 166:True 167:True 168:True 169:True 170:True 171:True 172:True 173:True 174:True 175:True\n",
            "I0420 22:44:14.115659 139904526645120 squad_lib.py:368] input_ids: 101 2065 3142 16371 28990 2015 2020 2109 1010 2054 2052 3565 4605 2753 2031 2042 2170 1029 102 3565 4605 2753 2001 2019 2137 2374 2208 2000 5646 1996 3410 1997 1996 2120 2374 2223 1006 5088 1007 2005 1996 2325 2161 1012 1996 2137 2374 3034 1006 10511 1007 3410 7573 14169 3249 1996 2120 2374 3034 1006 22309 1007 3410 3792 12915 2484 1516 2184 2000 7796 2037 2353 3565 4605 2516 1012 1996 2208 2001 2209 2006 2337 1021 1010 2355 1010 2012 11902 1005 1055 3346 1999 1996 2624 3799 3016 2181 2012 4203 10254 1010 2662 1012 2004 2023 2001 1996 12951 3565 4605 1010 1996 2223 13155 1996 1000 3585 5315 1000 2007 2536 2751 1011 11773 11107 1010 2004 2092 2004 8184 28324 2075 1996 4535 1997 10324 2169 3565 4605 2208 2007 3142 16371 28990 2015 1006 2104 2029 1996 2208 2052 2031 2042 2124 2004 1000 3565 4605 1048 1000 1007 1010 2061 2008 1996 8154 2071 14500 3444 1996 5640 16371 28990 2015 2753 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0420 22:44:14.115822 139904526645120 squad_lib.py:369] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0420 22:44:14.115977 139904526645120 squad_lib.py:370] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0420 22:44:14.119698 139904526645120 squad_lib.py:353] *** Example ***\n",
            "I0420 22:44:14.119789 139904526645120 squad_lib.py:354] unique_id: 1000000015\n",
            "I0420 22:44:14.119857 139904526645120 squad_lib.py:355] example_index: 15\n",
            "I0420 22:44:14.119913 139904526645120 squad_lib.py:356] doc_span_index: 0\n",
            "I0420 22:44:14.120038 139904526645120 squad_lib.py:358] tokens: [CLS] super bowl 50 decided the nfl champion for what season ? [SEP] super bowl 50 was an american football game to determine the champion of the national football league ( nfl ) for the 2015 season . the american football conference ( afc ) champion denver broncos defeated the national football conference ( nfc ) champion carolina panthers 24 – 10 to earn their third super bowl title . the game was played on february 7 , 2016 , at levi ' s stadium in the san francisco bay area at santa clara , california . as this was the 50th super bowl , the league emphasized the \" golden anniversary \" with various gold - themed initiatives , as well as temporarily suspend ##ing the tradition of naming each super bowl game with roman nu ##meral ##s ( under which the game would have been known as \" super bowl l \" ) , so that the logo could prominently feature the arabic nu ##meral ##s 50 . [SEP]\n",
            "I0420 22:44:14.120162 139904526645120 squad_lib.py:361] token_to_orig_map: 13:0 14:1 15:2 16:3 17:4 18:5 19:6 20:7 21:8 22:9 23:10 24:11 25:12 26:13 27:14 28:15 29:16 30:17 31:17 32:17 33:18 34:19 35:20 36:21 37:21 38:22 39:23 40:24 41:25 42:26 43:26 44:26 45:27 46:28 47:29 48:30 49:31 50:32 51:33 52:34 53:35 54:35 55:35 56:36 57:37 58:38 59:39 60:39 61:39 62:40 63:41 64:42 65:43 66:44 67:45 68:46 69:46 70:47 71:48 72:49 73:50 74:51 75:52 76:53 77:53 78:54 79:54 80:55 81:56 82:56 83:56 84:57 85:58 86:59 87:60 88:61 89:62 90:63 91:64 92:65 93:66 94:66 95:67 96:67 97:68 98:69 99:70 100:71 101:72 102:73 103:74 104:74 105:75 106:76 107:77 108:78 109:79 110:79 111:80 112:80 113:81 114:82 115:83 116:83 117:83 118:84 119:84 120:85 121:86 122:87 123:88 124:89 125:89 126:90 127:91 128:92 129:93 130:94 131:95 132:96 133:97 134:98 135:99 136:100 137:100 138:100 139:101 140:101 141:102 142:103 143:104 144:105 145:106 146:107 147:108 148:109 149:110 150:110 151:111 152:112 153:112 154:112 155:112 156:113 157:114 158:115 159:116 160:117 161:118 162:119 163:120 164:121 165:122 166:122 167:122 168:123 169:123\n",
            "I0420 22:44:14.215911 139904526645120 squad_lib.py:366] token_is_max_context: 13:True 14:True 15:True 16:True 17:True 18:True 19:True 20:True 21:True 22:True 23:True 24:True 25:True 26:True 27:True 28:True 29:True 30:True 31:True 32:True 33:True 34:True 35:True 36:True 37:True 38:True 39:True 40:True 41:True 42:True 43:True 44:True 45:True 46:True 47:True 48:True 49:True 50:True 51:True 52:True 53:True 54:True 55:True 56:True 57:True 58:True 59:True 60:True 61:True 62:True 63:True 64:True 65:True 66:True 67:True 68:True 69:True 70:True 71:True 72:True 73:True 74:True 75:True 76:True 77:True 78:True 79:True 80:True 81:True 82:True 83:True 84:True 85:True 86:True 87:True 88:True 89:True 90:True 91:True 92:True 93:True 94:True 95:True 96:True 97:True 98:True 99:True 100:True 101:True 102:True 103:True 104:True 105:True 106:True 107:True 108:True 109:True 110:True 111:True 112:True 113:True 114:True 115:True 116:True 117:True 118:True 119:True 120:True 121:True 122:True 123:True 124:True 125:True 126:True 127:True 128:True 129:True 130:True 131:True 132:True 133:True 134:True 135:True 136:True 137:True 138:True 139:True 140:True 141:True 142:True 143:True 144:True 145:True 146:True 147:True 148:True 149:True 150:True 151:True 152:True 153:True 154:True 155:True 156:True 157:True 158:True 159:True 160:True 161:True 162:True 163:True 164:True 165:True 166:True 167:True 168:True 169:True\n",
            "I0420 22:44:14.216289 139904526645120 squad_lib.py:368] input_ids: 101 3565 4605 2753 2787 1996 5088 3410 2005 2054 2161 1029 102 3565 4605 2753 2001 2019 2137 2374 2208 2000 5646 1996 3410 1997 1996 2120 2374 2223 1006 5088 1007 2005 1996 2325 2161 1012 1996 2137 2374 3034 1006 10511 1007 3410 7573 14169 3249 1996 2120 2374 3034 1006 22309 1007 3410 3792 12915 2484 1516 2184 2000 7796 2037 2353 3565 4605 2516 1012 1996 2208 2001 2209 2006 2337 1021 1010 2355 1010 2012 11902 1005 1055 3346 1999 1996 2624 3799 3016 2181 2012 4203 10254 1010 2662 1012 2004 2023 2001 1996 12951 3565 4605 1010 1996 2223 13155 1996 1000 3585 5315 1000 2007 2536 2751 1011 11773 11107 1010 2004 2092 2004 8184 28324 2075 1996 4535 1997 10324 2169 3565 4605 2208 2007 3142 16371 28990 2015 1006 2104 2029 1996 2208 2052 2031 2042 2124 2004 1000 3565 4605 1048 1000 1007 1010 2061 2008 1996 8154 2071 14500 3444 1996 5640 16371 28990 2015 2753 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0420 22:44:14.216620 139904526645120 squad_lib.py:369] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0420 22:44:14.216928 139904526645120 squad_lib.py:370] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0420 22:44:14.222831 139904526645120 squad_lib.py:353] *** Example ***\n",
            "I0420 22:44:14.222926 139904526645120 squad_lib.py:354] unique_id: 1000000016\n",
            "I0420 22:44:14.222999 139904526645120 squad_lib.py:355] example_index: 16\n",
            "I0420 22:44:14.223060 139904526645120 squad_lib.py:356] doc_span_index: 0\n",
            "I0420 22:44:14.223179 139904526645120 squad_lib.py:358] tokens: [CLS] what year did the denver broncos secure a super bowl title for the third time ? [SEP] super bowl 50 was an american football game to determine the champion of the national football league ( nfl ) for the 2015 season . the american football conference ( afc ) champion denver broncos defeated the national football conference ( nfc ) champion carolina panthers 24 – 10 to earn their third super bowl title . the game was played on february 7 , 2016 , at levi ' s stadium in the san francisco bay area at santa clara , california . as this was the 50th super bowl , the league emphasized the \" golden anniversary \" with various gold - themed initiatives , as well as temporarily suspend ##ing the tradition of naming each super bowl game with roman nu ##meral ##s ( under which the game would have been known as \" super bowl l \" ) , so that the logo could prominently feature the arabic nu ##meral ##s 50 . [SEP]\n",
            "I0420 22:44:14.223297 139904526645120 squad_lib.py:361] token_to_orig_map: 18:0 19:1 20:2 21:3 22:4 23:5 24:6 25:7 26:8 27:9 28:10 29:11 30:12 31:13 32:14 33:15 34:16 35:17 36:17 37:17 38:18 39:19 40:20 41:21 42:21 43:22 44:23 45:24 46:25 47:26 48:26 49:26 50:27 51:28 52:29 53:30 54:31 55:32 56:33 57:34 58:35 59:35 60:35 61:36 62:37 63:38 64:39 65:39 66:39 67:40 68:41 69:42 70:43 71:44 72:45 73:46 74:46 75:47 76:48 77:49 78:50 79:51 80:52 81:53 82:53 83:54 84:54 85:55 86:56 87:56 88:56 89:57 90:58 91:59 92:60 93:61 94:62 95:63 96:64 97:65 98:66 99:66 100:67 101:67 102:68 103:69 104:70 105:71 106:72 107:73 108:74 109:74 110:75 111:76 112:77 113:78 114:79 115:79 116:80 117:80 118:81 119:82 120:83 121:83 122:83 123:84 124:84 125:85 126:86 127:87 128:88 129:89 130:89 131:90 132:91 133:92 134:93 135:94 136:95 137:96 138:97 139:98 140:99 141:100 142:100 143:100 144:101 145:101 146:102 147:103 148:104 149:105 150:106 151:107 152:108 153:109 154:110 155:110 156:111 157:112 158:112 159:112 160:112 161:113 162:114 163:115 164:116 165:117 166:118 167:119 168:120 169:121 170:122 171:122 172:122 173:123 174:123\n",
            "I0420 22:44:14.223427 139904526645120 squad_lib.py:366] token_is_max_context: 18:True 19:True 20:True 21:True 22:True 23:True 24:True 25:True 26:True 27:True 28:True 29:True 30:True 31:True 32:True 33:True 34:True 35:True 36:True 37:True 38:True 39:True 40:True 41:True 42:True 43:True 44:True 45:True 46:True 47:True 48:True 49:True 50:True 51:True 52:True 53:True 54:True 55:True 56:True 57:True 58:True 59:True 60:True 61:True 62:True 63:True 64:True 65:True 66:True 67:True 68:True 69:True 70:True 71:True 72:True 73:True 74:True 75:True 76:True 77:True 78:True 79:True 80:True 81:True 82:True 83:True 84:True 85:True 86:True 87:True 88:True 89:True 90:True 91:True 92:True 93:True 94:True 95:True 96:True 97:True 98:True 99:True 100:True 101:True 102:True 103:True 104:True 105:True 106:True 107:True 108:True 109:True 110:True 111:True 112:True 113:True 114:True 115:True 116:True 117:True 118:True 119:True 120:True 121:True 122:True 123:True 124:True 125:True 126:True 127:True 128:True 129:True 130:True 131:True 132:True 133:True 134:True 135:True 136:True 137:True 138:True 139:True 140:True 141:True 142:True 143:True 144:True 145:True 146:True 147:True 148:True 149:True 150:True 151:True 152:True 153:True 154:True 155:True 156:True 157:True 158:True 159:True 160:True 161:True 162:True 163:True 164:True 165:True 166:True 167:True 168:True 169:True 170:True 171:True 172:True 173:True 174:True\n",
            "I0420 22:44:14.223657 139904526645120 squad_lib.py:368] input_ids: 101 2054 2095 2106 1996 7573 14169 5851 1037 3565 4605 2516 2005 1996 2353 2051 1029 102 3565 4605 2753 2001 2019 2137 2374 2208 2000 5646 1996 3410 1997 1996 2120 2374 2223 1006 5088 1007 2005 1996 2325 2161 1012 1996 2137 2374 3034 1006 10511 1007 3410 7573 14169 3249 1996 2120 2374 3034 1006 22309 1007 3410 3792 12915 2484 1516 2184 2000 7796 2037 2353 3565 4605 2516 1012 1996 2208 2001 2209 2006 2337 1021 1010 2355 1010 2012 11902 1005 1055 3346 1999 1996 2624 3799 3016 2181 2012 4203 10254 1010 2662 1012 2004 2023 2001 1996 12951 3565 4605 1010 1996 2223 13155 1996 1000 3585 5315 1000 2007 2536 2751 1011 11773 11107 1010 2004 2092 2004 8184 28324 2075 1996 4535 1997 10324 2169 3565 4605 2208 2007 3142 16371 28990 2015 1006 2104 2029 1996 2208 2052 2031 2042 2124 2004 1000 3565 4605 1048 1000 1007 1010 2061 2008 1996 8154 2071 14500 3444 1996 5640 16371 28990 2015 2753 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0420 22:44:14.223826 139904526645120 squad_lib.py:369] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0420 22:44:14.224009 139904526645120 squad_lib.py:370] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0420 22:44:14.227276 139904526645120 squad_lib.py:353] *** Example ***\n",
            "I0420 22:44:14.227368 139904526645120 squad_lib.py:354] unique_id: 1000000017\n",
            "I0420 22:44:14.227432 139904526645120 squad_lib.py:355] example_index: 17\n",
            "I0420 22:44:14.227487 139904526645120 squad_lib.py:356] doc_span_index: 0\n",
            "I0420 22:44:14.227631 139904526645120 squad_lib.py:358] tokens: [CLS] what city did super bowl 50 take place in ? [SEP] super bowl 50 was an american football game to determine the champion of the national football league ( nfl ) for the 2015 season . the american football conference ( afc ) champion denver broncos defeated the national football conference ( nfc ) champion carolina panthers 24 – 10 to earn their third super bowl title . the game was played on february 7 , 2016 , at levi ' s stadium in the san francisco bay area at santa clara , california . as this was the 50th super bowl , the league emphasized the \" golden anniversary \" with various gold - themed initiatives , as well as temporarily suspend ##ing the tradition of naming each super bowl game with roman nu ##meral ##s ( under which the game would have been known as \" super bowl l \" ) , so that the logo could prominently feature the arabic nu ##meral ##s 50 . [SEP]\n",
            "I0420 22:44:14.227755 139904526645120 squad_lib.py:361] token_to_orig_map: 12:0 13:1 14:2 15:3 16:4 17:5 18:6 19:7 20:8 21:9 22:10 23:11 24:12 25:13 26:14 27:15 28:16 29:17 30:17 31:17 32:18 33:19 34:20 35:21 36:21 37:22 38:23 39:24 40:25 41:26 42:26 43:26 44:27 45:28 46:29 47:30 48:31 49:32 50:33 51:34 52:35 53:35 54:35 55:36 56:37 57:38 58:39 59:39 60:39 61:40 62:41 63:42 64:43 65:44 66:45 67:46 68:46 69:47 70:48 71:49 72:50 73:51 74:52 75:53 76:53 77:54 78:54 79:55 80:56 81:56 82:56 83:57 84:58 85:59 86:60 87:61 88:62 89:63 90:64 91:65 92:66 93:66 94:67 95:67 96:68 97:69 98:70 99:71 100:72 101:73 102:74 103:74 104:75 105:76 106:77 107:78 108:79 109:79 110:80 111:80 112:81 113:82 114:83 115:83 116:83 117:84 118:84 119:85 120:86 121:87 122:88 123:89 124:89 125:90 126:91 127:92 128:93 129:94 130:95 131:96 132:97 133:98 134:99 135:100 136:100 137:100 138:101 139:101 140:102 141:103 142:104 143:105 144:106 145:107 146:108 147:109 148:110 149:110 150:111 151:112 152:112 153:112 154:112 155:113 156:114 157:115 158:116 159:117 160:118 161:119 162:120 163:121 164:122 165:122 166:122 167:123 168:123\n",
            "I0420 22:44:14.227869 139904526645120 squad_lib.py:366] token_is_max_context: 12:True 13:True 14:True 15:True 16:True 17:True 18:True 19:True 20:True 21:True 22:True 23:True 24:True 25:True 26:True 27:True 28:True 29:True 30:True 31:True 32:True 33:True 34:True 35:True 36:True 37:True 38:True 39:True 40:True 41:True 42:True 43:True 44:True 45:True 46:True 47:True 48:True 49:True 50:True 51:True 52:True 53:True 54:True 55:True 56:True 57:True 58:True 59:True 60:True 61:True 62:True 63:True 64:True 65:True 66:True 67:True 68:True 69:True 70:True 71:True 72:True 73:True 74:True 75:True 76:True 77:True 78:True 79:True 80:True 81:True 82:True 83:True 84:True 85:True 86:True 87:True 88:True 89:True 90:True 91:True 92:True 93:True 94:True 95:True 96:True 97:True 98:True 99:True 100:True 101:True 102:True 103:True 104:True 105:True 106:True 107:True 108:True 109:True 110:True 111:True 112:True 113:True 114:True 115:True 116:True 117:True 118:True 119:True 120:True 121:True 122:True 123:True 124:True 125:True 126:True 127:True 128:True 129:True 130:True 131:True 132:True 133:True 134:True 135:True 136:True 137:True 138:True 139:True 140:True 141:True 142:True 143:True 144:True 145:True 146:True 147:True 148:True 149:True 150:True 151:True 152:True 153:True 154:True 155:True 156:True 157:True 158:True 159:True 160:True 161:True 162:True 163:True 164:True 165:True 166:True 167:True 168:True\n",
            "I0420 22:44:14.228053 139904526645120 squad_lib.py:368] input_ids: 101 2054 2103 2106 3565 4605 2753 2202 2173 1999 1029 102 3565 4605 2753 2001 2019 2137 2374 2208 2000 5646 1996 3410 1997 1996 2120 2374 2223 1006 5088 1007 2005 1996 2325 2161 1012 1996 2137 2374 3034 1006 10511 1007 3410 7573 14169 3249 1996 2120 2374 3034 1006 22309 1007 3410 3792 12915 2484 1516 2184 2000 7796 2037 2353 3565 4605 2516 1012 1996 2208 2001 2209 2006 2337 1021 1010 2355 1010 2012 11902 1005 1055 3346 1999 1996 2624 3799 3016 2181 2012 4203 10254 1010 2662 1012 2004 2023 2001 1996 12951 3565 4605 1010 1996 2223 13155 1996 1000 3585 5315 1000 2007 2536 2751 1011 11773 11107 1010 2004 2092 2004 8184 28324 2075 1996 4535 1997 10324 2169 3565 4605 2208 2007 3142 16371 28990 2015 1006 2104 2029 1996 2208 2052 2031 2042 2124 2004 1000 3565 4605 1048 1000 1007 1010 2061 2008 1996 8154 2071 14500 3444 1996 5640 16371 28990 2015 2753 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0420 22:44:14.312494 139904526645120 squad_lib.py:369] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0420 22:44:14.312741 139904526645120 squad_lib.py:370] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0420 22:44:14.316672 139904526645120 squad_lib.py:353] *** Example ***\n",
            "I0420 22:44:14.316785 139904526645120 squad_lib.py:354] unique_id: 1000000018\n",
            "I0420 22:44:14.316858 139904526645120 squad_lib.py:355] example_index: 18\n",
            "I0420 22:44:14.316915 139904526645120 squad_lib.py:356] doc_span_index: 0\n",
            "I0420 22:44:14.317059 139904526645120 squad_lib.py:358] tokens: [CLS] what stadium did super bowl 50 take place in ? [SEP] super bowl 50 was an american football game to determine the champion of the national football league ( nfl ) for the 2015 season . the american football conference ( afc ) champion denver broncos defeated the national football conference ( nfc ) champion carolina panthers 24 – 10 to earn their third super bowl title . the game was played on february 7 , 2016 , at levi ' s stadium in the san francisco bay area at santa clara , california . as this was the 50th super bowl , the league emphasized the \" golden anniversary \" with various gold - themed initiatives , as well as temporarily suspend ##ing the tradition of naming each super bowl game with roman nu ##meral ##s ( under which the game would have been known as \" super bowl l \" ) , so that the logo could prominently feature the arabic nu ##meral ##s 50 . [SEP]\n",
            "I0420 22:44:14.317213 139904526645120 squad_lib.py:361] token_to_orig_map: 12:0 13:1 14:2 15:3 16:4 17:5 18:6 19:7 20:8 21:9 22:10 23:11 24:12 25:13 26:14 27:15 28:16 29:17 30:17 31:17 32:18 33:19 34:20 35:21 36:21 37:22 38:23 39:24 40:25 41:26 42:26 43:26 44:27 45:28 46:29 47:30 48:31 49:32 50:33 51:34 52:35 53:35 54:35 55:36 56:37 57:38 58:39 59:39 60:39 61:40 62:41 63:42 64:43 65:44 66:45 67:46 68:46 69:47 70:48 71:49 72:50 73:51 74:52 75:53 76:53 77:54 78:54 79:55 80:56 81:56 82:56 83:57 84:58 85:59 86:60 87:61 88:62 89:63 90:64 91:65 92:66 93:66 94:67 95:67 96:68 97:69 98:70 99:71 100:72 101:73 102:74 103:74 104:75 105:76 106:77 107:78 108:79 109:79 110:80 111:80 112:81 113:82 114:83 115:83 116:83 117:84 118:84 119:85 120:86 121:87 122:88 123:89 124:89 125:90 126:91 127:92 128:93 129:94 130:95 131:96 132:97 133:98 134:99 135:100 136:100 137:100 138:101 139:101 140:102 141:103 142:104 143:105 144:106 145:107 146:108 147:109 148:110 149:110 150:111 151:112 152:112 153:112 154:112 155:113 156:114 157:115 158:116 159:117 160:118 161:119 162:120 163:121 164:122 165:122 166:122 167:123 168:123\n",
            "I0420 22:44:14.317350 139904526645120 squad_lib.py:366] token_is_max_context: 12:True 13:True 14:True 15:True 16:True 17:True 18:True 19:True 20:True 21:True 22:True 23:True 24:True 25:True 26:True 27:True 28:True 29:True 30:True 31:True 32:True 33:True 34:True 35:True 36:True 37:True 38:True 39:True 40:True 41:True 42:True 43:True 44:True 45:True 46:True 47:True 48:True 49:True 50:True 51:True 52:True 53:True 54:True 55:True 56:True 57:True 58:True 59:True 60:True 61:True 62:True 63:True 64:True 65:True 66:True 67:True 68:True 69:True 70:True 71:True 72:True 73:True 74:True 75:True 76:True 77:True 78:True 79:True 80:True 81:True 82:True 83:True 84:True 85:True 86:True 87:True 88:True 89:True 90:True 91:True 92:True 93:True 94:True 95:True 96:True 97:True 98:True 99:True 100:True 101:True 102:True 103:True 104:True 105:True 106:True 107:True 108:True 109:True 110:True 111:True 112:True 113:True 114:True 115:True 116:True 117:True 118:True 119:True 120:True 121:True 122:True 123:True 124:True 125:True 126:True 127:True 128:True 129:True 130:True 131:True 132:True 133:True 134:True 135:True 136:True 137:True 138:True 139:True 140:True 141:True 142:True 143:True 144:True 145:True 146:True 147:True 148:True 149:True 150:True 151:True 152:True 153:True 154:True 155:True 156:True 157:True 158:True 159:True 160:True 161:True 162:True 163:True 164:True 165:True 166:True 167:True 168:True\n",
            "I0420 22:44:14.317529 139904526645120 squad_lib.py:368] input_ids: 101 2054 3346 2106 3565 4605 2753 2202 2173 1999 1029 102 3565 4605 2753 2001 2019 2137 2374 2208 2000 5646 1996 3410 1997 1996 2120 2374 2223 1006 5088 1007 2005 1996 2325 2161 1012 1996 2137 2374 3034 1006 10511 1007 3410 7573 14169 3249 1996 2120 2374 3034 1006 22309 1007 3410 3792 12915 2484 1516 2184 2000 7796 2037 2353 3565 4605 2516 1012 1996 2208 2001 2209 2006 2337 1021 1010 2355 1010 2012 11902 1005 1055 3346 1999 1996 2624 3799 3016 2181 2012 4203 10254 1010 2662 1012 2004 2023 2001 1996 12951 3565 4605 1010 1996 2223 13155 1996 1000 3585 5315 1000 2007 2536 2751 1011 11773 11107 1010 2004 2092 2004 8184 28324 2075 1996 4535 1997 10324 2169 3565 4605 2208 2007 3142 16371 28990 2015 1006 2104 2029 1996 2208 2052 2031 2042 2124 2004 1000 3565 4605 1048 1000 1007 1010 2061 2008 1996 8154 2071 14500 3444 1996 5640 16371 28990 2015 2753 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0420 22:44:14.317712 139904526645120 squad_lib.py:369] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0420 22:44:14.317905 139904526645120 squad_lib.py:370] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0420 22:44:14.321715 139904526645120 squad_lib.py:353] *** Example ***\n",
            "I0420 22:44:14.321822 139904526645120 squad_lib.py:354] unique_id: 1000000019\n",
            "I0420 22:44:14.321895 139904526645120 squad_lib.py:355] example_index: 19\n",
            "I0420 22:44:14.321968 139904526645120 squad_lib.py:356] doc_span_index: 0\n",
            "I0420 22:44:14.322091 139904526645120 squad_lib.py:358] tokens: [CLS] what was the final score of super bowl 50 ? [SEP] super bowl 50 was an american football game to determine the champion of the national football league ( nfl ) for the 2015 season . the american football conference ( afc ) champion denver broncos defeated the national football conference ( nfc ) champion carolina panthers 24 – 10 to earn their third super bowl title . the game was played on february 7 , 2016 , at levi ' s stadium in the san francisco bay area at santa clara , california . as this was the 50th super bowl , the league emphasized the \" golden anniversary \" with various gold - themed initiatives , as well as temporarily suspend ##ing the tradition of naming each super bowl game with roman nu ##meral ##s ( under which the game would have been known as \" super bowl l \" ) , so that the logo could prominently feature the arabic nu ##meral ##s 50 . [SEP]\n",
            "I0420 22:44:14.322212 139904526645120 squad_lib.py:361] token_to_orig_map: 12:0 13:1 14:2 15:3 16:4 17:5 18:6 19:7 20:8 21:9 22:10 23:11 24:12 25:13 26:14 27:15 28:16 29:17 30:17 31:17 32:18 33:19 34:20 35:21 36:21 37:22 38:23 39:24 40:25 41:26 42:26 43:26 44:27 45:28 46:29 47:30 48:31 49:32 50:33 51:34 52:35 53:35 54:35 55:36 56:37 57:38 58:39 59:39 60:39 61:40 62:41 63:42 64:43 65:44 66:45 67:46 68:46 69:47 70:48 71:49 72:50 73:51 74:52 75:53 76:53 77:54 78:54 79:55 80:56 81:56 82:56 83:57 84:58 85:59 86:60 87:61 88:62 89:63 90:64 91:65 92:66 93:66 94:67 95:67 96:68 97:69 98:70 99:71 100:72 101:73 102:74 103:74 104:75 105:76 106:77 107:78 108:79 109:79 110:80 111:80 112:81 113:82 114:83 115:83 116:83 117:84 118:84 119:85 120:86 121:87 122:88 123:89 124:89 125:90 126:91 127:92 128:93 129:94 130:95 131:96 132:97 133:98 134:99 135:100 136:100 137:100 138:101 139:101 140:102 141:103 142:104 143:105 144:106 145:107 146:108 147:109 148:110 149:110 150:111 151:112 152:112 153:112 154:112 155:113 156:114 157:115 158:116 159:117 160:118 161:119 162:120 163:121 164:122 165:122 166:122 167:123 168:123\n",
            "I0420 22:44:14.322325 139904526645120 squad_lib.py:366] token_is_max_context: 12:True 13:True 14:True 15:True 16:True 17:True 18:True 19:True 20:True 21:True 22:True 23:True 24:True 25:True 26:True 27:True 28:True 29:True 30:True 31:True 32:True 33:True 34:True 35:True 36:True 37:True 38:True 39:True 40:True 41:True 42:True 43:True 44:True 45:True 46:True 47:True 48:True 49:True 50:True 51:True 52:True 53:True 54:True 55:True 56:True 57:True 58:True 59:True 60:True 61:True 62:True 63:True 64:True 65:True 66:True 67:True 68:True 69:True 70:True 71:True 72:True 73:True 74:True 75:True 76:True 77:True 78:True 79:True 80:True 81:True 82:True 83:True 84:True 85:True 86:True 87:True 88:True 89:True 90:True 91:True 92:True 93:True 94:True 95:True 96:True 97:True 98:True 99:True 100:True 101:True 102:True 103:True 104:True 105:True 106:True 107:True 108:True 109:True 110:True 111:True 112:True 113:True 114:True 115:True 116:True 117:True 118:True 119:True 120:True 121:True 122:True 123:True 124:True 125:True 126:True 127:True 128:True 129:True 130:True 131:True 132:True 133:True 134:True 135:True 136:True 137:True 138:True 139:True 140:True 141:True 142:True 143:True 144:True 145:True 146:True 147:True 148:True 149:True 150:True 151:True 152:True 153:True 154:True 155:True 156:True 157:True 158:True 159:True 160:True 161:True 162:True 163:True 164:True 165:True 166:True 167:True 168:True\n",
            "I0420 22:44:14.322538 139904526645120 squad_lib.py:368] input_ids: 101 2054 2001 1996 2345 3556 1997 3565 4605 2753 1029 102 3565 4605 2753 2001 2019 2137 2374 2208 2000 5646 1996 3410 1997 1996 2120 2374 2223 1006 5088 1007 2005 1996 2325 2161 1012 1996 2137 2374 3034 1006 10511 1007 3410 7573 14169 3249 1996 2120 2374 3034 1006 22309 1007 3410 3792 12915 2484 1516 2184 2000 7796 2037 2353 3565 4605 2516 1012 1996 2208 2001 2209 2006 2337 1021 1010 2355 1010 2012 11902 1005 1055 3346 1999 1996 2624 3799 3016 2181 2012 4203 10254 1010 2662 1012 2004 2023 2001 1996 12951 3565 4605 1010 1996 2223 13155 1996 1000 3585 5315 1000 2007 2536 2751 1011 11773 11107 1010 2004 2092 2004 8184 28324 2075 1996 4535 1997 10324 2169 3565 4605 2208 2007 3142 16371 28990 2015 1006 2104 2029 1996 2208 2052 2031 2042 2124 2004 1000 3565 4605 1048 1000 1007 1010 2061 2008 1996 8154 2071 14500 3444 1996 5640 16371 28990 2015 2753 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0420 22:44:14.322796 139904526645120 squad_lib.py:369] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0420 22:44:14.323022 139904526645120 squad_lib.py:370] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0420 22:44:49.193243 139904526645120 squad_lib.py:407] Adding padding examples to make sure no partial batch.\n",
            "I0420 22:44:49.193413 139904526645120 squad_lib.py:408] Adds 6 padding examples for inference.\n",
            "I0420 22:44:49.197922 139904526645120 run_squad_helper.py:332] ***** Running predictions *****\n",
            "I0420 22:44:49.198487 139904526645120 run_squad_helper.py:333]   Num orig examples = 10570\n",
            "I0420 22:44:49.198635 139904526645120 run_squad_helper.py:334]   Num split examples = 10650\n",
            "I0420 22:44:49.198760 139904526645120 run_squad_helper.py:335]   Batch size = 8\n",
            "2020-04-20 22:44:49.251556: W tensorflow/core/kernels/data/captured_function.cc:458] Disabling multi-device execution for a function that uses the experimental_ints_on_device attribute.\n",
            "2020-04-20 22:44:49.252356: W tensorflow/core/kernels/data/captured_function.cc:458] Disabling multi-device execution for a function that uses the experimental_ints_on_device attribute.\n",
            "WARNING:tensorflow:5 out of the last 8 calls to <function recreate_function.<locals>.restored_function_body at 0x7f3b0ec3ea60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please  define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "W0420 22:44:55.936436 139904526645120 def_function.py:119] 5 out of the last 8 calls to <function recreate_function.<locals>.restored_function_body at 0x7f3b0ec3ea60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please  define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "I0420 22:44:56.215641 139904526645120 run_squad_helper.py:189] Restoring checkpoints from /mydrive/bert_finetuning_outputs/ctl_step_21936.ckpt-2\n",
            "WARNING:tensorflow:6 out of the last 11 calls to <function recreate_function.<locals>.restored_function_body at 0x7f3b0ec3ea60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please  define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "W0420 22:44:57.601665 139891598403328 def_function.py:119] 6 out of the last 11 calls to <function recreate_function.<locals>.restored_function_body at 0x7f3b0ec3ea60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please  define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "I0420 22:45:03.994854 139904526645120 run_squad_helper.py:216] Made predictions for 200 records.\n",
            "I0420 22:45:08.705829 139904526645120 run_squad_helper.py:216] Made predictions for 400 records.\n",
            "I0420 22:45:13.429950 139904526645120 run_squad_helper.py:216] Made predictions for 600 records.\n",
            "I0420 22:45:18.151204 139904526645120 run_squad_helper.py:216] Made predictions for 800 records.\n",
            "I0420 22:45:22.883185 139904526645120 run_squad_helper.py:216] Made predictions for 1000 records.\n",
            "I0420 22:45:27.589026 139904526645120 run_squad_helper.py:216] Made predictions for 1200 records.\n",
            "I0420 22:45:32.296429 139904526645120 run_squad_helper.py:216] Made predictions for 1400 records.\n",
            "I0420 22:45:37.008822 139904526645120 run_squad_helper.py:216] Made predictions for 1600 records.\n",
            "I0420 22:45:41.728224 139904526645120 run_squad_helper.py:216] Made predictions for 1800 records.\n",
            "I0420 22:45:46.464212 139904526645120 run_squad_helper.py:216] Made predictions for 2000 records.\n",
            "I0420 22:45:51.173946 139904526645120 run_squad_helper.py:216] Made predictions for 2200 records.\n",
            "I0420 22:45:55.877033 139904526645120 run_squad_helper.py:216] Made predictions for 2400 records.\n",
            "I0420 22:46:00.581340 139904526645120 run_squad_helper.py:216] Made predictions for 2600 records.\n",
            "I0420 22:46:05.284352 139904526645120 run_squad_helper.py:216] Made predictions for 2800 records.\n",
            "I0420 22:46:09.999274 139904526645120 run_squad_helper.py:216] Made predictions for 3000 records.\n",
            "I0420 22:46:14.712797 139904526645120 run_squad_helper.py:216] Made predictions for 3200 records.\n",
            "I0420 22:46:19.417431 139904526645120 run_squad_helper.py:216] Made predictions for 3400 records.\n",
            "I0420 22:46:24.138769 139904526645120 run_squad_helper.py:216] Made predictions for 3600 records.\n",
            "I0420 22:46:28.851788 139904526645120 run_squad_helper.py:216] Made predictions for 3800 records.\n",
            "I0420 22:46:33.570211 139904526645120 run_squad_helper.py:216] Made predictions for 4000 records.\n",
            "I0420 22:46:38.263759 139904526645120 run_squad_helper.py:216] Made predictions for 4200 records.\n",
            "I0420 22:46:42.969519 139904526645120 run_squad_helper.py:216] Made predictions for 4400 records.\n",
            "I0420 22:46:47.672791 139904526645120 run_squad_helper.py:216] Made predictions for 4600 records.\n",
            "I0420 22:46:52.360912 139904526645120 run_squad_helper.py:216] Made predictions for 4800 records.\n",
            "I0420 22:46:57.063907 139904526645120 run_squad_helper.py:216] Made predictions for 5000 records.\n",
            "I0420 22:47:01.765558 139904526645120 run_squad_helper.py:216] Made predictions for 5200 records.\n",
            "I0420 22:47:06.465041 139904526645120 run_squad_helper.py:216] Made predictions for 5400 records.\n",
            "I0420 22:47:11.184505 139904526645120 run_squad_helper.py:216] Made predictions for 5600 records.\n",
            "I0420 22:47:15.877136 139904526645120 run_squad_helper.py:216] Made predictions for 5800 records.\n",
            "I0420 22:47:20.575603 139904526645120 run_squad_helper.py:216] Made predictions for 6000 records.\n",
            "I0420 22:47:25.285771 139904526645120 run_squad_helper.py:216] Made predictions for 6200 records.\n",
            "I0420 22:47:29.970307 139904526645120 run_squad_helper.py:216] Made predictions for 6400 records.\n",
            "I0420 22:47:34.673005 139904526645120 run_squad_helper.py:216] Made predictions for 6600 records.\n",
            "I0420 22:47:39.359702 139904526645120 run_squad_helper.py:216] Made predictions for 6800 records.\n",
            "I0420 22:47:44.075354 139904526645120 run_squad_helper.py:216] Made predictions for 7000 records.\n",
            "I0420 22:47:49.096231 139904526645120 run_squad_helper.py:216] Made predictions for 7200 records.\n",
            "I0420 22:47:53.801461 139904526645120 run_squad_helper.py:216] Made predictions for 7400 records.\n",
            "I0420 22:47:58.498681 139904526645120 run_squad_helper.py:216] Made predictions for 7600 records.\n",
            "I0420 22:48:03.199521 139904526645120 run_squad_helper.py:216] Made predictions for 7800 records.\n",
            "I0420 22:48:07.895757 139904526645120 run_squad_helper.py:216] Made predictions for 8000 records.\n",
            "I0420 22:48:12.601211 139904526645120 run_squad_helper.py:216] Made predictions for 8200 records.\n",
            "I0420 22:48:17.294490 139904526645120 run_squad_helper.py:216] Made predictions for 8400 records.\n",
            "I0420 22:48:21.994778 139904526645120 run_squad_helper.py:216] Made predictions for 8600 records.\n",
            "I0420 22:48:26.678318 139904526645120 run_squad_helper.py:216] Made predictions for 8800 records.\n",
            "I0420 22:48:31.374160 139904526645120 run_squad_helper.py:216] Made predictions for 9000 records.\n",
            "I0420 22:48:36.053799 139904526645120 run_squad_helper.py:216] Made predictions for 9200 records.\n",
            "I0420 22:48:40.747599 139904526645120 run_squad_helper.py:216] Made predictions for 9400 records.\n",
            "I0420 22:48:45.444939 139904526645120 run_squad_helper.py:216] Made predictions for 9600 records.\n",
            "I0420 22:48:50.148407 139904526645120 run_squad_helper.py:216] Made predictions for 9800 records.\n",
            "I0420 22:48:54.836877 139904526645120 run_squad_helper.py:216] Made predictions for 10000 records.\n",
            "I0420 22:48:59.520454 139904526645120 run_squad_helper.py:216] Made predictions for 10200 records.\n",
            "I0420 22:49:04.219332 139904526645120 run_squad_helper.py:216] Made predictions for 10400 records.\n",
            "I0420 22:49:08.911295 139904526645120 run_squad_helper.py:216] Made predictions for 10600 records.\n",
            "I0420 22:49:48.885134 139904526645120 run_squad_helper.py:363] Writing predictions to: /mydrive/bert_finetuning_outputs/predictions.json\n",
            "I0420 22:49:48.885332 139904526645120 run_squad_helper.py:364] Writing nbest to: /mydrive/bert_finetuning_outputs/nbest_predictions.json\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-sF7Gcjhc0iY",
        "colab_type": "text"
      },
      "source": [
        "# Evaluate fine-tuned model using SQuAD official evaluation script"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F6KH4QMM3RyA",
        "colab_type": "text"
      },
      "source": [
        "*   Below scores can be compared with others on SQuAD leader board"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6wlFP5QEc-VE",
        "colab_type": "code",
        "outputId": "c3c609bb-b18e-4764-80e5-8634aee52a27",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "!python ${FINAL_EVALUATION_SCRIPT} ${SQUAD_PRED_FILE} ${OUTPUT_DIR}/predictions.json"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{\"exact_match\": 79.60264900662251, \"f1\": 87.47596501756003}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "kU987fLF1Ryi"
      },
      "source": [
        "# Save BERT fine-tuned model as TF 2.0 model in saved model format"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rCWJDP9n14N5",
        "colab_type": "code",
        "outputId": "80f5494c-3f0f-4cbf-92b7-1e056a4b0489",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "#################################################################################################\n",
        "# This script saves the fine-tuned model in 'saved_model' format for inference.\n",
        "#################################################################################################\n",
        "print(\"\\nModel is exported to: \" + os.environ['OUTPUT_DIR'] + '/exported_model' + \"\\n\")\n",
        "!python ${FINETUNING_TRG_SCRIPT} \\\n",
        "  --input_meta_data_path=${OUTPUT_DIR}/squad_${SQUAD_VERSION}_meta_data \\\n",
        "  --vocab_file=${OUTPUT_DIR}/vocab.txt \\\n",
        "  --bert_config_file=${OUTPUT_DIR}/bert_config.json \\\n",
        "  --mode=export_only \\\n",
        "  --model_dir=${OUTPUT_DIR} \\\n",
        "  --model_export_path=${OUTPUT_DIR}/exported_model \\\n",
        "  --init_checkpoint=${OUTPUT_DIR}/ctl_step_21936.ckpt-2"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "      dtype=float32)>, <tf.Variable 'transformer/layer_2/output_layer_norm/gamma:0' shape=(768,) dtype=float32, numpy=\n",
            "array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1.], dtype=float32)>, <tf.Variable 'transformer/layer_10/intermediate/kernel:0' shape=(768, 3072) dtype=float32, numpy=\n",
            "array([[ 0.0059864 , -0.0063958 , -0.01989965, ..., -0.02066928,\n",
            "         0.00227956,  0.01160802],\n",
            "       [-0.03633586,  0.00177276, -0.00064545, ...,  0.0299839 ,\n",
            "         0.00831256, -0.00957523],\n",
            "       [-0.00795144, -0.02377489, -0.01610888, ..., -0.00635925,\n",
            "         0.01654304,  0.00506604],\n",
            "       ...,\n",
            "       [-0.00343223, -0.01521706,  0.01278107, ..., -0.00483005,\n",
            "         0.00012815,  0.01803523],\n",
            "       [ 0.029449  , -0.00654653, -0.03348087, ...,  0.01001043,\n",
            "         0.00446961,  0.01517664],\n",
            "       [ 0.00200253,  0.00100216, -0.01248415, ..., -0.02429055,\n",
            "         0.01656369, -0.0209066 ]], dtype=float32)>, <tf.Variable 'transformer/layer_0/self_attention/query/kernel:0' shape=(768, 12, 64) dtype=float32, numpy=\n",
            "array([[[ 9.40925814e-03,  1.80902518e-03, -2.01751105e-02, ...,\n",
            "         -9.71686910e-04, -1.30842635e-02, -1.71577111e-02],\n",
            "        [-1.43387467e-02, -3.60918418e-02, -2.87558474e-02, ...,\n",
            "          3.05117127e-02, -2.29623262e-02,  9.10126697e-03],\n",
            "        [-3.01701482e-02, -3.30008790e-02, -7.74750998e-03, ...,\n",
            "         -3.90278138e-02, -4.06868663e-03,  2.28587762e-02],\n",
            "        ...,\n",
            "        [-3.27742659e-03, -3.15858871e-02,  4.45405813e-03, ...,\n",
            "         -1.55322021e-02,  3.93085033e-02, -8.32098443e-03],\n",
            "        [-2.62346994e-02, -1.18134217e-02,  4.78393352e-03, ...,\n",
            "         -3.06680729e-03,  1.61495879e-02, -2.66532898e-02],\n",
            "        [-1.36584211e-02, -5.05657808e-05,  2.13166885e-02, ...,\n",
            "         -2.22028233e-02, -1.25568127e-02, -3.11406376e-03]],\n",
            "\n",
            "       [[-1.02294385e-02,  6.06630556e-03, -8.86367634e-03, ...,\n",
            "          5.83467307e-04, -5.47288393e-04, -2.63254810e-03],\n",
            "        [-4.39761672e-03,  2.35298984e-02, -2.57519316e-02, ...,\n",
            "         -1.58411171e-02, -2.00492442e-02, -8.03389866e-03],\n",
            "        [-2.79645845e-02, -5.72992954e-03, -4.83777048e-03, ...,\n",
            "         -1.11579793e-02,  3.27495486e-02,  1.08033735e-02],\n",
            "        ...,\n",
            "        [-2.01821141e-02, -2.78074909e-02, -2.47470383e-02, ...,\n",
            "          1.40205221e-02,  8.13838094e-03, -2.96602044e-02],\n",
            "        [-1.34418923e-02,  8.45589768e-03,  1.14203030e-02, ...,\n",
            "         -2.15493720e-02, -8.39546695e-03, -7.90176913e-03],\n",
            "        [-4.33364417e-04,  1.43700540e-02,  1.57825258e-02, ...,\n",
            "          3.08865681e-02,  1.95522849e-02, -1.55337593e-02]],\n",
            "\n",
            "       [[ 1.65508948e-02, -5.32749295e-03,  1.35296155e-02, ...,\n",
            "          6.31280290e-03, -7.52690993e-03,  1.28013557e-02],\n",
            "        [-2.38585118e-02,  2.18513561e-03,  2.57023666e-02, ...,\n",
            "         -1.13606465e-03, -1.67392232e-02, -7.72383297e-03],\n",
            "        [-6.83697127e-03, -1.57968588e-02, -1.33675691e-02, ...,\n",
            "         -3.54981497e-02, -6.54787337e-03,  2.56611798e-02],\n",
            "        ...,\n",
            "        [ 1.45611661e-02,  1.08842645e-02,  4.80368128e-03, ...,\n",
            "          2.06431709e-02, -2.60129049e-02,  2.86834631e-02],\n",
            "        [ 1.23147003e-03, -3.25423889e-02, -2.29396615e-02, ...,\n",
            "         -9.22562182e-03,  1.22802267e-02,  1.84418056e-02],\n",
            "        [ 1.86534841e-02,  2.54647415e-02,  1.54463304e-02, ...,\n",
            "          1.11453934e-02, -1.86200049e-02, -2.61018309e-03]],\n",
            "\n",
            "       ...,\n",
            "\n",
            "       [[-1.23936459e-02, -9.77815129e-03, -2.42773211e-03, ...,\n",
            "          7.48591637e-03, -3.08839343e-02, -1.40867671e-02],\n",
            "        [-1.09547772e-03,  2.15501082e-03,  8.78302287e-03, ...,\n",
            "          9.21741687e-03, -2.29976270e-02, -4.23051277e-03],\n",
            "        [ 1.96726136e-02,  2.10985653e-02, -1.52381388e-02, ...,\n",
            "          1.51316365e-02, -1.18776988e-02, -4.79609845e-03],\n",
            "        ...,\n",
            "        [-1.25408620e-02, -1.01525281e-02, -2.89336895e-03, ...,\n",
            "         -3.06701777e-03, -1.46699725e-02,  2.95795798e-02],\n",
            "        [ 7.69320270e-03, -1.90593507e-02,  4.77635476e-05, ...,\n",
            "         -1.02158552e-02,  6.25564065e-03, -3.33621465e-02],\n",
            "        [-2.24422831e-02,  2.65521314e-02,  8.33032373e-03, ...,\n",
            "         -1.03896558e-02, -3.45369130e-02, -1.97190046e-03]],\n",
            "\n",
            "       [[-1.91862078e-03, -3.18884440e-02, -8.51060078e-03, ...,\n",
            "         -1.57863311e-02,  6.68319035e-03, -1.60115138e-02],\n",
            "        [ 1.50145339e-02, -2.06952374e-02,  1.73398983e-02, ...,\n",
            "          5.58060734e-03,  9.57977865e-03, -2.70659849e-02],\n",
            "        [ 1.84715092e-02,  1.84373930e-02, -3.22417286e-03, ...,\n",
            "         -1.99966878e-02,  7.34034227e-03, -3.73395495e-02],\n",
            "        ...,\n",
            "        [ 1.19358618e-02, -2.54371017e-02, -2.38797031e-02, ...,\n",
            "         -1.02331610e-02,  2.50217249e-03, -2.29981467e-02],\n",
            "        [-7.06784753e-03,  2.85415817e-02, -1.58841517e-02, ...,\n",
            "          1.94636639e-02,  6.14626857e-04, -2.54536867e-02],\n",
            "        [-1.67294871e-02,  3.11311218e-03, -3.67166623e-02, ...,\n",
            "         -2.52169836e-02, -5.28371241e-03,  2.79661082e-02]],\n",
            "\n",
            "       [[ 5.34706470e-03,  7.20829194e-05, -2.47808062e-02, ...,\n",
            "         -5.50366053e-03,  1.70872975e-02, -3.57465283e-03],\n",
            "        [-3.72724864e-03, -2.16912664e-02, -2.12044157e-02, ...,\n",
            "         -6.57059066e-03, -2.60695419e-03,  1.07579073e-02],\n",
            "        [-3.38350087e-02,  5.24014328e-03, -8.61169118e-03, ...,\n",
            "         -1.03854509e-02,  1.58416256e-02, -5.19258669e-03],\n",
            "        ...,\n",
            "        [ 3.47729810e-02, -5.00302250e-03,  9.40466486e-03, ...,\n",
            "          2.43952479e-02,  3.34789343e-02, -9.26106004e-04],\n",
            "        [-3.69736156e-03,  4.96001262e-03,  9.80595499e-03, ...,\n",
            "         -1.53272264e-02,  1.28721762e-02, -1.55437551e-02],\n",
            "        [ 1.09707499e-02,  7.73946289e-03, -1.50728635e-02, ...,\n",
            "         -2.82685701e-02, -1.73543897e-02,  7.91062484e-04]]],\n",
            "      dtype=float32)>, <tf.Variable 'transformer/layer_10/output_layer_norm/gamma:0' shape=(768,) dtype=float32, numpy=\n",
            "array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1.], dtype=float32)>, <tf.Variable 'word_embeddings/embeddings:0' shape=(30522, 768) dtype=float32, numpy=\n",
            "array([[ 0.01161963,  0.00297617,  0.01149797, ..., -0.00122651,\n",
            "         0.01670342, -0.02002489],\n",
            "       [-0.02135416, -0.00525723, -0.0052669 , ..., -0.00228582,\n",
            "        -0.01896088,  0.01652175],\n",
            "       [ 0.0034343 , -0.01861213, -0.01358107, ..., -0.00569148,\n",
            "         0.0030779 ,  0.03612855],\n",
            "       ...,\n",
            "       [-0.0091617 , -0.01529441, -0.00734753, ...,  0.0041876 ,\n",
            "         0.00708327, -0.00615302],\n",
            "       [ 0.01632316, -0.00772212, -0.03549284, ..., -0.00598726,\n",
            "         0.02807091,  0.02761985],\n",
            "       [-0.00572688, -0.01734021,  0.02332212, ...,  0.00197938,\n",
            "         0.0304336 ,  0.02108554]], dtype=float32)>, <tf.Variable 'type_embeddings/embeddings:0' shape=(2, 768) dtype=float32, numpy=\n",
            "array([[ 0.01131463, -0.01550944, -0.01022075, ..., -0.02060414,\n",
            "         0.02109537, -0.03259442],\n",
            "       [ 0.01247516, -0.01832894, -0.00171133, ...,  0.00329423,\n",
            "        -0.00525746,  0.00847103]], dtype=float32)>, <tf.Variable 'transformer/layer_5/self_attention/key/bias:0' shape=(12, 64) dtype=float32, numpy=\n",
            "array([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
            "      dtype=float32)>, <tf.Variable 'transformer/layer_3/self_attention/value/kernel:0' shape=(768, 12, 64) dtype=float32, numpy=\n",
            "array([[[-0.00402057, -0.03009354,  0.02798349, ..., -0.00121462,\n",
            "         -0.03723224, -0.01253059],\n",
            "        [ 0.02810772, -0.01215868,  0.02856105, ...,  0.02078455,\n",
            "          0.00173005, -0.01773972],\n",
            "        [-0.03362295, -0.00054824, -0.03944938, ...,  0.03188918,\n",
            "         -0.00875186,  0.01044784],\n",
            "        ...,\n",
            "        [ 0.01355899, -0.03493598, -0.00019231, ..., -0.00666324,\n",
            "          0.00907715, -0.00619759],\n",
            "        [ 0.00561872, -0.0301684 ,  0.01144139, ..., -0.02595799,\n",
            "          0.00165018, -0.00946844],\n",
            "        [-0.0020702 , -0.01067734,  0.01557139, ..., -0.01447313,\n",
            "         -0.00546558, -0.00904837]],\n",
            "\n",
            "       [[-0.03956513, -0.02145416, -0.01470981, ..., -0.00128182,\n",
            "          0.00987436, -0.0217861 ],\n",
            "        [ 0.01851988, -0.02566631, -0.00662116, ...,  0.00525957,\n",
            "          0.00081094,  0.00397285],\n",
            "        [ 0.02318681, -0.02499409, -0.00553066, ..., -0.00551572,\n",
            "         -0.0118347 , -0.03599324],\n",
            "        ...,\n",
            "        [-0.03053052,  0.00274023, -0.035712  , ...,  0.00890901,\n",
            "         -0.00447287, -0.0232912 ],\n",
            "        [ 0.03748339, -0.02750835,  0.02545915, ..., -0.0196877 ,\n",
            "          0.00867999, -0.00850415],\n",
            "        [ 0.01421682,  0.01728253, -0.02761672, ...,  0.02122985,\n",
            "          0.01237344,  0.01595462]],\n",
            "\n",
            "       [[ 0.00885938,  0.00359387, -0.00543866, ...,  0.02011806,\n",
            "         -0.00150933, -0.02272957],\n",
            "        [-0.02645611, -0.0030489 , -0.01575782, ...,  0.00762481,\n",
            "         -0.0138207 ,  0.03572115],\n",
            "        [-0.02659173, -0.00196957,  0.00483016, ..., -0.01097043,\n",
            "         -0.00186428,  0.02778539],\n",
            "        ...,\n",
            "        [ 0.00057294,  0.0043152 , -0.00217939, ..., -0.0285444 ,\n",
            "         -0.00100998, -0.02056869],\n",
            "        [ 0.00111302, -0.02497129, -0.02646185, ..., -0.03984946,\n",
            "          0.0240177 , -0.01410563],\n",
            "        [ 0.03605508,  0.00262068, -0.02102239, ...,  0.01269474,\n",
            "          0.03602551,  0.0069202 ]],\n",
            "\n",
            "       ...,\n",
            "\n",
            "       [[ 0.00450339, -0.00018143, -0.00314046, ..., -0.00165795,\n",
            "         -0.00791928, -0.01841296],\n",
            "        [ 0.01791692,  0.01556438,  0.00971411, ..., -0.00256541,\n",
            "          0.01510971, -0.0244642 ],\n",
            "        [-0.0046    ,  0.02211426, -0.00768378, ..., -0.00464553,\n",
            "          0.01264881,  0.0264808 ],\n",
            "        ...,\n",
            "        [ 0.02166558,  0.02134417, -0.00809399, ...,  0.0063239 ,\n",
            "          0.00561795, -0.01892645],\n",
            "        [-0.0246561 , -0.01783856,  0.03212632, ...,  0.0037608 ,\n",
            "          0.00306095, -0.00286951],\n",
            "        [-0.00712279, -0.00261111,  0.00230875, ...,  0.0021675 ,\n",
            "          0.03013713, -0.01692913]],\n",
            "\n",
            "       [[ 0.02433289,  0.00888482,  0.00936763, ..., -0.02835622,\n",
            "         -0.00213304,  0.0208712 ],\n",
            "        [-0.0002911 , -0.00197292,  0.01331495, ...,  0.00251274,\n",
            "          0.01067432, -0.01744081],\n",
            "        [ 0.01011358, -0.02512969,  0.00653661, ..., -0.01108111,\n",
            "         -0.01534024, -0.00895097],\n",
            "        ...,\n",
            "        [-0.01521061, -0.0066598 ,  0.01099017, ...,  0.00831612,\n",
            "         -0.02648812,  0.01608714],\n",
            "        [ 0.00311523,  0.01124971, -0.00207913, ..., -0.0169322 ,\n",
            "          0.00632486, -0.00078678],\n",
            "        [-0.03487792, -0.00330516,  0.00720514, ..., -0.01368342,\n",
            "          0.01308656,  0.01454437]],\n",
            "\n",
            "       [[-0.01680049,  0.01147355, -0.0099048 , ..., -0.00296808,\n",
            "         -0.01009963, -0.0086949 ],\n",
            "        [-0.00144347, -0.00070612,  0.0026755 , ..., -0.01183893,\n",
            "          0.01190915,  0.00782262],\n",
            "        [ 0.00517379, -0.00255913,  0.02320257, ...,  0.0083497 ,\n",
            "          0.01475927,  0.00136037],\n",
            "        ...,\n",
            "        [-0.00864197,  0.00984478, -0.02021938, ...,  0.00746202,\n",
            "          0.03422871,  0.01959084],\n",
            "        [ 0.00584603, -0.00760288, -0.02127695, ...,  0.00495559,\n",
            "         -0.01990252,  0.00371421],\n",
            "        [ 0.01599632,  0.00680812,  0.00014539, ...,  0.00911029,\n",
            "          0.00608264, -0.0228999 ]]], dtype=float32)>, <tf.Variable 'transformer/layer_9/self_attention/key/kernel:0' shape=(768, 12, 64) dtype=float32, numpy=\n",
            "array([[[-1.41454600e-02,  1.82643607e-02,  2.91648526e-02, ...,\n",
            "          1.75996812e-03,  1.95707195e-02, -6.36618258e-03],\n",
            "        [ 5.66392997e-03, -4.00270196e-03,  2.56965589e-02, ...,\n",
            "          9.77684278e-03,  2.03530826e-02, -4.05412260e-03],\n",
            "        [ 1.61531463e-03,  2.11816441e-05,  2.25541485e-03, ...,\n",
            "          2.15734709e-02,  3.28071676e-02,  8.24022945e-03],\n",
            "        ...,\n",
            "        [-1.06054433e-02, -2.41783098e-03, -1.29869319e-02, ...,\n",
            "         -1.32207805e-02, -3.49204428e-02, -1.91267971e-02],\n",
            "        [-2.41023232e-03,  2.18417570e-02, -2.67071333e-02, ...,\n",
            "          6.49861665e-03, -1.54415024e-02,  1.50165102e-02],\n",
            "        [ 3.14458087e-02,  1.79346483e-02,  9.07957554e-03, ...,\n",
            "         -9.63040534e-03,  1.68542285e-02, -1.14622116e-02]],\n",
            "\n",
            "       [[ 1.26542319e-02, -7.94641266e-04, -2.97443476e-03, ...,\n",
            "         -2.84732915e-02,  1.87004935e-02,  2.53146095e-03],\n",
            "        [-8.95611104e-03, -1.64561663e-02, -2.02149116e-02, ...,\n",
            "         -3.45125305e-03,  5.14398422e-03, -1.89817492e-02],\n",
            "        [-5.52763930e-03,  3.44458818e-02, -4.78718523e-03, ...,\n",
            "          1.58801246e-02, -2.44464120e-03,  4.52709291e-03],\n",
            "        ...,\n",
            "        [ 6.45138603e-03, -2.72737769e-03,  4.53837588e-03, ...,\n",
            "         -2.15000622e-02,  7.86689203e-03,  5.58741251e-03],\n",
            "        [-2.77570616e-02,  2.26102080e-02,  1.82434684e-03, ...,\n",
            "          4.84453328e-03, -1.71822924e-02,  2.89162202e-03],\n",
            "        [-6.90184720e-03, -1.58864204e-02,  3.67483236e-02, ...,\n",
            "          2.16149967e-02,  1.65057890e-02,  2.21512979e-03]],\n",
            "\n",
            "       [[-3.92888766e-03,  7.76192686e-03, -2.40460243e-02, ...,\n",
            "         -5.92080457e-03,  6.15874771e-03, -8.02846719e-03],\n",
            "        [-3.07424068e-02,  7.92241283e-03,  1.94967706e-02, ...,\n",
            "          3.19285728e-02,  1.03968587e-02, -1.59871625e-03],\n",
            "        [-8.85888375e-03, -5.68000227e-03, -9.99252591e-03, ...,\n",
            "          2.21731868e-02, -1.44673847e-02,  1.28208213e-02],\n",
            "        ...,\n",
            "        [ 1.62792727e-02, -1.28767721e-03,  2.72824205e-02, ...,\n",
            "         -2.15984844e-02,  4.83897468e-03, -7.99883506e-04],\n",
            "        [-4.68751881e-03,  1.95427034e-02, -4.44832863e-03, ...,\n",
            "          5.11633512e-03, -2.06602109e-03, -1.80140976e-02],\n",
            "        [-8.63847788e-03,  2.67689787e-02, -2.03955583e-02, ...,\n",
            "         -3.33077833e-03, -3.10075982e-03, -3.40359402e-03]],\n",
            "\n",
            "       ...,\n",
            "\n",
            "       [[ 9.32028703e-03, -2.89462134e-02, -8.34894087e-03, ...,\n",
            "          5.35920681e-03, -1.06685627e-02, -1.22801000e-02],\n",
            "        [-1.87494271e-02,  2.10743733e-02,  1.01749729e-02, ...,\n",
            "         -1.88979805e-02, -2.89787911e-02,  2.41902582e-02],\n",
            "        [-2.57058926e-02, -2.40434911e-02, -6.67319028e-03, ...,\n",
            "         -1.88717190e-02,  6.45225728e-03,  1.76364481e-02],\n",
            "        ...,\n",
            "        [-7.11390190e-03,  2.11828481e-02, -1.20310551e-02, ...,\n",
            "         -2.34243628e-02, -1.71971023e-02, -1.84389707e-02],\n",
            "        [-2.79561505e-02, -2.98865177e-02,  3.67887728e-02, ...,\n",
            "          6.85260957e-03, -1.82490740e-02,  3.53896618e-02],\n",
            "        [-2.27483502e-03, -1.71471350e-02,  1.89702597e-03, ...,\n",
            "          1.63587965e-02,  1.22697903e-02,  4.81202267e-04]],\n",
            "\n",
            "       [[ 3.02034393e-02,  1.18012987e-02,  6.00685040e-03, ...,\n",
            "          1.58728566e-02,  1.65090915e-02, -9.19269584e-03],\n",
            "        [-3.71615700e-02, -2.24291682e-02,  2.85470262e-02, ...,\n",
            "         -1.74657889e-02, -1.20095955e-03,  1.03339637e-02],\n",
            "        [ 3.23034003e-02,  4.25915606e-03,  8.61833151e-03, ...,\n",
            "         -8.60942900e-03, -2.82689948e-02, -1.23936720e-02],\n",
            "        ...,\n",
            "        [ 7.72266369e-03, -5.72876399e-03, -3.28638218e-02, ...,\n",
            "         -1.90129317e-02,  2.61210538e-02, -1.28245587e-02],\n",
            "        [-3.06471105e-04, -1.35898972e-02,  1.72018816e-04, ...,\n",
            "         -8.29095859e-03, -1.34210167e-02,  1.17372703e-02],\n",
            "        [-3.25980037e-02,  5.07304678e-03, -1.08499844e-02, ...,\n",
            "         -1.73507389e-02, -1.98749136e-02,  3.71674891e-03]],\n",
            "\n",
            "       [[ 1.19129373e-02,  6.72182941e-04,  4.25258093e-03, ...,\n",
            "         -2.46361159e-02,  1.79916900e-02, -9.85313393e-03],\n",
            "        [ 1.28163947e-02,  2.50665732e-02,  3.36125493e-02, ...,\n",
            "          2.34801266e-02, -6.23545470e-03,  7.14951521e-03],\n",
            "        [-6.20440906e-03,  7.15789245e-03,  1.70536581e-02, ...,\n",
            "          9.13079455e-03,  2.15259133e-04, -2.02065124e-03],\n",
            "        ...,\n",
            "        [ 1.82279944e-02,  2.72912737e-02,  1.64327621e-02, ...,\n",
            "          6.72344118e-03, -1.14489757e-02, -2.07480397e-02],\n",
            "        [-1.08808763e-02, -5.43731544e-03,  2.59001628e-02, ...,\n",
            "          1.79093250e-03,  8.37131031e-03, -7.86334369e-03],\n",
            "        [ 3.44757661e-02,  1.39004299e-02, -4.22728008e-05, ...,\n",
            "         -8.37302767e-03, -2.16185488e-02, -1.26358075e-02]]],\n",
            "      dtype=float32)>, <tf.Variable 'transformer/layer_0/intermediate/kernel:0' shape=(768, 3072) dtype=float32, numpy=\n",
            "array([[-0.00109807, -0.02592766, -0.01787044, ..., -0.00019507,\n",
            "         0.02039525,  0.00346563],\n",
            "       [ 0.03696458, -0.00349348,  0.01447714, ...,  0.03737544,\n",
            "        -0.00507063, -0.00630927],\n",
            "       [-0.01539542, -0.02422782,  0.01643895, ..., -0.03562268,\n",
            "        -0.02940766, -0.01200288],\n",
            "       ...,\n",
            "       [-0.02110136,  0.01484389,  0.00473079, ..., -0.01269241,\n",
            "         0.016117  ,  0.01617786],\n",
            "       [ 0.0158278 ,  0.01438258, -0.00178714, ..., -0.00526944,\n",
            "        -0.00089647, -0.01628892],\n",
            "       [-0.01850039,  0.00662138,  0.00240279, ...,  0.01574433,\n",
            "        -0.00671751, -0.01115138]], dtype=float32)>, <tf.Variable 'transformer/layer_3/output/bias:0' shape=(768,) dtype=float32, numpy=\n",
            "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0.], dtype=float32)>, <tf.Variable 'transformer/layer_5/self_attention/query/bias:0' shape=(12, 64) dtype=float32, numpy=\n",
            "array([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
            "      dtype=float32)>, <tf.Variable 'transformer/layer_6/self_attention_output/bias:0' shape=(768,) dtype=float32, numpy=\n",
            "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0.], dtype=float32)>, <tf.Variable 'transformer/layer_0/self_attention_layer_norm/beta:0' shape=(768,) dtype=float32, numpy=\n",
            "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0.], dtype=float32)>, <tf.Variable 'transformer/layer_6/self_attention_layer_norm/beta:0' shape=(768,) dtype=float32, numpy=\n",
            "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0.], dtype=float32)>, <tf.Variable 'transformer/layer_4/self_attention/key/kernel:0' shape=(768, 12, 64) dtype=float32, numpy=\n",
            "array([[[ 0.03074174,  0.02448618,  0.02877791, ...,  0.02871274,\n",
            "          0.01615836,  0.00017987],\n",
            "        [ 0.01640031,  0.03665626, -0.01098154, ..., -0.02496399,\n",
            "          0.02263141,  0.03506529],\n",
            "        [ 0.03746005, -0.03227579,  0.00657207, ...,  0.01508348,\n",
            "         -0.03671423,  0.0062838 ],\n",
            "        ...,\n",
            "        [-0.00394339, -0.01492562,  0.00172077, ..., -0.00536892,\n",
            "          0.00127554,  0.0179056 ],\n",
            "        [-0.00125429, -0.0013226 , -0.01553928, ...,  0.00569992,\n",
            "         -0.01246163,  0.02275122],\n",
            "        [ 0.02892493,  0.03057181, -0.01679878, ..., -0.01130249,\n",
            "         -0.00582115, -0.02605346]],\n",
            "\n",
            "       [[ 0.01944887,  0.00041843, -0.00972329, ..., -0.03905564,\n",
            "         -0.01650868, -0.00798498],\n",
            "        [ 0.00140826,  0.00582689, -0.02118964, ...,  0.00909817,\n",
            "         -0.00179381,  0.00230244],\n",
            "        [-0.01228523, -0.03294408,  0.01132436, ...,  0.02260468,\n",
            "          0.00731166, -0.00054098],\n",
            "        ...,\n",
            "        [-0.02637139, -0.01700984, -0.0033475 , ..., -0.00152994,\n",
            "         -0.01385767, -0.00923263],\n",
            "        [-0.00196801, -0.02594148, -0.00825627, ...,  0.01959021,\n",
            "         -0.01047088,  0.01768215],\n",
            "        [ 0.00429635,  0.02114758, -0.02892894, ...,  0.00379466,\n",
            "         -0.00692075,  0.00849456]],\n",
            "\n",
            "       [[ 0.0074006 , -0.01069153, -0.01056436, ..., -0.02194096,\n",
            "          0.00102503, -0.03910324],\n",
            "        [ 0.00831408,  0.01848806,  0.0070973 , ...,  0.02871814,\n",
            "          0.01835876, -0.0091183 ],\n",
            "        [ 0.00136671,  0.01023783, -0.00239522, ..., -0.0125908 ,\n",
            "         -0.01682754,  0.02960804],\n",
            "        ...,\n",
            "        [-0.00084923, -0.0185984 , -0.00641815, ...,  0.02642521,\n",
            "         -0.0026539 ,  0.03279082],\n",
            "        [ 0.00426981,  0.03144978, -0.03751299, ..., -0.00740399,\n",
            "         -0.00299231, -0.00458197],\n",
            "        [-0.0054338 , -0.0316207 ,  0.00407441, ...,  0.01115044,\n",
            "         -0.01094877, -0.01763393]],\n",
            "\n",
            "       ...,\n",
            "\n",
            "       [[-0.0272708 , -0.03105428,  0.03103559, ..., -0.00974126,\n",
            "          0.00303335,  0.01679854],\n",
            "        [-0.00096186, -0.02202434, -0.00067938, ..., -0.00507693,\n",
            "         -0.00309499, -0.01252981],\n",
            "        [ 0.02011223,  0.01494167,  0.00803969, ...,  0.01931604,\n",
            "         -0.00673638, -0.02504715],\n",
            "        ...,\n",
            "        [-0.0079218 , -0.00251457, -0.0165382 , ..., -0.00637944,\n",
            "          0.00735051,  0.01263397],\n",
            "        [-0.03495971, -0.03717443, -0.00196065, ..., -0.0075881 ,\n",
            "         -0.00701232,  0.01655213],\n",
            "        [ 0.01764615,  0.03935349,  0.0078413 , ..., -0.01225172,\n",
            "          0.0011627 , -0.00596529]],\n",
            "\n",
            "       [[-0.01176402, -0.02471456,  0.00227273, ..., -0.02911141,\n",
            "          0.01861425,  0.02179741],\n",
            "        [ 0.02067716,  0.00687186, -0.0249061 , ...,  0.01234764,\n",
            "         -0.01195035,  0.02407973],\n",
            "        [ 0.02259114,  0.02585362, -0.0053297 , ...,  0.01827023,\n",
            "         -0.02266692, -0.00952883],\n",
            "        ...,\n",
            "        [-0.02981583, -0.00067969,  0.01344015, ...,  0.00314874,\n",
            "         -0.03277122, -0.00598906],\n",
            "        [ 0.02970751,  0.00621902, -0.00511629, ...,  0.00775657,\n",
            "          0.00086546, -0.01027985],\n",
            "        [-0.03420833,  0.00348293, -0.01982772, ...,  0.0027825 ,\n",
            "          0.00538612, -0.02052291]],\n",
            "\n",
            "       [[-0.01091867, -0.00751152, -0.01791682, ...,  0.00260993,\n",
            "         -0.03213503, -0.00059857],\n",
            "        [-0.0207453 , -0.01611053,  0.01511438, ..., -0.01954374,\n",
            "         -0.00145176, -0.0030525 ],\n",
            "        [ 0.02224254, -0.00846305,  0.02118967, ..., -0.01601947,\n",
            "         -0.02199985, -0.01361128],\n",
            "        ...,\n",
            "        [-0.00971863,  0.01196448,  0.00491459, ...,  0.03639223,\n",
            "         -0.02708472,  0.03562772],\n",
            "        [-0.01526424,  0.00035769,  0.00198446, ..., -0.02416852,\n",
            "          0.00062245,  0.03274436],\n",
            "        [ 0.00417638, -0.02452478, -0.00685436, ..., -0.02325743,\n",
            "         -0.00956123,  0.00146895]]], dtype=float32)>, <tf.Variable 'transformer/layer_2/output/kernel:0' shape=(3072, 768) dtype=float32, numpy=\n",
            "array([[-0.01158383,  0.00709947,  0.00594062, ...,  0.00497546,\n",
            "         0.01902045,  0.01875923],\n",
            "       [-0.00440074,  0.00494192, -0.01506975, ..., -0.01371221,\n",
            "        -0.02959498,  0.03347103],\n",
            "       [-0.02705275,  0.0030323 ,  0.0364528 , ...,  0.00955489,\n",
            "         0.00655145,  0.00376937],\n",
            "       ...,\n",
            "       [ 0.00994883,  0.02888902, -0.0193994 , ..., -0.00950911,\n",
            "        -0.00318175,  0.01477685],\n",
            "       [ 0.01678074, -0.01800873,  0.00133404, ..., -0.00135087,\n",
            "         0.01290282,  0.0171307 ],\n",
            "       [ 0.03419185, -0.0087023 , -0.0047025 , ..., -0.00193177,\n",
            "        -0.01864174, -0.03648484]], dtype=float32)>, <tf.Variable 'transformer/layer_1/self_attention/value/kernel:0' shape=(768, 12, 64) dtype=float32, numpy=\n",
            "array([[[ 0.00061068, -0.00656394,  0.00847711, ...,  0.01296392,\n",
            "         -0.01516981, -0.00057901],\n",
            "        [-0.01609661,  0.00619654,  0.01702886, ...,  0.00720779,\n",
            "         -0.0220341 ,  0.00943564],\n",
            "        [-0.00489424, -0.00213118,  0.02185969, ..., -0.01127158,\n",
            "          0.00403617, -0.0046526 ],\n",
            "        ...,\n",
            "        [ 0.0158171 ,  0.01963454,  0.01272012, ..., -0.0075367 ,\n",
            "          0.0181529 ,  0.00347645],\n",
            "        [ 0.02016573,  0.00157873,  0.00021417, ...,  0.0014529 ,\n",
            "         -0.0239341 , -0.00971683],\n",
            "        [ 0.01873865, -0.00060024, -0.01155488, ...,  0.015313  ,\n",
            "          0.01225246,  0.02874656]],\n",
            "\n",
            "       [[ 0.00729471, -0.0269294 ,  0.0176854 , ...,  0.01553461,\n",
            "         -0.02280242,  0.02566555],\n",
            "        [ 0.00977947,  0.00545146, -0.00126037, ..., -0.01204697,\n",
            "         -0.01663269, -0.01199981],\n",
            "        [ 0.00453955, -0.02275582, -0.00290094, ..., -0.02007703,\n",
            "         -0.00069615,  0.01742065],\n",
            "        ...,\n",
            "        [-0.00361985, -0.00544323,  0.02611908, ..., -0.01403822,\n",
            "          0.02308083,  0.01242006],\n",
            "        [-0.00875951, -0.01159231,  0.01639634, ...,  0.03212104,\n",
            "          0.0072609 ,  0.00491853],\n",
            "        [ 0.0035213 ,  0.01092308,  0.00212076, ..., -0.03995795,\n",
            "          0.03491441, -0.02504392]],\n",
            "\n",
            "       [[-0.0018746 , -0.00536005, -0.01716808, ..., -0.02810759,\n",
            "         -0.01911396,  0.01285634],\n",
            "        [ 0.03271106,  0.00054138,  0.03127887, ...,  0.016295  ,\n",
            "          0.03435534,  0.01351583],\n",
            "        [ 0.02607463,  0.03059337,  0.02008763, ...,  0.03766362,\n",
            "          0.00504713,  0.00524054],\n",
            "        ...,\n",
            "        [ 0.00855023,  0.01099792, -0.03582807, ..., -0.00242137,\n",
            "         -0.00262899,  0.00940159],\n",
            "        [ 0.01669432, -0.02123995,  0.03002628, ..., -0.00274221,\n",
            "         -0.01971009, -0.01193176],\n",
            "        [-0.03541414, -0.02417007,  0.01143983, ..., -0.02771197,\n",
            "          0.00190532,  0.00224669]],\n",
            "\n",
            "       ...,\n",
            "\n",
            "       [[-0.02478989, -0.01957313, -0.02509825, ..., -0.00741688,\n",
            "         -0.00304893,  0.03470708],\n",
            "        [-0.005819  , -0.02744318,  0.01193944, ...,  0.0017018 ,\n",
            "         -0.00267539, -0.01330737],\n",
            "        [ 0.00411501, -0.0019607 , -0.01690443, ..., -0.00218059,\n",
            "         -0.01263662, -0.01777676],\n",
            "        ...,\n",
            "        [ 0.01922789,  0.00477898, -0.02391507, ...,  0.01733179,\n",
            "         -0.01588093,  0.01887893],\n",
            "        [ 0.01093377,  0.01861396, -0.01311961, ...,  0.0039826 ,\n",
            "          0.00963153, -0.00196543],\n",
            "        [-0.02155425, -0.00171664,  0.0030054 , ...,  0.01516739,\n",
            "         -0.01339976, -0.00926425]],\n",
            "\n",
            "       [[-0.01549793, -0.03710528,  0.00984017, ..., -0.00644508,\n",
            "          0.00952738, -0.01145149],\n",
            "        [ 0.01624477,  0.02143788,  0.01542127, ...,  0.01023496,\n",
            "         -0.00065688,  0.00925775],\n",
            "        [-0.01780079, -0.02531406, -0.0313588 , ...,  0.00561821,\n",
            "          0.0375286 , -0.00985365],\n",
            "        ...,\n",
            "        [ 0.02967214, -0.03893398,  0.02512923, ...,  0.0176887 ,\n",
            "         -0.00707127, -0.0009162 ],\n",
            "        [-0.01691711, -0.03469339, -0.00488157, ..., -0.03233651,\n",
            "          0.01184732, -0.03085868],\n",
            "        [ 0.01729388,  0.01582156,  0.01428987, ..., -0.02548675,\n",
            "         -0.03187348, -0.00569566]],\n",
            "\n",
            "       [[-0.01132367,  0.03908531,  0.00268909, ...,  0.01151863,\n",
            "         -0.01233195, -0.00364613],\n",
            "        [ 0.01946015,  0.02936726,  0.01405585, ..., -0.00200612,\n",
            "         -0.00969842,  0.01098394],\n",
            "        [-0.0005196 ,  0.02399823, -0.0233671 , ..., -0.00085575,\n",
            "         -0.00900737,  0.03542074],\n",
            "        ...,\n",
            "        [ 0.01291925, -0.00726844, -0.00264995, ...,  0.00269169,\n",
            "         -0.02186184, -0.01692597],\n",
            "        [ 0.00059373,  0.00901994,  0.00720988, ..., -0.00330872,\n",
            "          0.01924507, -0.02221321],\n",
            "        [-0.01376148,  0.00841298,  0.01722341, ...,  0.00809731,\n",
            "         -0.00518893,  0.00661706]]], dtype=float32)>, <tf.Variable 'transformer/layer_3/intermediate/bias:0' shape=(3072,) dtype=float32, numpy=array([0., 0., 0., ..., 0., 0., 0.], dtype=float32)>, <tf.Variable 'transformer/layer_5/self_attention_layer_norm/gamma:0' shape=(768,) dtype=float32, numpy=\n",
            "array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1.], dtype=float32)>, <tf.Variable 'transformer/layer_11/intermediate/kernel:0' shape=(768, 3072) dtype=float32, numpy=\n",
            "array([[ 0.00133026,  0.00697444, -0.02673079, ...,  0.01558356,\n",
            "        -0.02945007,  0.00784914],\n",
            "       [ 0.01352982, -0.00054649, -0.01078958, ...,  0.00229384,\n",
            "        -0.01671277,  0.01259224],\n",
            "       [ 0.03515584,  0.01027144,  0.01500263, ...,  0.00455107,\n",
            "         0.00563707, -0.02331553],\n",
            "       ...,\n",
            "       [-0.03339776,  0.02371521,  0.02864063, ...,  0.00412439,\n",
            "        -0.00781335,  0.009713  ],\n",
            "       [-0.00303014, -0.03868338, -0.03301414, ...,  0.02163965,\n",
            "         0.02442702, -0.00355273],\n",
            "       [ 0.00525496,  0.00949341, -0.01594853, ..., -0.01775267,\n",
            "        -0.02191335,  0.00461645]], dtype=float32)>, <tf.Variable 'transformer/layer_3/self_attention/key/bias:0' shape=(12, 64) dtype=float32, numpy=\n",
            "array([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
            "      dtype=float32)>, <tf.Variable 'transformer/layer_5/self_attention_output/kernel:0' shape=(12, 64, 768) dtype=float32, numpy=\n",
            "array([[[-1.43549796e-02, -1.11230789e-02,  2.52147261e-02, ...,\n",
            "          1.16470056e-02,  1.84433237e-02, -1.86059438e-02],\n",
            "        [ 2.97559937e-03,  1.00819059e-02, -3.25969756e-02, ...,\n",
            "         -4.12301254e-03,  3.50818131e-03, -3.92329581e-02],\n",
            "        [-8.29093996e-03, -1.21494485e-02,  2.25748345e-02, ...,\n",
            "          2.65275184e-02, -5.94909117e-03, -2.32434664e-02],\n",
            "        ...,\n",
            "        [ 2.88778637e-03, -2.19818298e-02, -7.76797300e-03, ...,\n",
            "          3.99295725e-02, -3.20972456e-03, -1.93805210e-02],\n",
            "        [ 6.01616281e-04,  1.71888154e-03, -1.55566586e-02, ...,\n",
            "          2.16069557e-02,  1.55137824e-02, -1.63465794e-02],\n",
            "        [ 1.82811655e-02,  3.67930136e-03, -1.42517667e-02, ...,\n",
            "         -1.75539195e-03, -2.18522195e-02,  3.33847478e-02]],\n",
            "\n",
            "       [[-9.81678627e-03,  1.88502558e-02,  1.84913874e-02, ...,\n",
            "          2.13121846e-02,  2.37570778e-02, -3.66254221e-03],\n",
            "        [ 1.21676847e-02, -3.89372406e-04,  7.02222437e-03, ...,\n",
            "         -1.71030052e-02, -2.54917536e-02,  4.73690568e-04],\n",
            "        [ 3.46303470e-02, -7.06430525e-03,  2.58017946e-02, ...,\n",
            "          2.36025499e-03, -1.25005133e-02, -2.44075693e-02],\n",
            "        ...,\n",
            "        [-2.37266673e-03,  5.55600319e-03,  2.73248050e-02, ...,\n",
            "          4.17221396e-04, -2.56762952e-02,  5.98370621e-04],\n",
            "        [-4.27640625e-04, -9.16578458e-04, -1.36060314e-02, ...,\n",
            "          4.71119383e-05, -6.90491823e-03,  2.07080450e-02],\n",
            "        [-2.92968377e-02,  1.58303324e-02, -1.76738109e-02, ...,\n",
            "         -2.26571597e-02,  8.19043536e-03, -1.13209356e-02]],\n",
            "\n",
            "       [[ 3.41912056e-03,  1.93865411e-02,  7.07833096e-03, ...,\n",
            "         -2.42062677e-02,  1.11776786e-02, -1.28182387e-02],\n",
            "        [ 2.47766115e-02,  1.90434081e-03, -1.55656449e-02, ...,\n",
            "          1.86575223e-02, -1.73380729e-02, -8.79482366e-04],\n",
            "        [-2.22305022e-03, -4.39176057e-03, -3.60530941e-03, ...,\n",
            "          1.70507673e-02, -2.16630995e-02, -3.18458416e-02],\n",
            "        ...,\n",
            "        [-1.18479263e-02, -4.51469095e-03,  2.75050960e-02, ...,\n",
            "          4.97674476e-03,  1.73736643e-02,  7.92511180e-03],\n",
            "        [ 2.24542283e-02,  2.18776204e-02,  3.54407988e-02, ...,\n",
            "         -3.63712721e-02, -2.27637216e-02, -2.02146289e-03],\n",
            "        [-1.63608342e-02,  1.31053049e-02,  3.54750603e-02, ...,\n",
            "         -3.90548185e-02,  5.47651062e-03, -2.14562751e-02]],\n",
            "\n",
            "       ...,\n",
            "\n",
            "       [[-1.77845601e-02, -1.66877825e-02, -2.58096531e-02, ...,\n",
            "          7.64032034e-03,  1.66099407e-02,  3.25254281e-03],\n",
            "        [ 3.20886374e-02,  3.03800926e-02,  7.51057686e-03, ...,\n",
            "          1.14937183e-02,  1.72064081e-02,  1.45623852e-02],\n",
            "        [ 2.61298977e-02,  1.34577304e-02, -3.80853489e-02, ...,\n",
            "         -2.52497960e-02,  7.78151071e-03, -6.48419256e-04],\n",
            "        ...,\n",
            "        [ 7.66165694e-03,  1.79287512e-02,  3.54827079e-03, ...,\n",
            "          1.71540808e-02, -1.20758789e-03, -5.68517717e-03],\n",
            "        [ 1.58720482e-02, -2.61654938e-03, -1.04650960e-03, ...,\n",
            "         -2.33334471e-02, -6.35782350e-03, -5.47266472e-03],\n",
            "        [-1.18301827e-02,  3.98638733e-02, -1.62132960e-02, ...,\n",
            "          1.57085778e-05,  5.00702858e-03, -3.07361428e-02]],\n",
            "\n",
            "       [[-2.13830639e-02,  2.69027390e-02, -3.43421362e-02, ...,\n",
            "          3.99530679e-03,  1.88536607e-02, -2.34924853e-02],\n",
            "        [-1.86933447e-02, -8.99114087e-03,  1.39116384e-02, ...,\n",
            "         -4.65040281e-03,  4.31275647e-03, -8.25145282e-03],\n",
            "        [-1.91613641e-02,  1.40241692e-02,  2.68948469e-02, ...,\n",
            "         -3.85477615e-04, -5.41877002e-03, -7.45051028e-03],\n",
            "        ...,\n",
            "        [ 6.40783599e-03, -1.71518736e-02,  1.84231694e-03, ...,\n",
            "          2.62319464e-02,  3.44462395e-02, -8.89437739e-03],\n",
            "        [-6.12597447e-03, -3.40312645e-02, -2.28675548e-02, ...,\n",
            "          1.12231737e-02,  1.23967035e-02, -1.57983124e-03],\n",
            "        [-8.83050787e-04,  1.60450917e-02, -9.28107928e-03, ...,\n",
            "          2.08660569e-02, -9.78356134e-03, -3.04263402e-02]],\n",
            "\n",
            "       [[-9.87035595e-03,  2.74676643e-03, -4.98649012e-03, ...,\n",
            "          3.23204533e-03,  6.60808710e-03, -7.44270300e-03],\n",
            "        [ 2.11187848e-03, -2.37669908e-02,  1.07999109e-02, ...,\n",
            "          1.09806163e-02,  1.94259714e-02,  2.16946863e-02],\n",
            "        [ 1.50915356e-02, -3.77861671e-02, -1.06156850e-02, ...,\n",
            "          3.65259219e-03, -2.66613625e-03, -2.35531572e-02],\n",
            "        ...,\n",
            "        [ 2.82244175e-03, -2.54499875e-02, -3.47739793e-02, ...,\n",
            "         -8.09848495e-03,  2.17119828e-02,  2.17492245e-02],\n",
            "        [ 1.61592197e-02,  3.34031368e-03,  1.52761191e-02, ...,\n",
            "         -1.47581194e-02, -2.05902532e-02, -1.21737935e-03],\n",
            "        [ 8.01222119e-03, -6.90667098e-03, -1.98000576e-02, ...,\n",
            "         -1.31338080e-02, -1.47369383e-02, -1.74829941e-02]]],\n",
            "      dtype=float32)>, <tf.Variable 'transformer/layer_2/output_layer_norm/beta:0' shape=(768,) dtype=float32, numpy=\n",
            "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0.], dtype=float32)>, <tf.Variable 'transformer/layer_4/self_attention_layer_norm/gamma:0' shape=(768,) dtype=float32, numpy=\n",
            "array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1.], dtype=float32)>, <tf.Variable 'transformer/layer_7/self_attention/key/kernel:0' shape=(768, 12, 64) dtype=float32, numpy=\n",
            "array([[[ 4.20269324e-03,  1.47613846e-02, -6.31255889e-03, ...,\n",
            "         -5.31240227e-03,  2.14884374e-02,  2.99388692e-02],\n",
            "        [ 7.68602313e-03, -6.01649936e-03, -9.19608027e-03, ...,\n",
            "         -3.96899916e-02, -6.86297379e-03, -1.37573457e-03],\n",
            "        [ 3.05871144e-02, -3.31467669e-03, -3.17205186e-03, ...,\n",
            "         -2.61916989e-03, -3.65253985e-02,  1.13839190e-02],\n",
            "        ...,\n",
            "        [ 9.05942731e-03, -2.57362728e-03, -6.75005047e-03, ...,\n",
            "         -1.01624907e-03, -7.06809387e-03, -7.54350703e-03],\n",
            "        [-3.01535148e-02,  2.65035760e-02, -1.96359027e-02, ...,\n",
            "         -1.18194772e-02, -1.33033162e-02, -6.97706640e-03],\n",
            "        [-3.05724200e-02, -1.16578359e-02,  2.67439019e-02, ...,\n",
            "          1.87947284e-02,  1.50560448e-02,  1.86539032e-02]],\n",
            "\n",
            "       [[ 8.29290785e-03,  3.88006940e-02,  1.17259175e-02, ...,\n",
            "          7.65964869e-05, -2.50149239e-02, -1.22592319e-02],\n",
            "        [-2.42458861e-02,  3.89706492e-02, -5.57504920e-03, ...,\n",
            "         -1.53202368e-02,  1.37299821e-02,  3.63858566e-02],\n",
            "        [-1.90308522e-02, -1.02241151e-03, -1.85799804e-02, ...,\n",
            "         -1.85726825e-02,  1.87263992e-02, -1.16216752e-03],\n",
            "        ...,\n",
            "        [ 3.41204088e-03,  1.46321915e-02, -1.57665126e-02, ...,\n",
            "          1.44654531e-02,  1.88591424e-02, -1.63608678e-02],\n",
            "        [ 1.42694088e-02,  7.90397637e-03,  5.53265540e-03, ...,\n",
            "          1.08376965e-02, -2.08819322e-02, -1.52558656e-02],\n",
            "        [ 2.22560521e-02,  2.32938910e-04,  1.00996746e-02, ...,\n",
            "          3.00838556e-02, -2.47897068e-03,  5.05144475e-03]],\n",
            "\n",
            "       [[ 2.84203165e-03,  1.36189135e-02,  2.88556865e-03, ...,\n",
            "         -5.17935201e-04,  6.65520411e-03,  1.79107413e-02],\n",
            "        [-2.52268445e-02, -2.37947050e-02, -3.30321374e-03, ...,\n",
            "          5.33253839e-03,  4.89417522e-04,  2.52174353e-03],\n",
            "        [ 2.42588464e-02,  2.69054361e-02,  1.29120389e-03, ...,\n",
            "          1.37837166e-02, -9.26367752e-03,  2.09727641e-02],\n",
            "        ...,\n",
            "        [ 1.96687765e-02, -5.14878519e-03,  1.45103736e-02, ...,\n",
            "         -6.70014881e-03,  6.53451541e-03, -2.93977540e-02],\n",
            "        [-2.42160335e-02, -7.61487009e-03, -1.83440698e-03, ...,\n",
            "         -3.66406096e-03, -1.15661416e-02,  2.65660081e-02],\n",
            "        [ 1.09911179e-02, -3.57591845e-02,  7.33882235e-03, ...,\n",
            "         -2.62422040e-02,  4.47709858e-03,  2.13878397e-02]],\n",
            "\n",
            "       ...,\n",
            "\n",
            "       [[ 3.09453774e-02, -3.87020335e-02, -2.00909451e-02, ...,\n",
            "         -1.52068660e-02,  8.75427015e-03, -5.93007449e-03],\n",
            "        [-1.23407710e-02, -4.07355651e-03,  3.50823291e-02, ...,\n",
            "         -2.17555580e-03,  2.53194533e-02, -4.62163659e-03],\n",
            "        [-2.32875394e-03, -2.31754743e-02,  3.63772586e-02, ...,\n",
            "          2.75321119e-03, -1.23812193e-02, -3.03911362e-02],\n",
            "        ...,\n",
            "        [-1.36075150e-02, -1.73674501e-03,  8.53688456e-03, ...,\n",
            "          3.89812491e-03, -1.70645863e-02,  7.98282959e-03],\n",
            "        [ 9.62544326e-03,  1.47339711e-02, -2.17354838e-02, ...,\n",
            "         -1.70868803e-02, -2.03642268e-02, -3.46427364e-03],\n",
            "        [ 2.65162066e-02, -1.24705099e-02,  2.23488305e-02, ...,\n",
            "         -2.61986274e-02,  1.42028695e-02, -2.35550757e-02]],\n",
            "\n",
            "       [[-3.31413560e-02,  8.46256223e-03, -1.37124844e-02, ...,\n",
            "         -8.98625422e-03, -2.63713859e-02,  1.64316297e-02],\n",
            "        [ 1.35641061e-02, -1.40205361e-02,  9.25297383e-03, ...,\n",
            "          2.14275462e-03, -3.62938978e-02,  3.90764512e-03],\n",
            "        [-1.33269494e-02, -3.08514964e-02,  1.20250601e-02, ...,\n",
            "          1.20363059e-02, -1.58477537e-02,  1.19564617e-02],\n",
            "        ...,\n",
            "        [ 2.56543439e-02,  9.50734783e-03, -3.02388016e-02, ...,\n",
            "         -1.42787062e-02, -2.02421211e-02, -2.88378593e-04],\n",
            "        [-1.63998846e-02,  1.09770698e-02, -3.27569596e-03, ...,\n",
            "          6.98776031e-03, -5.81702031e-03,  1.99008081e-02],\n",
            "        [ 1.57844406e-02, -5.57667483e-03,  6.55488111e-03, ...,\n",
            "          5.22970129e-03, -1.42060630e-02,  1.08108586e-02]],\n",
            "\n",
            "       [[-4.52100346e-03, -2.19114264e-03,  1.71138924e-02, ...,\n",
            "         -1.99394790e-03,  1.68859828e-02, -1.74765997e-02],\n",
            "        [ 1.17086051e-02, -4.10318794e-03,  2.03769021e-02, ...,\n",
            "         -2.25408329e-03,  2.34356280e-02,  1.09368628e-02],\n",
            "        [-3.95097323e-02, -1.75045431e-02,  1.08689377e-02, ...,\n",
            "          7.12680072e-03,  3.07905097e-02,  1.22093547e-06],\n",
            "        ...,\n",
            "        [-3.14655490e-02,  4.85386606e-03, -5.24020428e-03, ...,\n",
            "          1.91724375e-02, -1.89369079e-02,  2.41664946e-02],\n",
            "        [ 9.24707763e-03, -3.81397270e-03,  1.59044005e-03, ...,\n",
            "         -2.91569512e-02, -8.22296832e-03,  3.00896280e-02],\n",
            "        [ 9.36887227e-03, -2.13191006e-02,  6.58413954e-03, ...,\n",
            "          5.14308922e-03, -1.50455162e-02, -1.37117524e-02]]],\n",
            "      dtype=float32)>, <tf.Variable 'transformer/layer_9/output/kernel:0' shape=(3072, 768) dtype=float32, numpy=\n",
            "array([[ 0.01160473,  0.01281426, -0.0075882 , ...,  0.00478645,\n",
            "         0.02456507, -0.01212643],\n",
            "       [-0.00519517, -0.01069908, -0.00866456, ..., -0.01729787,\n",
            "        -0.01262874, -0.00033924],\n",
            "       [-0.02023862,  0.03027501,  0.01190863, ...,  0.00600636,\n",
            "        -0.01064201,  0.01755866],\n",
            "       ...,\n",
            "       [-0.03125523, -0.01427473, -0.00562313, ...,  0.02084044,\n",
            "        -0.0250668 ,  0.00942744],\n",
            "       [ 0.0389307 , -0.02166586,  0.00572309, ..., -0.00836912,\n",
            "        -0.00938553,  0.00243707],\n",
            "       [-0.01623175,  0.00335312,  0.03405824, ..., -0.00497506,\n",
            "         0.01915916, -0.01336267]], dtype=float32)>, <tf.Variable 'transformer/layer_8/self_attention/value/bias:0' shape=(12, 64) dtype=float32, numpy=\n",
            "array([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
            "      dtype=float32)>, <tf.Variable 'transformer/layer_4/self_attention_output/kernel:0' shape=(12, 64, 768) dtype=float32, numpy=\n",
            "array([[[ 7.69674685e-03,  1.02685438e-02,  8.25777370e-03, ...,\n",
            "          1.49226713e-03, -2.79487460e-04, -8.07882473e-03],\n",
            "        [-1.06457975e-02, -2.89297625e-02,  6.76014787e-03, ...,\n",
            "         -4.78979247e-03,  1.33510825e-04, -3.63357738e-03],\n",
            "        [-1.69241577e-02,  2.07298412e-03, -3.19581516e-02, ...,\n",
            "         -2.05090754e-02,  7.27072172e-03,  1.80808138e-02],\n",
            "        ...,\n",
            "        [-3.99828069e-02,  1.10621192e-02,  6.17769780e-03, ...,\n",
            "         -2.65889382e-03, -1.39454911e-02,  3.79747176e-03],\n",
            "        [ 2.04135384e-02,  4.08969773e-03, -3.11836526e-02, ...,\n",
            "         -3.94778978e-03,  1.23098316e-02,  3.18485759e-02],\n",
            "        [ 1.59015693e-02,  3.05242231e-03,  2.56979410e-02, ...,\n",
            "          4.00247937e-03,  1.32157728e-02,  2.37733852e-02]],\n",
            "\n",
            "       [[ 8.28556222e-05,  2.66167540e-02, -3.74678802e-03, ...,\n",
            "         -3.05325314e-02, -1.82389133e-02,  5.86919952e-03],\n",
            "        [ 1.55396787e-02,  6.50782557e-03,  1.78979919e-03, ...,\n",
            "         -1.01005612e-02,  1.25264544e-02, -2.65815482e-03],\n",
            "        [ 1.15296505e-02,  2.00076010e-02, -1.62673090e-02, ...,\n",
            "          1.19049568e-02, -5.04963705e-03, -2.58050840e-02],\n",
            "        ...,\n",
            "        [ 1.26611544e-02, -1.87268236e-03,  7.84532912e-03, ...,\n",
            "         -3.45583484e-02, -5.42476540e-03, -8.84337537e-03],\n",
            "        [-8.84703640e-03,  3.17229284e-03,  8.03720020e-03, ...,\n",
            "          1.42181180e-02, -4.34481259e-03,  1.57788943e-03],\n",
            "        [-3.61652635e-02, -2.92057600e-02,  1.49229476e-02, ...,\n",
            "          3.91067285e-03, -1.52203953e-02,  3.55348270e-03]],\n",
            "\n",
            "       [[-1.35333044e-02, -9.38330591e-03, -1.92802586e-03, ...,\n",
            "          1.15546933e-03, -1.39668025e-02, -8.57582875e-03],\n",
            "        [-1.15709649e-02,  9.74472705e-03,  2.58483719e-02, ...,\n",
            "         -1.50654363e-02,  1.62202753e-02,  2.03676522e-02],\n",
            "        [ 1.90840696e-03,  3.71639468e-02, -9.93432943e-03, ...,\n",
            "         -2.62466464e-02, -2.67372727e-02,  2.75725592e-03],\n",
            "        ...,\n",
            "        [-7.93590490e-03, -1.29713044e-02,  5.08203916e-03, ...,\n",
            "          1.20568639e-02, -1.37710441e-02,  4.36057150e-03],\n",
            "        [-9.32839978e-03, -1.01558650e-02,  8.91670957e-03, ...,\n",
            "         -3.09224892e-02, -1.44925378e-02, -4.05860599e-04],\n",
            "        [ 2.89899707e-02,  1.93401836e-02,  6.01639692e-03, ...,\n",
            "          3.46008092e-02, -2.45118532e-02,  1.65615603e-02]],\n",
            "\n",
            "       ...,\n",
            "\n",
            "       [[-2.08108630e-02,  1.06224502e-02, -6.31341172e-05, ...,\n",
            "          1.36821279e-02, -1.90709985e-03, -3.10605485e-03],\n",
            "        [-2.10611224e-02,  7.86427595e-03, -1.42808752e-02, ...,\n",
            "          4.97161690e-03,  2.72188056e-02,  2.36556437e-02],\n",
            "        [-2.79603340e-02, -2.19980231e-03, -4.73563356e-04, ...,\n",
            "         -1.90919079e-02,  7.91872293e-03,  5.20776724e-03],\n",
            "        ...,\n",
            "        [-9.38967429e-03, -1.33020040e-02, -1.39584886e-02, ...,\n",
            "         -7.19556818e-04, -1.04949130e-02,  9.88532673e-04],\n",
            "        [ 2.00356971e-02,  4.37012734e-03, -9.89224296e-03, ...,\n",
            "          2.58587971e-02, -2.03087758e-02, -1.06536727e-02],\n",
            "        [ 3.70895048e-03, -2.45782826e-02,  1.41756563e-02, ...,\n",
            "          1.09897619e-02, -2.35677212e-02, -1.62506010e-02]],\n",
            "\n",
            "       [[ 6.16960460e-03,  7.75833195e-03, -1.33483512e-02, ...,\n",
            "         -8.58142134e-03, -3.42071913e-02,  2.92329304e-02],\n",
            "        [-3.05370558e-02,  5.73254470e-03, -8.92267283e-03, ...,\n",
            "          3.44928876e-02,  1.49471983e-02, -1.36543056e-02],\n",
            "        [-2.11806744e-02, -5.71997650e-03, -1.95808262e-02, ...,\n",
            "         -1.03455111e-02,  2.14689318e-03,  9.77898203e-03],\n",
            "        ...,\n",
            "        [ 3.17123011e-02, -2.40748860e-02,  1.84800159e-02, ...,\n",
            "          2.30192821e-02,  4.11229022e-03, -3.52574550e-02],\n",
            "        [-2.67285341e-03, -2.59003695e-03,  1.33912303e-02, ...,\n",
            "          1.95429958e-02, -2.61110030e-02, -4.76781698e-03],\n",
            "        [ 1.07808728e-02,  3.34916008e-03,  3.09717692e-02, ...,\n",
            "          1.97246708e-02, -3.04643176e-02,  2.12185718e-02]],\n",
            "\n",
            "       [[ 7.67259765e-03,  2.67118383e-02,  2.07868833e-02, ...,\n",
            "         -8.30419548e-03,  1.52227283e-03, -1.99794006e-02],\n",
            "        [ 3.23071354e-03, -6.46872586e-03,  1.17668016e-02, ...,\n",
            "          3.99251748e-03,  1.49865197e-02, -1.85350068e-02],\n",
            "        [ 4.51381877e-03,  1.19948061e-03, -2.29426287e-02, ...,\n",
            "         -2.88842712e-03, -1.78302694e-02, -1.68618970e-02],\n",
            "        ...,\n",
            "        [ 2.51115263e-02, -3.91988968e-03, -1.44086322e-02, ...,\n",
            "          6.26116060e-03, -1.30553404e-03, -3.52467522e-02],\n",
            "        [ 1.61238965e-02,  6.93965470e-03, -1.16658211e-02, ...,\n",
            "          8.88778921e-03, -8.82611203e-05,  5.39253047e-03],\n",
            "        [ 1.20384060e-02,  5.99175366e-03, -6.65301550e-03, ...,\n",
            "         -1.72193777e-02,  2.08808649e-02,  1.18225021e-02]]],\n",
            "      dtype=float32)>, <tf.Variable 'transformer/layer_0/output/kernel:0' shape=(3072, 768) dtype=float32, numpy=\n",
            "array([[ 0.01086119,  0.02344113, -0.03911921, ...,  0.02552973,\n",
            "        -0.02097603,  0.01713545],\n",
            "       [ 0.02153398, -0.02402964,  0.01287862, ..., -0.02742059,\n",
            "         0.00897762, -0.00680922],\n",
            "       [-0.0041513 , -0.00055439,  0.02707449, ...,  0.02790997,\n",
            "        -0.0052062 , -0.02258724],\n",
            "       ...,\n",
            "       [-0.00924351,  0.02406055,  0.01974658, ..., -0.0276228 ,\n",
            "         0.00950515, -0.03252303],\n",
            "       [ 0.01240689, -0.00387173, -0.00723346, ...,  0.00794799,\n",
            "        -0.02511707, -0.01401972],\n",
            "       [-0.03243754,  0.02425998, -0.02846932, ...,  0.02199493,\n",
            "         0.02257606, -0.01422177]], dtype=float32)>, <tf.Variable 'transformer/layer_3/self_attention/value/bias:0' shape=(12, 64) dtype=float32, numpy=\n",
            "array([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
            "      dtype=float32)>, <tf.Variable 'transformer/layer_0/self_attention/value/bias:0' shape=(12, 64) dtype=float32, numpy=\n",
            "array([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
            "      dtype=float32)>, <tf.Variable 'transformer/layer_1/self_attention/key/bias:0' shape=(12, 64) dtype=float32, numpy=\n",
            "array([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
            "      dtype=float32)>, <tf.Variable 'transformer/layer_10/output_layer_norm/beta:0' shape=(768,) dtype=float32, numpy=\n",
            "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0.], dtype=float32)>, <tf.Variable 'transformer/layer_0/self_attention_layer_norm/gamma:0' shape=(768,) dtype=float32, numpy=\n",
            "array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1.], dtype=float32)>, <tf.Variable 'transformer/layer_6/self_attention/query/kernel:0' shape=(768, 12, 64) dtype=float32, numpy=\n",
            "array([[[-1.84356421e-02,  2.97083855e-02,  1.89681053e-02, ...,\n",
            "          1.53858215e-02,  5.13574947e-03, -3.49882990e-02],\n",
            "        [ 2.87622921e-02, -3.75667959e-03, -1.95818283e-02, ...,\n",
            "         -2.05584290e-03,  7.54041970e-03,  1.63844042e-02],\n",
            "        [ 2.13616677e-02, -6.20897766e-03,  1.43294884e-02, ...,\n",
            "          1.63709791e-03, -3.56387421e-02,  1.27171371e-02],\n",
            "        ...,\n",
            "        [-9.30852536e-03,  8.61239620e-04, -1.43896136e-02, ...,\n",
            "         -2.35006250e-02, -1.01281228e-02, -6.70626177e-04],\n",
            "        [-1.22325718e-02,  2.91057993e-02,  1.24124810e-02, ...,\n",
            "          2.84504648e-02, -3.06888158e-03,  3.01373145e-03],\n",
            "        [ 2.02469919e-02, -1.17865461e-03, -2.59589963e-03, ...,\n",
            "         -5.64374635e-03,  7.84959551e-03, -3.96680366e-03]],\n",
            "\n",
            "       [[-2.88214553e-02, -3.94321010e-02,  2.04487126e-02, ...,\n",
            "          1.35330921e-02, -2.95467060e-02, -1.18051898e-02],\n",
            "        [-1.06027443e-02,  3.30277756e-02,  3.37558123e-03, ...,\n",
            "         -1.90212708e-02,  2.62391344e-02,  1.45828919e-02],\n",
            "        [-4.86658682e-04,  1.66212469e-02,  2.70504970e-02, ...,\n",
            "         -1.01263961e-02,  2.92295474e-03,  4.68573812e-03],\n",
            "        ...,\n",
            "        [-6.94400538e-03, -1.66586172e-02,  9.81642865e-03, ...,\n",
            "          7.81822775e-04, -7.65351532e-03,  2.50832224e-03],\n",
            "        [ 1.26157887e-02, -1.64277386e-02,  2.93832105e-02, ...,\n",
            "         -1.83037315e-02, -2.02835575e-02,  7.25292647e-03],\n",
            "        [-6.92242291e-04,  1.26264673e-02, -8.42512678e-03, ...,\n",
            "         -1.50931152e-02,  1.22351870e-02,  8.06114636e-03]],\n",
            "\n",
            "       [[-3.54219824e-02, -2.31088474e-02,  7.84214679e-03, ...,\n",
            "          1.32346405e-02,  1.93631854e-02, -1.85154416e-02],\n",
            "        [ 2.99013462e-02,  1.48311965e-02,  1.21777505e-02, ...,\n",
            "          3.25194634e-02, -6.11450383e-03,  8.54287704e-04],\n",
            "        [ 8.14786181e-03,  9.43358615e-03, -1.17904386e-02, ...,\n",
            "          4.24030237e-03, -2.31308844e-02, -5.73774613e-03],\n",
            "        ...,\n",
            "        [-5.68630314e-03, -3.84723283e-02,  1.65601145e-03, ...,\n",
            "          1.16673000e-02,  3.39724123e-03, -1.73435770e-02],\n",
            "        [ 2.90156086e-03, -2.17806455e-02, -1.50586041e-02, ...,\n",
            "         -8.10230151e-03, -2.46448652e-03, -7.51124416e-03],\n",
            "        [-2.72178594e-02,  4.85679274e-03,  1.83513835e-02, ...,\n",
            "         -3.83154792e-03, -2.58575510e-02,  7.18094889e-05]],\n",
            "\n",
            "       ...,\n",
            "\n",
            "       [[-6.42163679e-03, -1.57187190e-02,  2.43419665e-03, ...,\n",
            "         -7.51200086e-03,  2.40489263e-02,  6.13047183e-03],\n",
            "        [ 1.33732511e-02,  1.62463617e-02,  1.39842341e-02, ...,\n",
            "          2.56820524e-04,  1.99130345e-02,  5.64762065e-03],\n",
            "        [ 8.04453995e-03, -7.84993288e-04,  1.86132714e-02, ...,\n",
            "          1.10754033e-03,  3.09187956e-02,  4.52518323e-03],\n",
            "        ...,\n",
            "        [ 1.54282218e-02,  1.49056427e-02,  2.09857170e-02, ...,\n",
            "         -1.74910165e-02,  3.10581699e-02, -1.27969850e-02],\n",
            "        [ 6.44422020e-04, -2.88721658e-02, -2.48021260e-02, ...,\n",
            "          2.46558897e-02, -3.50491446e-03, -1.73360687e-02],\n",
            "        [ 6.49531698e-03, -9.05568618e-03,  1.41423056e-02, ...,\n",
            "         -1.05260350e-02, -1.03165684e-02, -6.77768048e-03]],\n",
            "\n",
            "       [[ 2.59247962e-02,  2.75504366e-02, -1.79990120e-02, ...,\n",
            "         -1.87882073e-02, -8.40011239e-03, -5.15942555e-03],\n",
            "        [-2.03643204e-03,  5.88608859e-03,  1.15438448e-02, ...,\n",
            "         -1.98469078e-03,  2.70145144e-02,  5.80744492e-03],\n",
            "        [ 3.50663587e-02,  6.88725477e-03, -2.13593282e-02, ...,\n",
            "         -2.85163634e-02,  2.64666583e-02,  4.14406043e-03],\n",
            "        ...,\n",
            "        [-3.80039476e-02, -2.31102351e-02, -1.56591777e-02, ...,\n",
            "          3.76210082e-03, -1.05414893e-02, -1.03800185e-02],\n",
            "        [-9.03126970e-03,  3.12408563e-02, -2.00979542e-02, ...,\n",
            "          1.26842316e-02, -2.14763880e-02, -1.38219986e-02],\n",
            "        [-2.51415502e-02, -8.56475811e-03,  1.04356036e-02, ...,\n",
            "          1.30509716e-02, -2.05740612e-02,  7.61661353e-03]],\n",
            "\n",
            "       [[ 1.47332652e-02,  2.65895091e-02, -8.56218114e-03, ...,\n",
            "          3.81703908e-03, -1.02411546e-02,  9.99599509e-03],\n",
            "        [ 2.13212110e-02, -1.14851529e-02, -2.52941716e-02, ...,\n",
            "          7.01129530e-03,  5.83488913e-03,  1.20124910e-02],\n",
            "        [ 2.49661645e-03,  1.49799129e-02,  8.60789325e-03, ...,\n",
            "         -1.11517718e-03, -9.47108027e-03,  2.18682364e-02],\n",
            "        ...,\n",
            "        [ 8.30758270e-03, -8.20113253e-03,  2.99641304e-03, ...,\n",
            "         -2.88528786e-03, -1.42891416e-02, -1.38479611e-02],\n",
            "        [-3.15233530e-03,  1.34239979e-02, -2.03500334e-02, ...,\n",
            "         -6.10014657e-03,  4.19252459e-03,  2.09946372e-02],\n",
            "        [-1.31205935e-02, -5.77216595e-03, -6.70523383e-03, ...,\n",
            "          2.89881788e-02,  5.68830548e-03,  1.51068531e-02]]],\n",
            "      dtype=float32)>, <tf.Variable 'transformer/layer_1/output_layer_norm/gamma:0' shape=(768,) dtype=float32, numpy=\n",
            "array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1.], dtype=float32)>, <tf.Variable 'transformer/layer_1/intermediate/kernel:0' shape=(768, 3072) dtype=float32, numpy=\n",
            "array([[-0.01444959, -0.00699944, -0.02449738, ...,  0.01154122,\n",
            "        -0.01273033, -0.02226411],\n",
            "       [ 0.00649715,  0.00975298, -0.00851952, ..., -0.02996091,\n",
            "        -0.03855466,  0.00908301],\n",
            "       [ 0.00856954, -0.01285471,  0.00423965, ..., -0.01068677,\n",
            "        -0.00485682,  0.02111143],\n",
            "       ...,\n",
            "       [ 0.00657043, -0.00582356,  0.02150467, ..., -0.00172811,\n",
            "        -0.00547971, -0.00528345],\n",
            "       [ 0.0180258 ,  0.00573132, -0.02968675, ..., -0.00036925,\n",
            "        -0.00442659,  0.00387407],\n",
            "       [ 0.02326219,  0.00496921, -0.00662184, ..., -0.0110189 ,\n",
            "         0.01137904,  0.00346444]], dtype=float32)>, <tf.Variable 'transformer/layer_11/self_attention/value/kernel:0' shape=(768, 12, 64) dtype=float32, numpy=\n",
            "array([[[ 2.32479740e-02, -4.75391326e-03, -2.46258080e-02, ...,\n",
            "         -3.11216917e-02, -2.78698988e-02, -7.25138839e-03],\n",
            "        [-1.07829114e-02, -7.43224192e-03,  2.89742853e-02, ...,\n",
            "         -1.13477372e-02, -1.25189973e-02, -3.41680497e-02],\n",
            "        [-1.78839304e-02, -1.80284902e-02, -1.25108967e-02, ...,\n",
            "          6.62720297e-03,  3.89458076e-03, -1.02087259e-02],\n",
            "        ...,\n",
            "        [-3.39598991e-02,  1.34234903e-02, -1.38079543e-02, ...,\n",
            "         -2.10946016e-02, -1.99093334e-02,  1.86055340e-02],\n",
            "        [-8.75393336e-04,  1.98246725e-03, -7.74646457e-03, ...,\n",
            "         -7.74239283e-03,  8.83263350e-03,  1.22625669e-02],\n",
            "        [-1.62280872e-02,  1.22708231e-02,  3.08111007e-03, ...,\n",
            "         -7.31148152e-03,  7.87218940e-03,  1.24952139e-03]],\n",
            "\n",
            "       [[-3.70364301e-02,  1.29224174e-02,  1.93216763e-02, ...,\n",
            "         -3.20529863e-02, -6.71533635e-04, -3.99961974e-03],\n",
            "        [ 3.20483446e-02, -1.81885958e-02,  4.86124074e-03, ...,\n",
            "          7.17295706e-03, -1.19305085e-02, -7.86384195e-03],\n",
            "        [-6.31981343e-03, -4.81309649e-03, -3.32396105e-02, ...,\n",
            "          1.30007323e-03, -1.00134705e-04,  1.72033776e-02],\n",
            "        ...,\n",
            "        [ 2.51422590e-03,  3.04044620e-03, -1.42315694e-03, ...,\n",
            "          8.15486815e-03,  1.93573236e-02,  8.90974770e-05],\n",
            "        [ 5.63366571e-03, -1.88442264e-02, -1.16022620e-02, ...,\n",
            "          1.79134235e-02, -7.05673266e-03,  6.96245907e-03],\n",
            "        [-1.22738304e-02, -1.10663967e-02, -5.67270024e-03, ...,\n",
            "         -3.33814994e-02, -3.61119621e-02,  7.32338708e-03]],\n",
            "\n",
            "       [[-1.70164946e-02, -1.59942415e-02, -2.62176916e-02, ...,\n",
            "          7.10885925e-03, -1.06945392e-02, -7.52195204e-03],\n",
            "        [ 3.34424525e-02, -1.26129529e-02,  7.44269555e-03, ...,\n",
            "         -1.55838141e-02,  3.16520073e-02, -9.96111147e-03],\n",
            "        [ 1.52506502e-02, -3.59178078e-03,  2.97606643e-02, ...,\n",
            "         -1.09734004e-02, -1.06275305e-02,  8.81229993e-03],\n",
            "        ...,\n",
            "        [ 4.48179618e-03, -8.26602895e-03, -7.91871734e-03, ...,\n",
            "         -1.95982289e-02, -3.54389241e-03, -3.25101838e-02],\n",
            "        [ 1.94044374e-02,  8.84640776e-03, -1.42507348e-02, ...,\n",
            "         -8.92157294e-03, -1.01627624e-02,  1.39482366e-02],\n",
            "        [-1.23881660e-02,  4.44765575e-03,  2.23342050e-02, ...,\n",
            "          1.71852540e-02,  1.39552134e-03, -1.15668066e-02]],\n",
            "\n",
            "       ...,\n",
            "\n",
            "       [[ 3.94205898e-02, -1.19258719e-03, -1.63253944e-03, ...,\n",
            "          2.13733353e-02, -1.38279255e-02, -1.70291476e-02],\n",
            "        [ 1.13031100e-02, -3.64140957e-03, -2.86977901e-03, ...,\n",
            "         -3.72017510e-02,  2.14471053e-02,  1.51305813e-02],\n",
            "        [-1.15208849e-02,  3.41709168e-03,  5.95608214e-03, ...,\n",
            "          5.38602797e-03, -3.63483578e-02, -6.08271686e-03],\n",
            "        ...,\n",
            "        [-1.88521773e-03, -2.29229424e-02,  7.16366991e-03, ...,\n",
            "          3.86568643e-02,  2.21064184e-02, -5.40685887e-03],\n",
            "        [ 1.18296286e-02, -2.43558339e-03, -1.59450658e-02, ...,\n",
            "          5.34689240e-03, -1.37021707e-03, -2.44242046e-02],\n",
            "        [-8.38603918e-03, -1.72715134e-03,  1.26331728e-02, ...,\n",
            "          2.57501453e-02, -2.15238929e-02,  2.39485335e-02]],\n",
            "\n",
            "       [[-2.01656520e-02, -1.66984163e-02,  6.24673348e-03, ...,\n",
            "          2.20724735e-02,  2.72609759e-03,  1.09454442e-03],\n",
            "        [ 6.93342648e-03, -2.23677941e-02,  1.58271356e-03, ...,\n",
            "         -7.85709266e-03,  2.94710435e-02,  1.94499195e-02],\n",
            "        [-5.00932522e-03,  2.25577783e-02,  6.73557306e-03, ...,\n",
            "         -1.47556572e-03, -2.36956170e-03, -2.55458131e-02],\n",
            "        ...,\n",
            "        [ 9.99191217e-03,  1.44030210e-02, -1.41504025e-02, ...,\n",
            "          9.16264765e-03,  5.87182399e-03,  7.36027956e-03],\n",
            "        [ 2.32835971e-02, -1.01394276e-03, -2.64850594e-02, ...,\n",
            "          1.47546465e-02,  3.38735208e-02, -1.85729731e-02],\n",
            "        [-9.59527586e-03,  2.30125058e-02,  1.15437228e-02, ...,\n",
            "         -5.03227208e-03, -1.28387718e-03,  3.72511633e-02]],\n",
            "\n",
            "       [[-1.29794646e-02, -3.75865288e-02,  1.15073109e-02, ...,\n",
            "         -7.87991006e-03,  9.14439186e-03,  4.17360710e-03],\n",
            "        [-1.99878402e-02,  2.32509826e-03,  1.48710748e-02, ...,\n",
            "         -1.64325759e-02,  4.30346839e-03, -1.21995807e-02],\n",
            "        [-1.50936134e-02, -3.69715714e-03, -6.80699944e-03, ...,\n",
            "          3.55256634e-04, -1.63579136e-02,  3.01547255e-02],\n",
            "        ...,\n",
            "        [ 2.01471727e-02, -1.17683420e-02,  1.97292473e-02, ...,\n",
            "          1.89521965e-02, -1.16529642e-02, -2.46197488e-02],\n",
            "        [-1.13382814e-02,  1.98757406e-02, -1.41510749e-02, ...,\n",
            "         -2.95002963e-02, -2.68154088e-02, -3.43812108e-02],\n",
            "        [ 2.15287437e-03,  1.27505809e-02,  1.48802586e-02, ...,\n",
            "          2.92378124e-02, -1.18574779e-02, -1.02464082e-02]]],\n",
            "      dtype=float32)>, <tf.Variable 'transformer/layer_9/output_layer_norm/gamma:0' shape=(768,) dtype=float32, numpy=\n",
            "array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1.], dtype=float32)>, <tf.Variable 'transformer/layer_1/self_attention_output/bias:0' shape=(768,) dtype=float32, numpy=\n",
            "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0.], dtype=float32)>, <tf.Variable 'transformer/layer_9/intermediate/kernel:0' shape=(768, 3072) dtype=float32, numpy=\n",
            "array([[-0.03441278, -0.02622689, -0.01427815, ..., -0.00182942,\n",
            "         0.01149649, -0.00232668],\n",
            "       [-0.01976625, -0.00162455, -0.00519364, ...,  0.01378231,\n",
            "        -0.00290348,  0.03479131],\n",
            "       [ 0.00497426,  0.01222355, -0.00087712, ..., -0.02983472,\n",
            "         0.02455424, -0.00600173],\n",
            "       ...,\n",
            "       [ 0.00425477,  0.00977651, -0.01030602, ..., -0.02411399,\n",
            "        -0.03932488, -0.01804815],\n",
            "       [ 0.01901873, -0.02132853, -0.01065276, ..., -0.0090758 ,\n",
            "         0.01830068, -0.00776724],\n",
            "       [ 0.00493634, -0.00831899, -0.02709901, ..., -0.02615986,\n",
            "         0.00202709,  0.02759915]], dtype=float32)>, <tf.Variable 'embeddings/layer_norm/beta:0' shape=(768,) dtype=float32, numpy=\n",
            "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0.], dtype=float32)>, <tf.Variable 'transformer/layer_0/output_layer_norm/gamma:0' shape=(768,) dtype=float32, numpy=\n",
            "array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1.], dtype=float32)>, <tf.Variable 'transformer/layer_11/output/bias:0' shape=(768,) dtype=float32, numpy=\n",
            "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0.], dtype=float32)>, <tf.Variable 'transformer/layer_4/self_attention/key/bias:0' shape=(12, 64) dtype=float32, numpy=\n",
            "array([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
            "      dtype=float32)>, <tf.Variable 'transformer/layer_5/self_attention_output/bias:0' shape=(768,) dtype=float32, numpy=\n",
            "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0.], dtype=float32)>, <tf.Variable 'transformer/layer_5/self_attention_layer_norm/beta:0' shape=(768,) dtype=float32, numpy=\n",
            "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0.], dtype=float32)>, <tf.Variable 'transformer/layer_4/self_attention/query/bias:0' shape=(12, 64) dtype=float32, numpy=\n",
            "array([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
            "      dtype=float32)>, <tf.Variable 'transformer/layer_11/self_attention/key/kernel:0' shape=(768, 12, 64) dtype=float32, numpy=\n",
            "array([[[ 2.51083653e-02,  2.16016807e-02,  1.32064074e-02, ...,\n",
            "          3.70759005e-03, -1.36926770e-02,  3.52358893e-02],\n",
            "        [ 3.92395630e-03,  1.78454518e-02, -1.97893996e-02, ...,\n",
            "          7.62200449e-03,  2.14403006e-03,  1.07684815e-02],\n",
            "        [ 1.65856234e-03, -3.47769889e-03,  2.31571048e-02, ...,\n",
            "         -8.93760938e-03,  6.87735947e-03,  9.36832093e-03],\n",
            "        ...,\n",
            "        [-1.49821350e-02,  1.12293125e-03, -1.59413333e-03, ...,\n",
            "         -3.34576555e-02,  1.28641007e-02,  2.30518971e-02],\n",
            "        [-3.48398685e-02, -8.84928834e-03, -2.64309440e-02, ...,\n",
            "          8.21846351e-03, -1.11515950e-02,  2.17624009e-02],\n",
            "        [ 1.33256717e-02,  2.26145815e-02,  5.45881165e-04, ...,\n",
            "          9.45060141e-03, -3.62926833e-02,  2.99765691e-02]],\n",
            "\n",
            "       [[ 6.93181437e-03,  1.10288532e-02,  1.17295433e-03, ...,\n",
            "         -1.79290003e-03, -5.39232465e-03, -1.76093671e-02],\n",
            "        [ 5.26820717e-04, -1.88623164e-02, -2.37624208e-03, ...,\n",
            "          1.87580008e-02, -4.99545736e-03,  3.30037102e-02],\n",
            "        [ 1.17668891e-02, -1.38595270e-03,  1.67780928e-02, ...,\n",
            "          5.92299039e-04, -2.55094972e-02, -1.82375796e-02],\n",
            "        ...,\n",
            "        [-1.60458162e-02,  1.47736324e-02,  3.57755758e-02, ...,\n",
            "          4.70896391e-03,  1.24739576e-02,  2.21224260e-02],\n",
            "        [-8.32092948e-04, -1.63793433e-02, -2.01864098e-03, ...,\n",
            "         -2.70065106e-02, -2.69346242e-03,  3.12663689e-02],\n",
            "        [-1.36803044e-02, -5.74768940e-03, -7.01170927e-03, ...,\n",
            "         -1.33752078e-02,  1.15189869e-04,  8.36937688e-03]],\n",
            "\n",
            "       [[-2.28242986e-02, -2.32554600e-02,  2.40994059e-02, ...,\n",
            "         -2.37174914e-03, -1.27310175e-02,  2.17333622e-02],\n",
            "        [ 4.17459244e-03,  2.10435148e-02,  1.51987504e-02, ...,\n",
            "         -1.66235771e-02, -3.16170650e-03,  1.10424682e-02],\n",
            "        [ 3.77930589e-02,  1.22403400e-02,  2.15041935e-02, ...,\n",
            "          3.50475237e-02,  2.17456780e-02, -3.01005295e-03],\n",
            "        ...,\n",
            "        [-2.96513438e-02, -5.19882306e-04, -2.41316985e-02, ...,\n",
            "         -5.64683869e-04,  4.28953720e-03, -6.96961163e-03],\n",
            "        [ 1.66954175e-02, -4.04058304e-03, -4.88409493e-03, ...,\n",
            "          1.84907988e-02, -5.09735383e-03, -1.48451077e-02],\n",
            "        [-6.48180780e-04, -7.96600617e-03, -3.58775519e-02, ...,\n",
            "          3.17912921e-02, -6.30537979e-03, -4.50706994e-03]],\n",
            "\n",
            "       ...,\n",
            "\n",
            "       [[-2.92995246e-03,  1.39689595e-02, -1.65582299e-02, ...,\n",
            "          1.69714708e-02,  1.06617091e-02, -1.71064343e-02],\n",
            "        [ 1.37515168e-03,  5.33259660e-03, -3.95321846e-02, ...,\n",
            "         -1.52159045e-02,  1.31523330e-02,  1.95183791e-02],\n",
            "        [ 1.48336571e-02, -1.17610320e-02,  1.67093948e-02, ...,\n",
            "          2.22054124e-02,  1.67835820e-02, -6.35471442e-05],\n",
            "        ...,\n",
            "        [ 7.49085681e-04, -1.58756580e-02, -3.03265639e-02, ...,\n",
            "         -4.77473903e-03, -2.13966276e-02, -1.10211987e-02],\n",
            "        [-1.81632880e-02,  3.07666883e-02, -6.91026729e-03, ...,\n",
            "         -6.41049305e-03,  8.65998387e-04,  9.98127367e-03],\n",
            "        [ 1.21564958e-02, -6.03714213e-03,  5.15257567e-03, ...,\n",
            "          1.08869970e-02,  1.46979587e-02, -3.41792987e-03]],\n",
            "\n",
            "       [[ 2.94140214e-03, -1.76944642e-03, -2.52560880e-02, ...,\n",
            "         -9.14140325e-03,  1.94650106e-02, -8.69197678e-03],\n",
            "        [ 9.71621461e-03, -1.66040787e-03, -2.14697793e-02, ...,\n",
            "          2.43757442e-02, -1.02694088e-03, -1.89680047e-03],\n",
            "        [ 3.25281136e-02,  5.83014218e-03,  1.29301939e-02, ...,\n",
            "         -3.11467387e-02,  9.77142990e-05, -3.37023772e-02],\n",
            "        ...,\n",
            "        [ 1.51853729e-02,  2.36822497e-02, -1.34895705e-02, ...,\n",
            "         -3.14146355e-02, -3.55338454e-02, -6.29875576e-03],\n",
            "        [ 1.52197224e-03, -5.56599721e-03,  2.94428151e-02, ...,\n",
            "          1.54093858e-02,  2.88795792e-02, -4.18619206e-03],\n",
            "        [-2.45658085e-02, -2.46680304e-02,  1.80375129e-02, ...,\n",
            "         -3.08537316e-02,  1.38114961e-02, -1.28456275e-03]],\n",
            "\n",
            "       [[-1.36244083e-02, -4.55223629e-03, -1.57050975e-02, ...,\n",
            "         -3.57176661e-02,  3.40354852e-02, -1.61528084e-02],\n",
            "        [-2.83333659e-03, -1.60658229e-02,  1.20943783e-04, ...,\n",
            "         -2.15848684e-02, -2.13465858e-02,  4.91940696e-03],\n",
            "        [-7.07501406e-03, -2.79627405e-02, -4.81533678e-03, ...,\n",
            "         -2.10767202e-02,  8.99172295e-03,  9.58569534e-03],\n",
            "        ...,\n",
            "        [ 4.53145913e-04,  1.53136691e-02,  1.37157375e-02, ...,\n",
            "          1.70018896e-02, -1.91500820e-02,  5.69956889e-03],\n",
            "        [-1.32121993e-02, -3.21307853e-02,  1.40326628e-02, ...,\n",
            "          1.29474327e-02,  1.33335637e-03, -1.22665288e-02],\n",
            "        [-2.01849788e-02,  1.01351459e-02, -1.06593631e-02, ...,\n",
            "          3.34182046e-02,  1.24126226e-02, -3.37947048e-02]]],\n",
            "      dtype=float32)>, <tf.Variable 'transformer/layer_4/self_attention_layer_norm/beta:0' shape=(768,) dtype=float32, numpy=\n",
            "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0.], dtype=float32)>, <tf.Variable 'transformer/layer_3/self_attention/query/bias:0' shape=(12, 64) dtype=float32, numpy=\n",
            "array([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
            "      dtype=float32)>, <tf.Variable 'transformer/layer_2/intermediate/bias:0' shape=(3072,) dtype=float32, numpy=array([0., 0., 0., ..., 0., 0., 0.], dtype=float32)>, <tf.Variable 'transformer/layer_4/self_attention_output/bias:0' shape=(768,) dtype=float32, numpy=\n",
            "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0.], dtype=float32)>, <tf.Variable 'transformer/layer_7/intermediate/kernel:0' shape=(768, 3072) dtype=float32, numpy=\n",
            "array([[-0.00748691, -0.02817082,  0.00922632, ..., -0.0296451 ,\n",
            "         0.01262657,  0.00772737],\n",
            "       [ 0.02285505, -0.01228889,  0.01704732, ...,  0.00309884,\n",
            "         0.01093659, -0.02622513],\n",
            "       [-0.026846  ,  0.0090192 ,  0.03234046, ...,  0.00637708,\n",
            "        -0.00217661, -0.01601581],\n",
            "       ...,\n",
            "       [ 0.01910167, -0.0313925 ,  0.0014743 , ..., -0.03149531,\n",
            "         0.02271578,  0.01453758],\n",
            "       [ 0.00480806,  0.02226276, -0.01444142, ...,  0.02862189,\n",
            "        -0.02592415, -0.00336807],\n",
            "       [-0.0128015 , -0.00883297, -0.00555434, ..., -0.03972126,\n",
            "         0.0376718 ,  0.00019763]], dtype=float32)>, <tf.Variable 'transformer/layer_10/intermediate/bias:0' shape=(3072,) dtype=float32, numpy=array([0., 0., 0., ..., 0., 0., 0.], dtype=float32)>, <tf.Variable 'transformer/layer_0/self_attention/value/kernel:0' shape=(768, 12, 64) dtype=float32, numpy=\n",
            "array([[[ 0.01138382,  0.01687828,  0.02052318, ...,  0.01222879,\n",
            "         -0.02564675,  0.00633124],\n",
            "        [ 0.00194701,  0.00836258,  0.01305116, ...,  0.00069063,\n",
            "          0.00071949, -0.02283807],\n",
            "        [-0.00704168, -0.00819595,  0.02804832, ...,  0.01898756,\n",
            "         -0.00433555,  0.0194391 ],\n",
            "        ...,\n",
            "        [ 0.00125468,  0.02599007, -0.02449533, ...,  0.00695914,\n",
            "          0.01831761, -0.01134377],\n",
            "        [-0.024613  , -0.01225886, -0.02023831, ...,  0.02067781,\n",
            "          0.03099317, -0.03365055],\n",
            "        [-0.00763786,  0.01823296,  0.00145872, ...,  0.01217189,\n",
            "         -0.00127637, -0.02518274]],\n",
            "\n",
            "       [[ 0.01746386,  0.01902641,  0.01622993, ...,  0.00527541,\n",
            "         -0.0098576 , -0.00486125],\n",
            "        [ 0.0221996 , -0.01128665,  0.00156398, ..., -0.00613528,\n",
            "          0.01890654, -0.00447384],\n",
            "        [-0.02468528,  0.00191613,  0.01068423, ..., -0.02839803,\n",
            "          0.03270891, -0.02593589],\n",
            "        ...,\n",
            "        [ 0.01615267, -0.01915615,  0.02884972, ..., -0.01357608,\n",
            "         -0.0087326 , -0.02054983],\n",
            "        [-0.03153939, -0.00731092, -0.00481302, ..., -0.00638563,\n",
            "          0.01238239, -0.00200216],\n",
            "        [-0.02516985,  0.01220352,  0.0208048 , ...,  0.01828605,\n",
            "          0.00825058, -0.02577407]],\n",
            "\n",
            "       [[-0.01242188, -0.00290993, -0.00275575, ...,  0.02106312,\n",
            "          0.015545  ,  0.00036936],\n",
            "        [-0.02629707,  0.0197676 , -0.02206287, ..., -0.0104821 ,\n",
            "          0.03412537,  0.03163277],\n",
            "        [ 0.00970734,  0.00989418,  0.00260544, ...,  0.00811172,\n",
            "         -0.02286357, -0.00114903],\n",
            "        ...,\n",
            "        [ 0.02278572, -0.00648602, -0.00996882, ...,  0.01258851,\n",
            "          0.02795505,  0.03636515],\n",
            "        [ 0.01537096,  0.00251574,  0.0085578 , ..., -0.02344684,\n",
            "         -0.026853  ,  0.03059329],\n",
            "        [ 0.016266  , -0.01342159, -0.0107159 , ..., -0.00148836,\n",
            "          0.00050618,  0.00856776]],\n",
            "\n",
            "       ...,\n",
            "\n",
            "       [[ 0.02773115, -0.00087666,  0.01516961, ...,  0.0156415 ,\n",
            "          0.00648306, -0.00852801],\n",
            "        [ 0.01490121, -0.01045956, -0.02567886, ..., -0.00289833,\n",
            "          0.00468625,  0.02438162],\n",
            "        [ 0.01222423, -0.00183295,  0.01615385, ...,  0.03052912,\n",
            "          0.01170857, -0.01193408],\n",
            "        ...,\n",
            "        [ 0.03014961,  0.01259242, -0.0040446 , ..., -0.0060178 ,\n",
            "         -0.00857152, -0.03227709],\n",
            "        [-0.01522668, -0.00734376,  0.00228207, ...,  0.00272765,\n",
            "          0.01469613,  0.02212549],\n",
            "        [-0.02041551, -0.01496977,  0.00297426, ...,  0.01526449,\n",
            "          0.01216278, -0.01038087]],\n",
            "\n",
            "       [[ 0.00853304, -0.02771727, -0.0089689 , ...,  0.01950081,\n",
            "         -0.00262402,  0.029157  ],\n",
            "        [-0.00274764,  0.01208732,  0.00335835, ...,  0.02255272,\n",
            "          0.00985226, -0.00900243],\n",
            "        [-0.01639858,  0.00300996,  0.02655839, ..., -0.00975974,\n",
            "          0.01229718, -0.00579626],\n",
            "        ...,\n",
            "        [ 0.0006829 ,  0.0236225 , -0.01518955, ...,  0.00122722,\n",
            "         -0.00830326, -0.0137084 ],\n",
            "        [-0.0015983 , -0.01782796, -0.02642667, ...,  0.02191933,\n",
            "         -0.02838595, -0.01899162],\n",
            "        [ 0.03605282,  0.00556878, -0.00568269, ...,  0.0216813 ,\n",
            "          0.03593041, -0.00367604]],\n",
            "\n",
            "       [[-0.008034  , -0.01436694,  0.01412166, ...,  0.0088568 ,\n",
            "          0.00665058,  0.01349128],\n",
            "        [-0.0052162 ,  0.02958436,  0.01215573, ..., -0.02365822,\n",
            "          0.01972807,  0.00538597],\n",
            "        [ 0.00511069,  0.02621364,  0.0049058 , ..., -0.03834401,\n",
            "          0.00133878,  0.00113665],\n",
            "        ...,\n",
            "        [-0.03088758, -0.01458597, -0.00837126, ..., -0.01871999,\n",
            "          0.00553333, -0.00816899],\n",
            "        [ 0.03483518, -0.00250735, -0.00306269, ...,  0.0002011 ,\n",
            "         -0.01963707, -0.00954291],\n",
            "        [ 0.00809326, -0.00921817, -0.00545934, ...,  0.00288008,\n",
            "          0.00932818, -0.02473089]]], dtype=float32)>, <tf.Variable 'transformer/layer_3/self_attention_output/kernel:0' shape=(12, 64, 768) dtype=float32, numpy=\n",
            "array([[[-6.64578937e-03,  6.54018484e-03, -1.52694909e-02, ...,\n",
            "         -2.61011254e-02,  1.79923717e-02, -5.39520662e-03],\n",
            "        [ 3.65513633e-03, -6.51322212e-03, -4.20831703e-03, ...,\n",
            "          1.22329984e-02, -2.53917743e-02, -9.09750722e-03],\n",
            "        [ 1.17890425e-02, -1.95915774e-02,  1.12711340e-02, ...,\n",
            "          2.12639812e-02,  6.83030114e-04, -4.03418532e-03],\n",
            "        ...,\n",
            "        [ 2.08025984e-02,  1.16643589e-02,  3.15709971e-02, ...,\n",
            "          1.62439682e-02,  2.02270076e-02, -1.28964232e-02],\n",
            "        [-4.46474133e-03, -7.74645293e-03,  1.02058798e-02, ...,\n",
            "         -2.20350996e-02, -1.32315827e-03, -4.60572017e-04],\n",
            "        [-3.71812610e-03, -2.64666881e-02,  5.54187177e-03, ...,\n",
            "         -5.17257582e-03, -1.77320708e-02, -2.10703723e-02]],\n",
            "\n",
            "       [[-2.80218888e-02,  5.61999250e-03, -7.99937814e-04, ...,\n",
            "         -3.15168165e-02,  3.22152837e-03,  1.17820213e-02],\n",
            "        [ 2.49916408e-03,  1.26978522e-02,  1.57937892e-02, ...,\n",
            "          3.46804336e-02,  1.42487045e-02,  8.83277133e-03],\n",
            "        [-1.15126437e-02, -2.86212307e-03,  2.84513198e-02, ...,\n",
            "          1.35974837e-02,  1.02550136e-02,  2.61261128e-03],\n",
            "        ...,\n",
            "        [-1.07426820e-02, -7.74535444e-03, -2.24783421e-02, ...,\n",
            "         -8.39184318e-03, -2.57507749e-02,  1.02103027e-02],\n",
            "        [-1.70008447e-02,  9.51730530e-04, -1.28562283e-03, ...,\n",
            "          1.14700235e-02, -1.11095095e-02,  1.49772987e-02],\n",
            "        [-6.52139215e-03,  7.88917858e-03,  2.48581953e-02, ...,\n",
            "          1.36774620e-02, -2.15276238e-02,  1.95722114e-02]],\n",
            "\n",
            "       [[-5.48866158e-03, -3.16173285e-02, -1.20909084e-02, ...,\n",
            "          2.88533866e-02, -2.10854020e-02, -3.58741684e-03],\n",
            "        [-8.51379940e-04, -1.44594954e-03,  1.92222185e-02, ...,\n",
            "         -1.85349677e-02,  2.09279060e-02,  2.90745888e-02],\n",
            "        [ 3.43096741e-02,  9.03526880e-03,  2.48281355e-03, ...,\n",
            "         -2.76389383e-02, -1.27798915e-02,  1.42746372e-02],\n",
            "        ...,\n",
            "        [-9.64978524e-03,  2.26396173e-02, -2.20020339e-02, ...,\n",
            "         -6.95661129e-03, -1.16205867e-02, -6.90314919e-04],\n",
            "        [-1.68300495e-02,  1.19494554e-02,  2.84781363e-02, ...,\n",
            "          8.34401418e-03, -2.53543425e-02, -1.41283087e-02],\n",
            "        [ 3.29338363e-03,  8.51617951e-04,  2.29717344e-02, ...,\n",
            "         -3.89322154e-02, -2.37360802e-02,  8.28814134e-03]],\n",
            "\n",
            "       ...,\n",
            "\n",
            "       [[ 2.72693466e-02, -1.50776226e-02, -2.23181527e-02, ...,\n",
            "         -2.06385111e-03, -7.58084003e-03, -1.82402600e-02],\n",
            "        [-8.29489809e-03, -1.14707183e-02,  1.61870066e-02, ...,\n",
            "          1.77618712e-02, -2.00244654e-02, -1.11505389e-02],\n",
            "        [ 7.22153485e-03, -2.29339898e-02, -3.63145652e-03, ...,\n",
            "         -2.76658926e-02, -5.07968012e-04,  6.23377645e-03],\n",
            "        ...,\n",
            "        [ 1.01916827e-02, -2.42384197e-03, -4.88643395e-03, ...,\n",
            "          3.53669673e-02,  1.81778949e-02,  5.35108033e-04],\n",
            "        [ 8.43750313e-03, -7.59262173e-03,  7.77629158e-03, ...,\n",
            "          9.85093601e-03,  1.70646552e-02, -3.23767029e-02],\n",
            "        [-1.26692606e-02, -3.17230821e-02, -3.04499026e-02, ...,\n",
            "         -1.93056893e-02, -5.23061911e-03, -5.17310947e-03]],\n",
            "\n",
            "       [[ 1.75354797e-02,  9.08339454e-04, -7.71724805e-03, ...,\n",
            "          2.79941261e-02, -6.85336418e-04, -1.29296063e-02],\n",
            "        [ 6.28300291e-03,  4.46234457e-03,  2.94529600e-03, ...,\n",
            "          5.62781747e-03,  1.63120534e-02,  1.33797703e-02],\n",
            "        [-2.54167640e-03,  5.50920377e-03,  1.73362363e-02, ...,\n",
            "         -1.31612672e-02,  2.74814498e-02,  3.00979940e-03],\n",
            "        ...,\n",
            "        [ 2.13281978e-02,  2.27866359e-02, -1.98180862e-02, ...,\n",
            "         -2.24364661e-02, -2.81340852e-02,  2.83016507e-02],\n",
            "        [-1.65127870e-02,  1.10523226e-02, -6.13634894e-03, ...,\n",
            "         -5.98680635e-04,  3.78627912e-03,  7.55551038e-03],\n",
            "        [ 2.17977930e-02, -6.89006085e-03,  2.30995547e-02, ...,\n",
            "          2.72146123e-03,  2.92418934e-02, -2.12556310e-02]],\n",
            "\n",
            "       [[ 4.30121645e-03,  1.34148439e-02, -2.78308857e-02, ...,\n",
            "         -2.09558029e-02, -6.46183980e-05, -2.27384940e-02],\n",
            "        [-3.19223292e-02, -6.22225553e-03, -5.72282774e-03, ...,\n",
            "          1.98549926e-02, -6.32593129e-03,  8.26823711e-03],\n",
            "        [ 8.03607237e-03, -1.81873492e-03, -3.92470434e-02, ...,\n",
            "          3.86030483e-03, -1.11995554e-02, -3.01493779e-02],\n",
            "        ...,\n",
            "        [ 9.99402534e-03,  2.52478234e-02,  9.11679468e-04, ...,\n",
            "          2.14983169e-02,  4.41931468e-03, -7.39128329e-03],\n",
            "        [ 7.47925788e-03,  1.40668787e-02,  2.50942279e-02, ...,\n",
            "         -9.90396272e-03, -2.81533897e-02,  2.15419233e-02],\n",
            "        [-7.40808574e-03,  1.36921359e-02, -3.28736827e-02, ...,\n",
            "         -1.51255948e-03,  2.18129829e-02,  9.36346408e-03]]],\n",
            "      dtype=float32)>, <tf.Variable 'transformer/layer_1/output_layer_norm/beta:0' shape=(768,) dtype=float32, numpy=\n",
            "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0.], dtype=float32)>, <tf.Variable 'transformer/layer_3/self_attention_layer_norm/gamma:0' shape=(768,) dtype=float32, numpy=\n",
            "array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1.], dtype=float32)>, <tf.Variable 'transformer/layer_8/output/kernel:0' shape=(3072, 768) dtype=float32, numpy=\n",
            "array([[ 1.63050799e-03, -3.46203223e-02,  1.24294998e-03, ...,\n",
            "        -2.05675233e-02,  8.18397943e-03, -1.31557751e-02],\n",
            "       [ 2.70305239e-02, -8.67995620e-03,  1.14601655e-02, ...,\n",
            "         9.31376591e-03,  5.71413245e-03,  1.55951164e-03],\n",
            "       [ 4.30765375e-03, -1.66184753e-02, -1.11393901e-02, ...,\n",
            "        -2.46093385e-02,  7.39477342e-03, -6.86982600e-03],\n",
            "       ...,\n",
            "       [ 9.23622400e-03,  8.35918542e-03, -5.83274942e-03, ...,\n",
            "        -2.25904230e-02,  1.06944330e-02, -6.88866340e-03],\n",
            "       [ 3.86940944e-03,  9.21573211e-03,  3.87274772e-02, ...,\n",
            "        -1.46784715e-03,  4.93211555e-04, -6.32552058e-03],\n",
            "       [ 1.33003695e-02,  9.69754532e-03, -1.29511971e-02, ...,\n",
            "        -8.55569541e-03,  2.34494749e-02,  7.99336049e-05]], dtype=float32)>, <tf.Variable 'transformer/layer_11/self_attention/value/bias:0' shape=(12, 64) dtype=float32, numpy=\n",
            "array([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
            "      dtype=float32)>, <tf.Variable 'transformer/layer_9/output_layer_norm/beta:0' shape=(768,) dtype=float32, numpy=\n",
            "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0.], dtype=float32)>, <tf.Variable 'transformer/layer_2/self_attention/value/kernel:0' shape=(768, 12, 64) dtype=float32, numpy=\n",
            "array([[[ 0.01857219,  0.00263296, -0.02620531, ..., -0.00315053,\n",
            "          0.02018673, -0.00895936],\n",
            "        [-0.01952106, -0.01698178,  0.02340524, ...,  0.0266294 ,\n",
            "          0.01717958, -0.01119292],\n",
            "        [-0.01265688,  0.01014454,  0.01463929, ...,  0.01093201,\n",
            "         -0.01037987,  0.0234175 ],\n",
            "        ...,\n",
            "        [ 0.00016423,  0.00344814, -0.02447262, ..., -0.02122338,\n",
            "          0.00623792, -0.01863439],\n",
            "        [-0.01687109,  0.02376219,  0.00303059, ...,  0.00261554,\n",
            "          0.00470298,  0.01207712],\n",
            "        [-0.01927252, -0.0066631 ,  0.01159483, ...,  0.01906813,\n",
            "          0.00703261, -0.00262298]],\n",
            "\n",
            "       [[ 0.03290949,  0.02031552, -0.00124348, ...,  0.01135235,\n",
            "          0.01507661, -0.00647771],\n",
            "        [-0.00865258, -0.02559668, -0.00236537, ...,  0.01243829,\n",
            "          0.00782049, -0.01194987],\n",
            "        [ 0.00208925, -0.01529403,  0.00359651, ..., -0.00134057,\n",
            "          0.00212086,  0.00232385],\n",
            "        ...,\n",
            "        [-0.02047399, -0.0110062 ,  0.00579755, ..., -0.01247714,\n",
            "          0.00160158,  0.00304692],\n",
            "        [-0.01249169, -0.0150229 , -0.02417356, ...,  0.00455748,\n",
            "          0.02099827, -0.03293399],\n",
            "        [-0.01222011, -0.03689455, -0.03015323, ...,  0.0084685 ,\n",
            "         -0.01003362, -0.00217856]],\n",
            "\n",
            "       [[ 0.00039796,  0.01734753,  0.00115604, ..., -0.01870037,\n",
            "         -0.00644806,  0.03099501],\n",
            "        [-0.00404076, -0.01307326,  0.00569336, ..., -0.02194711,\n",
            "         -0.0177603 ,  0.0093618 ],\n",
            "        [ 0.01219376,  0.00836296, -0.01476904, ...,  0.00827781,\n",
            "         -0.00675068, -0.00895546],\n",
            "        ...,\n",
            "        [ 0.01019526, -0.02338131, -0.00667233, ...,  0.0345462 ,\n",
            "          0.00585316,  0.00736483],\n",
            "        [ 0.02706432,  0.00045767, -0.00713297, ...,  0.01463113,\n",
            "          0.012092  , -0.02694245],\n",
            "        [-0.00556428, -0.00077707, -0.0333611 , ...,  0.01041443,\n",
            "         -0.01144931,  0.03415414]],\n",
            "\n",
            "       ...,\n",
            "\n",
            "       [[-0.01324809, -0.00361398, -0.01946309, ..., -0.00923617,\n",
            "          0.00898144,  0.01854998],\n",
            "        [-0.00510962,  0.03107432, -0.02347333, ..., -0.01115856,\n",
            "          0.00546337, -0.00179196],\n",
            "        [-0.03378629, -0.00763076,  0.01166203, ..., -0.00272293,\n",
            "          0.0084739 , -0.014306  ],\n",
            "        ...,\n",
            "        [-0.0198228 ,  0.01758305, -0.00733042, ..., -0.01852124,\n",
            "          0.00611529, -0.0194563 ],\n",
            "        [-0.01779109, -0.01041025,  0.01380394, ...,  0.02249058,\n",
            "         -0.00721697, -0.01226874],\n",
            "        [ 0.02900611, -0.02123386,  0.00536571, ..., -0.03922645,\n",
            "          0.01450913,  0.01660734]],\n",
            "\n",
            "       [[ 0.01610823,  0.00015503,  0.00254999, ...,  0.01101751,\n",
            "          0.0149359 ,  0.00070768],\n",
            "        [-0.01984281, -0.0154898 ,  0.01205227, ..., -0.02717848,\n",
            "          0.01959057, -0.02923905],\n",
            "        [ 0.01090776, -0.03137794, -0.03486991, ..., -0.01660004,\n",
            "         -0.014333  ,  0.01553871],\n",
            "        ...,\n",
            "        [ 0.00745851,  0.02528887,  0.02614326, ...,  0.0160189 ,\n",
            "         -0.01126397,  0.00289792],\n",
            "        [ 0.01748865, -0.01642294, -0.00805053, ..., -0.00076016,\n",
            "         -0.01701578,  0.00123829],\n",
            "        [ 0.01570795,  0.01200352, -0.03042674, ...,  0.01322705,\n",
            "          0.02360276, -0.02574139]],\n",
            "\n",
            "       [[-0.00711569, -0.02285942, -0.00999778, ...,  0.00511468,\n",
            "         -0.01302971, -0.01396648],\n",
            "        [ 0.00463674,  0.02824562, -0.00099   , ..., -0.03538413,\n",
            "          0.01987241,  0.02304784],\n",
            "        [ 0.03356823,  0.02459167,  0.00241076, ..., -0.00397436,\n",
            "         -0.00951168,  0.00554333],\n",
            "        ...,\n",
            "        [ 0.00243216, -0.00528962,  0.00586489, ..., -0.02333437,\n",
            "         -0.00634919, -0.01795955],\n",
            "        [ 0.0160455 , -0.02188881,  0.01986612, ...,  0.0169162 ,\n",
            "         -0.01634056, -0.00989484],\n",
            "        [-0.01260434, -0.02377699, -0.0135808 , ..., -0.00205071,\n",
            "          0.01394132,  0.0289992 ]]], dtype=float32)>, <tf.Variable 'transformer/layer_5/self_attention/query/kernel:0' shape=(768, 12, 64) dtype=float32, numpy=\n",
            "array([[[ 3.56335135e-04, -2.32735856e-05,  1.33331772e-02, ...,\n",
            "          1.61852092e-02,  1.25169586e-02, -9.07323603e-03],\n",
            "        [-3.24009103e-03, -2.55751107e-02,  6.53225835e-03, ...,\n",
            "          1.06802424e-02, -1.11865085e-02, -1.20697441e-02],\n",
            "        [ 3.18457298e-02, -1.34844789e-02, -2.17419267e-02, ...,\n",
            "          1.43670319e-02,  6.36564055e-03, -3.96040082e-02],\n",
            "        ...,\n",
            "        [-3.55916545e-02, -6.88164867e-03, -1.14550386e-02, ...,\n",
            "         -1.71850678e-02,  8.19591898e-03,  1.82850417e-02],\n",
            "        [-1.68826524e-02, -6.41713524e-03,  1.63960662e-02, ...,\n",
            "         -2.91497968e-02, -2.49095485e-02, -1.55727891e-03],\n",
            "        [ 2.50646891e-03,  2.24127769e-02, -8.88537709e-03, ...,\n",
            "          1.52450018e-02,  1.66412760e-02,  2.77943090e-02]],\n",
            "\n",
            "       [[-1.00143915e-02, -4.82043810e-03, -1.21133430e-02, ...,\n",
            "         -2.00980902e-02,  8.78606923e-03, -5.24366787e-03],\n",
            "        [-6.97352225e-03,  8.40086211e-03, -1.92533098e-02, ...,\n",
            "         -8.99020769e-03, -1.90297607e-02,  1.42751047e-02],\n",
            "        [ 4.38408274e-03, -3.84003781e-02, -2.90074460e-02, ...,\n",
            "         -4.90498450e-03, -1.00186653e-02,  1.60772633e-02],\n",
            "        ...,\n",
            "        [ 3.51984729e-03, -2.60677710e-02, -4.01436724e-03, ...,\n",
            "         -7.26494333e-03,  1.65921599e-02,  9.43986420e-03],\n",
            "        [-2.19367887e-03,  2.77596563e-02, -6.14754716e-03, ...,\n",
            "          6.64869882e-03, -2.35262718e-02,  2.67976467e-02],\n",
            "        [-1.65292621e-02, -7.35483016e-04, -1.79096721e-02, ...,\n",
            "         -3.00570037e-02, -4.06934507e-03,  2.81708278e-02]],\n",
            "\n",
            "       [[-2.75929105e-02, -1.42425369e-03,  2.16096658e-02, ...,\n",
            "         -3.61361168e-02,  7.99643993e-03,  1.76639855e-03],\n",
            "        [ 3.51686440e-02, -7.32105784e-03, -2.11124634e-03, ...,\n",
            "          8.33581574e-03,  1.05309754e-03, -3.11690327e-02],\n",
            "        [-1.65101681e-02,  7.84467906e-03, -6.52687438e-03, ...,\n",
            "         -1.91618502e-02, -1.21344160e-03,  3.43308300e-02],\n",
            "        ...,\n",
            "        [ 2.04825830e-02, -1.28496597e-02, -1.35432696e-02, ...,\n",
            "         -4.48501436e-03,  2.29730248e-03, -8.21327046e-03],\n",
            "        [ 8.20034649e-03,  3.10740969e-03, -1.56295225e-02, ...,\n",
            "         -1.16739208e-02,  5.71083417e-03,  3.49741243e-02],\n",
            "        [ 5.87546220e-03,  2.59113014e-02,  4.07530647e-03, ...,\n",
            "         -1.04460036e-02, -1.02885137e-03,  1.49746118e-02]],\n",
            "\n",
            "       ...,\n",
            "\n",
            "       [[-1.93753690e-02, -7.01978570e-03,  1.37912547e-02, ...,\n",
            "          3.63377552e-03, -2.24654339e-02, -3.45739275e-02],\n",
            "        [ 9.23623051e-03,  1.57913901e-02, -2.93385219e-02, ...,\n",
            "         -1.35402577e-02,  2.59163175e-02, -1.76091436e-02],\n",
            "        [-1.70429479e-02, -2.19473243e-02,  4.48269909e-03, ...,\n",
            "         -2.62889266e-02,  2.68142149e-02,  1.08198181e-03],\n",
            "        ...,\n",
            "        [ 2.27384847e-02,  1.01455324e-03,  2.57161297e-02, ...,\n",
            "         -2.82955188e-02,  3.64420079e-02,  1.15805038e-03],\n",
            "        [ 1.89109184e-02, -2.86126770e-02, -2.98335142e-02, ...,\n",
            "          2.11662650e-02,  2.41156332e-02,  2.15139147e-02],\n",
            "        [-6.04787469e-03,  1.67272594e-02, -1.30752455e-02, ...,\n",
            "          3.49946879e-02,  1.61797635e-03, -5.19174105e-03]],\n",
            "\n",
            "       [[ 2.01301388e-02, -2.48187035e-02,  2.34728884e-02, ...,\n",
            "          2.44837180e-02,  2.16034052e-04, -3.18484660e-03],\n",
            "        [-4.21620440e-03,  2.41202898e-02,  2.81057879e-02, ...,\n",
            "         -1.59815466e-03,  2.83372663e-02, -1.72247067e-02],\n",
            "        [ 1.58773195e-02, -1.38092078e-02, -1.78879574e-02, ...,\n",
            "         -7.57325767e-03,  3.30804810e-02,  1.43452054e-02],\n",
            "        ...,\n",
            "        [ 1.65340640e-02,  1.14257736e-02, -9.77638178e-03, ...,\n",
            "          2.54719649e-02, -1.46735124e-02,  1.18024899e-02],\n",
            "        [ 8.06490984e-03,  1.42918201e-03, -4.40722844e-03, ...,\n",
            "          6.72632782e-03,  2.41471268e-02,  2.60095522e-02],\n",
            "        [ 1.03340670e-02, -2.99003031e-02,  4.51040035e-03, ...,\n",
            "          5.74800884e-03,  2.18160767e-02,  2.93497578e-03]],\n",
            "\n",
            "       [[-9.57218837e-03,  2.86422495e-04,  2.86736228e-02, ...,\n",
            "         -1.57884900e-02,  2.72097462e-03,  1.19082993e-02],\n",
            "        [-1.00375153e-03, -3.59839690e-03,  1.87993757e-02, ...,\n",
            "         -1.13554168e-02, -2.13033929e-02, -7.39891175e-03],\n",
            "        [ 3.08725741e-02,  3.37167308e-02, -6.19724719e-03, ...,\n",
            "         -2.25871638e-03, -1.50927156e-02, -2.61294954e-02],\n",
            "        ...,\n",
            "        [-1.70711968e-02,  1.64682101e-02,  2.03144047e-02, ...,\n",
            "         -6.22906431e-04, -3.67704267e-03,  2.81105423e-03],\n",
            "        [-8.38693697e-03, -1.28459632e-02, -2.08328236e-02, ...,\n",
            "         -4.48200526e-03,  1.49425417e-02, -4.10502637e-03],\n",
            "        [ 7.41163408e-03, -2.05717795e-02,  1.14371674e-02, ...,\n",
            "         -2.99211256e-02, -3.40752746e-03,  3.37894969e-02]]],\n",
            "      dtype=float32)>, <tf.Variable 'transformer/layer_10/self_attention/value/kernel:0' shape=(768, 12, 64) dtype=float32, numpy=\n",
            "array([[[ 0.00253291,  0.00360948, -0.00822956, ..., -0.01059766,\n",
            "         -0.01738737, -0.00563838],\n",
            "        [ 0.00956649, -0.00608305,  0.01478258, ...,  0.01251805,\n",
            "          0.00484285, -0.00527903],\n",
            "        [ 0.02873963, -0.00703076,  0.02377206, ...,  0.01809891,\n",
            "         -0.01797575,  0.03163191],\n",
            "        ...,\n",
            "        [-0.02802426,  0.0001475 ,  0.01997671, ..., -0.02334676,\n",
            "         -0.01032293, -0.0135519 ],\n",
            "        [ 0.00118509, -0.00786914,  0.01208247, ..., -0.01849513,\n",
            "         -0.02432666, -0.03383921],\n",
            "        [-0.02667532,  0.00382225,  0.00722597, ...,  0.00125773,\n",
            "         -0.02920449,  0.00537784]],\n",
            "\n",
            "       [[ 0.02701816, -0.00790904,  0.00840387, ...,  0.01344662,\n",
            "          0.01571135, -0.01045477],\n",
            "        [ 0.03018543,  0.03757926,  0.03463691, ..., -0.01522513,\n",
            "         -0.00253229,  0.00222473],\n",
            "        [-0.00558223,  0.01441545, -0.00095843, ..., -0.00264192,\n",
            "          0.02321912, -0.03263977],\n",
            "        ...,\n",
            "        [ 0.00431362, -0.00302172, -0.02333912, ..., -0.0113334 ,\n",
            "         -0.01095101, -0.00929168],\n",
            "        [-0.03288312,  0.01055465, -0.00367782, ...,  0.00409723,\n",
            "          0.02053689, -0.01215454],\n",
            "        [-0.01474165, -0.009899  , -0.00260639, ...,  0.0107824 ,\n",
            "          0.03150645, -0.00648062]],\n",
            "\n",
            "       [[ 0.00057509, -0.00861601,  0.02568314, ...,  0.0001624 ,\n",
            "         -0.03260073, -0.00545998],\n",
            "        [-0.01664523,  0.01822418,  0.00549182, ..., -0.01997329,\n",
            "          0.00160599, -0.01014197],\n",
            "        [-0.0231503 ,  0.02684089, -0.03765827, ...,  0.02897092,\n",
            "          0.03560318,  0.03035774],\n",
            "        ...,\n",
            "        [ 0.02352046,  0.01235967, -0.02291855, ...,  0.01714728,\n",
            "          0.01434196, -0.03823558],\n",
            "        [ 0.00580255, -0.00480754,  0.01384667, ..., -0.02409382,\n",
            "         -0.01981004,  0.00880588],\n",
            "        [-0.01276037, -0.02551072,  0.00384431, ..., -0.02135518,\n",
            "         -0.00010969,  0.00110761]],\n",
            "\n",
            "       ...,\n",
            "\n",
            "       [[ 0.00306106, -0.00348323, -0.01260918, ..., -0.01719235,\n",
            "          0.00138271, -0.03486465],\n",
            "        [ 0.00165505,  0.02509753, -0.00602012, ...,  0.00096098,\n",
            "          0.00662884,  0.00428996],\n",
            "        [ 0.02220692, -0.00773395,  0.01540833, ..., -0.01923705,\n",
            "         -0.02027796,  0.02177273],\n",
            "        ...,\n",
            "        [ 0.02071967,  0.01607929, -0.00155623, ...,  0.00190846,\n",
            "          0.00388259, -0.02491432],\n",
            "        [-0.0204132 ,  0.02261025, -0.0231829 , ..., -0.00592366,\n",
            "          0.01575719, -0.0163698 ],\n",
            "        [ 0.000334  ,  0.00580313,  0.0276912 , ...,  0.00117043,\n",
            "         -0.014785  ,  0.00359264]],\n",
            "\n",
            "       [[-0.01382082, -0.01119491, -0.00814428, ..., -0.00789185,\n",
            "         -0.00146319, -0.01648418],\n",
            "        [ 0.01793027,  0.00218664, -0.03403345, ...,  0.00955099,\n",
            "         -0.00924776, -0.02843373],\n",
            "        [ 0.02123085,  0.03396   , -0.02099849, ...,  0.02884684,\n",
            "         -0.01548149,  0.01510671],\n",
            "        ...,\n",
            "        [-0.00456902, -0.02124647,  0.01818153, ...,  0.00052247,\n",
            "         -0.00980371, -0.02855741],\n",
            "        [-0.00467034, -0.01412127,  0.01282842, ..., -0.01227206,\n",
            "          0.02433664,  0.03567555],\n",
            "        [ 0.00334888, -0.03102642, -0.00451099, ...,  0.0109637 ,\n",
            "         -0.00326718, -0.01601491]],\n",
            "\n",
            "       [[-0.00361069,  0.01969981, -0.0276685 , ..., -0.00690684,\n",
            "         -0.02825072,  0.02378279],\n",
            "        [-0.02942946,  0.00353662, -0.01336262, ...,  0.00586204,\n",
            "          0.01165358,  0.01144239],\n",
            "        [-0.02766266, -0.02440829, -0.00201748, ..., -0.01169798,\n",
            "         -0.03877532,  0.0352767 ],\n",
            "        ...,\n",
            "        [ 0.02828536,  0.00216737,  0.00520374, ...,  0.00252082,\n",
            "          0.01256758, -0.00187707],\n",
            "        [-0.0177106 , -0.00939036, -0.00283812, ..., -0.02363204,\n",
            "          0.00839708,  0.0287434 ],\n",
            "        [ 0.01553113, -0.02477494, -0.00503806, ...,  0.00330489,\n",
            "         -0.00377038,  0.02008582]]], dtype=float32)>, <tf.Variable 'transformer/layer_8/output_layer_norm/gamma:0' shape=(768,) dtype=float32, numpy=\n",
            "array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1.], dtype=float32)>, <tf.Variable 'transformer/layer_0/intermediate/bias:0' shape=(3072,) dtype=float32, numpy=array([0., 0., 0., ..., 0., 0., 0.], dtype=float32)>, <tf.Variable 'transformer/layer_8/intermediate/kernel:0' shape=(768, 3072) dtype=float32, numpy=\n",
            "array([[ 0.02252238,  0.00949301, -0.00412111, ...,  0.00138086,\n",
            "        -0.00694582, -0.01160689],\n",
            "       [-0.01875431,  0.00559358,  0.03416299, ...,  0.02474842,\n",
            "        -0.00972873,  0.01206683],\n",
            "       [ 0.02053335, -0.01685546, -0.01893356, ...,  0.00515254,\n",
            "         0.01295144,  0.0075004 ],\n",
            "       ...,\n",
            "       [ 0.0277773 , -0.00293457, -0.01040187, ..., -0.03441886,\n",
            "         0.01202073, -0.01324569],\n",
            "       [-0.00970065, -0.03521056,  0.01237178, ...,  0.00240132,\n",
            "         0.02589898,  0.01750536],\n",
            "       [-0.00373762,  0.00593145, -0.00070142, ...,  0.01475507,\n",
            "        -0.01688491, -0.00554673]], dtype=float32)>, <tf.Variable 'transformer/layer_2/output/bias:0' shape=(768,) dtype=float32, numpy=\n",
            "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0.], dtype=float32)>, <tf.Variable 'transformer/layer_0/self_attention_output/bias:0' shape=(768,) dtype=float32, numpy=\n",
            "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0.], dtype=float32)>, <tf.Variable 'transformer/layer_10/output/bias:0' shape=(768,) dtype=float32, numpy=\n",
            "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0.], dtype=float32)>, <tf.Variable 'transformer/layer_7/output_layer_norm/gamma:0' shape=(768,) dtype=float32, numpy=\n",
            "array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1.], dtype=float32)>, <tf.Variable 'pooler_transform/kernel:0' shape=(768, 768) dtype=float32, numpy=\n",
            "array([[-0.02691879, -0.01547131, -0.00954922, ..., -0.01522219,\n",
            "        -0.00187504, -0.00247219],\n",
            "       [ 0.00023604, -0.0031182 , -0.00548958, ..., -0.02421261,\n",
            "         0.00555143,  0.00149786],\n",
            "       [ 0.02680034, -0.00178333,  0.00731596, ..., -0.00393132,\n",
            "        -0.01433073,  0.01482682],\n",
            "       ...,\n",
            "       [ 0.02819691, -0.02432967,  0.00936369, ...,  0.02359879,\n",
            "         0.01582903, -0.01080854],\n",
            "       [-0.03943216, -0.02054652,  0.00419633, ..., -0.00910006,\n",
            "        -0.02138307, -0.03039858],\n",
            "       [ 0.00031095,  0.00659131, -0.00170383, ..., -0.00466294,\n",
            "         0.00625639, -0.03214692]], dtype=float32)>, <tf.Variable 'transformer/layer_9/output/bias:0' shape=(768,) dtype=float32, numpy=\n",
            "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0.], dtype=float32)>, <tf.Variable 'transformer/layer_0/output_layer_norm/beta:0' shape=(768,) dtype=float32, numpy=\n",
            "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0.], dtype=float32)>, <tf.Variable 'transformer/layer_11/self_attention/query/bias:0' shape=(12, 64) dtype=float32, numpy=\n",
            "array([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
            "      dtype=float32)>, <tf.Variable 'transformer/layer_2/self_attention/key/kernel:0' shape=(768, 12, 64) dtype=float32, numpy=\n",
            "array([[[-3.40603255e-02,  2.48340908e-02,  4.31621354e-03, ...,\n",
            "          4.47451742e-03,  8.72777135e-04,  3.72187421e-02],\n",
            "        [-9.59481520e-04, -1.55715644e-02, -1.27598429e-02, ...,\n",
            "         -4.70777601e-03, -8.55033693e-04,  2.84534562e-02],\n",
            "        [ 8.71170123e-05, -2.06581112e-02,  1.49908578e-02, ...,\n",
            "         -1.21901715e-02, -6.51284866e-03,  3.87053117e-02],\n",
            "        ...,\n",
            "        [ 2.76383981e-02,  5.02336072e-04,  1.73572579e-03, ...,\n",
            "          2.19244137e-02,  1.71695054e-02, -1.92925427e-02],\n",
            "        [ 9.91322286e-03, -3.19079123e-03, -1.83524266e-02, ...,\n",
            "          2.25744322e-02,  5.62916137e-03,  1.05902683e-02],\n",
            "        [-3.10181640e-03,  1.80769619e-02, -3.42601873e-02, ...,\n",
            "         -3.10952775e-03,  4.46626963e-03, -5.53352060e-03]],\n",
            "\n",
            "       [[-4.28526709e-03,  1.94319952e-02,  8.38772301e-03, ...,\n",
            "         -2.28314195e-02,  3.49486433e-03, -5.96043468e-03],\n",
            "        [ 2.93814316e-02, -1.16673503e-02,  1.33026382e-02, ...,\n",
            "         -2.13910080e-02,  2.75825355e-02,  3.52222612e-03],\n",
            "        [-5.62544214e-03,  8.46358761e-03,  1.85151305e-02, ...,\n",
            "          1.69285741e-02,  9.70479567e-03, -2.05745026e-02],\n",
            "        ...,\n",
            "        [-1.72263663e-02, -6.15604920e-03,  2.46229004e-02, ...,\n",
            "         -3.36917536e-03, -3.51118520e-02,  1.43541209e-02],\n",
            "        [ 4.14974382e-03, -8.27355590e-03, -1.97280962e-02, ...,\n",
            "         -1.79501753e-02,  7.71045918e-03, -1.41087770e-02],\n",
            "        [ 1.60559453e-02, -2.24860106e-02,  2.39391010e-02, ...,\n",
            "          3.04303002e-02,  1.57857407e-02,  2.55276952e-02]],\n",
            "\n",
            "       [[ 2.16094367e-02, -1.88782252e-02,  1.39932586e-02, ...,\n",
            "          6.50862325e-03,  1.51790315e-02,  1.35706458e-02],\n",
            "        [-2.10323790e-03, -2.67631784e-02, -2.02333857e-03, ...,\n",
            "          1.64469201e-02, -2.09895931e-02,  4.16407798e-04],\n",
            "        [-4.46999911e-03, -5.53364493e-03, -6.90267468e-03, ...,\n",
            "          1.00729298e-02, -1.81053877e-02,  3.69198644e-03],\n",
            "        ...,\n",
            "        [ 2.31600516e-02, -2.33805622e-03,  3.67734791e-03, ...,\n",
            "         -2.82402453e-03, -1.03015108e-02, -3.35995443e-02],\n",
            "        [-7.16204103e-03,  4.43715369e-03,  1.51013276e-02, ...,\n",
            "         -2.26933975e-02, -1.73591413e-02,  1.19380653e-02],\n",
            "        [-2.80363159e-03,  2.43193302e-02,  7.77245872e-03, ...,\n",
            "         -2.83370707e-02,  1.67228829e-03,  5.70050860e-03]],\n",
            "\n",
            "       ...,\n",
            "\n",
            "       [[-9.21569299e-03, -1.22946687e-03,  2.60464922e-02, ...,\n",
            "          3.27050239e-02,  5.61113935e-03,  5.59445797e-03],\n",
            "        [ 1.10704396e-02,  2.85006315e-03,  1.35416649e-02, ...,\n",
            "          2.62173023e-02, -6.23550639e-03, -1.14198332e-03],\n",
            "        [ 1.85304340e-02,  1.92693435e-02, -2.01000646e-02, ...,\n",
            "         -6.78383559e-03, -1.70001462e-02,  2.78275106e-02],\n",
            "        ...,\n",
            "        [ 1.19172847e-02,  2.09587067e-02,  8.75656959e-03, ...,\n",
            "         -1.94258429e-03,  1.20462403e-02,  2.31243074e-02],\n",
            "        [ 1.67887788e-02,  7.99499452e-03, -1.58891063e-02, ...,\n",
            "          2.78969686e-02,  2.77028815e-03,  1.23582585e-02],\n",
            "        [-2.67471168e-02, -2.08699796e-02, -5.39774867e-03, ...,\n",
            "          3.32105048e-02, -7.77245313e-03,  1.93273611e-02]],\n",
            "\n",
            "       [[ 8.00679321e-04,  1.14204688e-02,  7.99567963e-04, ...,\n",
            "          5.70321269e-03,  5.53084258e-03,  6.21204963e-03],\n",
            "        [ 1.67677049e-02, -3.39433439e-02, -4.18802770e-03, ...,\n",
            "         -2.58650519e-02,  2.01869872e-03,  1.07343458e-02],\n",
            "        [-2.12085322e-02, -3.27258334e-02,  1.86669792e-03, ...,\n",
            "          1.56602673e-02,  1.00975595e-02,  1.36732245e-02],\n",
            "        ...,\n",
            "        [ 3.95926498e-02,  2.80120336e-02, -1.13151819e-02, ...,\n",
            "          3.75003438e-03,  1.36369606e-04,  3.25754881e-02],\n",
            "        [ 1.71828344e-02,  7.98472203e-03, -1.34489248e-02, ...,\n",
            "          1.71628278e-02, -1.37812598e-02,  2.12091934e-02],\n",
            "        [-1.21067530e-02, -1.12261623e-03,  2.03924701e-02, ...,\n",
            "         -1.26401028e-02, -2.20300201e-02, -2.27548815e-02]],\n",
            "\n",
            "       [[-4.93544526e-03, -2.17870809e-02,  4.82348772e-03, ...,\n",
            "          4.46491176e-03, -7.85106141e-03, -1.64548624e-02],\n",
            "        [-1.18212507e-03, -2.94696297e-02, -1.44997127e-02, ...,\n",
            "         -1.27725559e-03, -4.81611869e-06,  1.67878550e-02],\n",
            "        [-2.09424961e-02, -1.44257760e-02,  4.82122134e-03, ...,\n",
            "         -1.91680100e-02,  7.10360054e-03,  3.68419359e-03],\n",
            "        ...,\n",
            "        [ 3.21170539e-02, -2.60854438e-02, -7.81090325e-03, ...,\n",
            "         -1.00867273e-02,  1.39100011e-02, -2.23832042e-03],\n",
            "        [-5.45487413e-03,  2.24798848e-03,  1.16290888e-02, ...,\n",
            "          8.73968285e-03,  8.23229179e-03, -3.00336222e-04],\n",
            "        [ 8.72703921e-03, -1.09310169e-02, -3.35149467e-02, ...,\n",
            "          1.26837206e-03,  1.61604155e-02,  1.10363802e-02]]],\n",
            "      dtype=float32)>, <tf.Variable 'transformer/layer_0/self_attention/key/kernel:0' shape=(768, 12, 64) dtype=float32, numpy=\n",
            "array([[[-2.16488983e-03,  2.97806710e-02, -5.49196592e-03, ...,\n",
            "         -1.05251204e-02,  6.81113871e-03,  3.05113532e-02],\n",
            "        [ 2.47225631e-02, -1.48986904e-02,  4.63280408e-03, ...,\n",
            "          2.09972877e-02, -3.16646658e-02, -2.23713722e-02],\n",
            "        [ 3.93751077e-02,  2.61410587e-02,  1.04044089e-02, ...,\n",
            "          5.33897290e-03,  7.31071504e-03, -9.90463141e-03],\n",
            "        ...,\n",
            "        [ 5.33894962e-03,  1.17995800e-03, -7.39645038e-04, ...,\n",
            "          1.81909055e-02, -1.37966154e-02,  2.39914395e-02],\n",
            "        [ 3.56150530e-02,  3.93366441e-02, -1.17674274e-02, ...,\n",
            "          1.44291483e-02, -1.19023491e-02, -2.66117640e-02],\n",
            "        [-1.93523895e-02, -6.19581377e-04, -2.03719903e-02, ...,\n",
            "         -1.18005159e-03, -1.63699724e-02, -6.81179995e-03]],\n",
            "\n",
            "       [[-3.77027616e-02,  2.87856329e-02, -1.70779321e-02, ...,\n",
            "          1.54628663e-03,  1.49468752e-02, -4.78248252e-03],\n",
            "        [ 2.32736245e-02,  1.10322535e-02,  3.49715934e-03, ...,\n",
            "          2.48069800e-02, -3.19811106e-02, -1.76390202e-03],\n",
            "        [ 1.29249471e-03, -1.98022649e-02,  1.71814840e-02, ...,\n",
            "          3.32888849e-02, -8.19712598e-03,  1.21512199e-02],\n",
            "        ...,\n",
            "        [-7.36288307e-03,  2.14622240e-04, -2.00766418e-03, ...,\n",
            "         -1.01765264e-02, -1.40917096e-02, -2.84823403e-02],\n",
            "        [ 5.51954983e-03,  2.92394925e-02, -2.48422399e-02, ...,\n",
            "         -6.90581836e-03,  5.62288705e-03,  2.78788935e-02],\n",
            "        [-2.92720031e-02,  2.06919499e-02,  3.91299017e-02, ...,\n",
            "          1.04333758e-02, -1.08133741e-02,  2.14731432e-02]],\n",
            "\n",
            "       [[-1.36425300e-02,  1.62859559e-02, -2.26382632e-03, ...,\n",
            "         -1.07309660e-02, -5.97388064e-03,  1.46280462e-02],\n",
            "        [-2.57362239e-02,  2.67235562e-03,  2.84969853e-03, ...,\n",
            "          2.26693079e-02, -1.74207762e-02,  7.52275251e-03],\n",
            "        [-1.32016717e-02,  1.59863345e-02, -2.77632428e-03, ...,\n",
            "          1.12070488e-02,  8.51046853e-03,  2.53667235e-02],\n",
            "        ...,\n",
            "        [-2.79159239e-03,  2.01083291e-02,  1.99589878e-02, ...,\n",
            "          2.54215114e-03,  3.37676667e-02,  1.72066856e-02],\n",
            "        [-9.13888775e-03, -1.40141612e-02,  1.38186768e-03, ...,\n",
            "         -2.43562460e-02, -3.53556536e-02, -2.17577145e-02],\n",
            "        [-3.21795680e-02, -2.47424934e-03,  1.85724925e-02, ...,\n",
            "         -2.13478245e-02,  2.54456419e-02,  2.78547127e-03]],\n",
            "\n",
            "       ...,\n",
            "\n",
            "       [[-8.94660410e-03, -1.56287476e-02,  1.02973022e-02, ...,\n",
            "          1.89399719e-02, -1.05916103e-02, -1.91691164e-02],\n",
            "        [-1.60819534e-02,  1.86877325e-02, -1.75029449e-02, ...,\n",
            "          5.07054199e-03,  9.34123178e-04, -2.17459481e-02],\n",
            "        [ 9.39454045e-03,  6.89299079e-03, -3.19688469e-02, ...,\n",
            "          2.56278608e-02,  2.40053609e-02, -7.59006361e-04],\n",
            "        ...,\n",
            "        [-2.16874736e-03,  1.60393286e-02, -1.07758138e-02, ...,\n",
            "         -1.11955339e-02, -3.55313122e-02, -1.37181096e-02],\n",
            "        [ 3.45195942e-02, -8.04140605e-03,  1.24056516e-02, ...,\n",
            "         -9.12469160e-03, -1.99400354e-02,  1.16732642e-02],\n",
            "        [-2.12215986e-02,  1.78668916e-03, -9.14311595e-03, ...,\n",
            "         -3.17855738e-02, -3.03239264e-02, -8.03388376e-03]],\n",
            "\n",
            "       [[-1.42850252e-02, -1.77695137e-02,  1.32433837e-02, ...,\n",
            "          9.70601849e-03,  6.81483326e-03, -1.24405893e-02],\n",
            "        [ 2.27898322e-02,  2.07445282e-03, -8.77294689e-04, ...,\n",
            "         -1.02220774e-02,  2.46393355e-03, -5.92535222e-03],\n",
            "        [-2.11732760e-02,  3.24669154e-03, -2.46202610e-02, ...,\n",
            "          2.06276886e-02,  4.84917639e-03, -1.16675301e-02],\n",
            "        ...,\n",
            "        [ 2.39800560e-04,  2.18482297e-02, -5.32313855e-03, ...,\n",
            "         -1.45258401e-02,  1.28739784e-02, -7.91986752e-03],\n",
            "        [ 1.36811016e-02, -1.91645846e-02, -2.14459766e-02, ...,\n",
            "          2.81122816e-03,  3.11123696e-03,  1.03602884e-02],\n",
            "        [-1.37554836e-02,  1.07159317e-02,  3.22132446e-02, ...,\n",
            "          3.46678160e-02, -5.82183106e-03, -2.92133000e-02]],\n",
            "\n",
            "       [[-4.02812846e-03, -1.73314370e-03, -3.73222232e-02, ...,\n",
            "         -1.31357484e-03, -6.08620560e-03, -4.14306344e-03],\n",
            "        [ 2.00692657e-02,  1.54347476e-02, -2.40550726e-03, ...,\n",
            "          1.58375446e-02,  1.48836216e-02,  3.71165364e-03],\n",
            "        [ 3.02246921e-02,  8.82034563e-03,  2.72462890e-02, ...,\n",
            "         -3.38396505e-02, -2.13852823e-02,  1.95666542e-03],\n",
            "        ...,\n",
            "        [-6.09196977e-05,  2.26054229e-02, -1.80104538e-03, ...,\n",
            "         -3.42465267e-02,  4.77356277e-03, -1.05115408e-02],\n",
            "        [-1.58289075e-02, -2.91673914e-02,  3.31048062e-03, ...,\n",
            "         -3.01729771e-03, -2.01483667e-02, -4.30618413e-03],\n",
            "        [-2.79390644e-02, -1.82271264e-02,  7.90431921e-04, ...,\n",
            "         -1.01067675e-02,  2.04442069e-02, -7.22406572e-03]]],\n",
            "      dtype=float32)>, <tf.Variable 'transformer/layer_1/intermediate/bias:0' shape=(3072,) dtype=float32, numpy=array([0., 0., 0., ..., 0., 0., 0.], dtype=float32)>, <tf.Variable 'embeddings/layer_norm/gamma:0' shape=(768,) dtype=float32, numpy=\n",
            "array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1.], dtype=float32)>, <tf.Variable 'transformer/layer_3/self_attention_output/bias:0' shape=(768,) dtype=float32, numpy=\n",
            "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0.], dtype=float32)>, <tf.Variable 'transformer/layer_10/self_attention/key/kernel:0' shape=(768, 12, 64) dtype=float32, numpy=\n",
            "array([[[ 1.26620643e-02, -9.49203223e-03,  6.34855777e-03, ...,\n",
            "         -3.35969292e-02, -9.82747134e-03, -3.45065631e-02],\n",
            "        [-2.58174865e-03,  6.17407123e-03,  1.87952109e-02, ...,\n",
            "          8.18939880e-03,  1.41727719e-02,  2.46475115e-02],\n",
            "        [ 1.24116894e-02, -7.77692581e-03,  3.53119969e-02, ...,\n",
            "          6.91379793e-03, -9.84181091e-03, -3.55637632e-03],\n",
            "        ...,\n",
            "        [-1.66501559e-03,  3.03626480e-03, -2.86333468e-02, ...,\n",
            "         -2.06425525e-02,  6.40923949e-03,  9.04533546e-03],\n",
            "        [ 1.85543243e-02,  2.23028008e-02,  2.13198327e-02, ...,\n",
            "          2.55976338e-02, -6.48212992e-03,  1.30484230e-03],\n",
            "        [ 1.79667715e-02,  2.54358333e-02,  1.58508923e-02, ...,\n",
            "          2.61625205e-03,  7.57199619e-03,  3.00478726e-03]],\n",
            "\n",
            "       [[ 7.56555609e-03,  5.22137387e-03, -1.55756557e-02, ...,\n",
            "         -3.02766799e-03, -8.88847001e-03, -5.57916565e-03],\n",
            "        [-2.05644369e-02,  2.45831273e-02,  2.76032183e-02, ...,\n",
            "          1.66841391e-02, -1.53502338e-02,  3.39165069e-02],\n",
            "        [ 1.23859309e-02,  7.83370342e-03,  7.38278450e-03, ...,\n",
            "          7.52397813e-03, -5.45027293e-03,  3.25048305e-02],\n",
            "        ...,\n",
            "        [-1.24415094e-02,  1.38340592e-02, -9.07975342e-03, ...,\n",
            "          2.55691446e-02, -5.10144746e-03,  7.19064192e-05],\n",
            "        [ 2.62559834e-03,  1.99484397e-02,  3.44851054e-02, ...,\n",
            "         -1.45502342e-02,  1.50413450e-03, -2.52914280e-02],\n",
            "        [-9.30064265e-03, -2.07926724e-02,  7.83094950e-03, ...,\n",
            "          1.78886782e-02, -1.87336560e-02,  1.20065659e-02]],\n",
            "\n",
            "       [[ 9.25070047e-03, -8.41292739e-03,  2.36751139e-02, ...,\n",
            "          3.01191378e-02, -1.02042537e-02, -8.47237092e-03],\n",
            "        [ 1.05584906e-02, -2.32641641e-02, -3.12632136e-02, ...,\n",
            "          2.97421310e-02, -7.58527266e-03,  2.24743020e-02],\n",
            "        [-1.80352665e-02,  1.55894728e-02, -5.35401609e-03, ...,\n",
            "          1.23532861e-02, -1.29677756e-02,  3.19408998e-02],\n",
            "        ...,\n",
            "        [-9.22492333e-03, -3.11503597e-02,  2.78739464e-02, ...,\n",
            "          1.46549316e-02,  2.21812986e-02, -3.21060768e-03],\n",
            "        [-1.01949181e-02,  1.12055792e-02,  2.41517206e-03, ...,\n",
            "         -9.56037175e-03, -1.62602607e-02, -2.22811010e-02],\n",
            "        [ 3.24280793e-03,  3.54414899e-03, -3.48886251e-02, ...,\n",
            "         -3.43164953e-04,  1.58394035e-02, -8.94027855e-03]],\n",
            "\n",
            "       ...,\n",
            "\n",
            "       [[-1.79713890e-02, -8.79825372e-03, -6.10009022e-03, ...,\n",
            "          2.19751112e-02, -1.40069835e-02, -5.91348810e-03],\n",
            "        [ 1.03982175e-02, -2.18134858e-02,  1.78229045e-02, ...,\n",
            "          3.53744626e-02, -4.65866970e-03,  1.06104324e-02],\n",
            "        [ 9.99976043e-03,  4.90683923e-03,  2.04605963e-02, ...,\n",
            "         -2.41274480e-02, -1.97768379e-02, -4.99842642e-03],\n",
            "        ...,\n",
            "        [ 1.79415941e-02, -9.07415058e-03,  7.99767114e-03, ...,\n",
            "          1.21019008e-02,  1.76106524e-02, -2.33016536e-02],\n",
            "        [-1.59387402e-02,  7.96488114e-03, -2.32556257e-02, ...,\n",
            "          6.56773895e-03, -3.28114047e-03, -2.61450522e-02],\n",
            "        [-1.06762219e-02,  1.72622446e-02,  2.11423226e-02, ...,\n",
            "          1.49050495e-02,  1.44330887e-02, -7.21823750e-03]],\n",
            "\n",
            "       [[ 9.69014969e-03,  1.24677913e-02, -3.68395634e-02, ...,\n",
            "         -9.79928393e-03,  4.39819414e-03,  5.93533786e-03],\n",
            "        [ 1.69232264e-02, -2.67432202e-02,  1.36959562e-02, ...,\n",
            "         -1.98780429e-02,  3.53789981e-03,  1.90391690e-02],\n",
            "        [ 3.42542157e-02, -1.61167588e-02, -5.03914710e-03, ...,\n",
            "          2.97253486e-02, -3.20408097e-03, -4.67711780e-03],\n",
            "        ...,\n",
            "        [-7.81021267e-03,  2.14766879e-02, -3.78856063e-02, ...,\n",
            "          1.49144363e-02,  2.54221428e-02, -5.65621071e-03],\n",
            "        [ 2.40372680e-02, -7.94946682e-03, -6.30434044e-03, ...,\n",
            "          2.58231796e-02,  4.38703364e-03,  3.97137832e-03],\n",
            "        [-1.65306893e-03,  1.13387201e-02,  2.37930790e-02, ...,\n",
            "         -3.88443246e-02,  1.17706396e-02,  1.38191925e-02]],\n",
            "\n",
            "       [[ 8.63347482e-03, -2.71938667e-02, -3.08683421e-03, ...,\n",
            "         -2.29129735e-02,  2.56447191e-03, -6.87343068e-04],\n",
            "        [-1.00917136e-02,  3.38850287e-03,  1.57090221e-02, ...,\n",
            "          1.28486864e-02, -2.08643638e-02, -1.19568026e-02],\n",
            "        [ 2.12942306e-02, -7.17233168e-03,  1.40598333e-02, ...,\n",
            "          2.57468928e-04,  1.15915127e-02,  2.16092039e-02],\n",
            "        ...,\n",
            "        [-3.05564087e-02, -1.30460262e-02, -3.85324210e-02, ...,\n",
            "         -2.29610987e-02,  1.31320879e-02, -4.70431801e-03],\n",
            "        [ 8.39558616e-03,  8.88247159e-04,  3.18535231e-02, ...,\n",
            "         -2.40566432e-02, -1.07532879e-02, -3.25048119e-02],\n",
            "        [ 3.99442902e-03,  1.21479202e-02, -2.59302510e-03, ...,\n",
            "          8.65557231e-03,  5.81969274e-04, -4.32468392e-03]]],\n",
            "      dtype=float32)>, <tf.Variable 'transformer/layer_3/self_attention_layer_norm/beta:0' shape=(768,) dtype=float32, numpy=\n",
            "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0.], dtype=float32)>, <tf.Variable 'transformer/layer_9/intermediate/bias:0' shape=(3072,) dtype=float32, numpy=array([0., 0., 0., ..., 0., 0., 0.], dtype=float32)>, <tf.Variable 'transformer/layer_11/self_attention_output/kernel:0' shape=(12, 64, 768) dtype=float32, numpy=\n",
            "array([[[-1.31572401e-02, -2.96838190e-02,  9.04094800e-03, ...,\n",
            "          1.71969272e-02, -1.53277663e-03, -3.48775014e-02],\n",
            "        [ 1.27782868e-02,  9.44816880e-03, -1.30363591e-02, ...,\n",
            "          6.02292363e-03, -8.08397867e-03,  2.92179678e-02],\n",
            "        [-3.23938997e-03, -1.93472505e-02,  6.96254242e-03, ...,\n",
            "         -1.47687108e-03, -2.88429372e-02, -1.09696807e-02],\n",
            "        ...,\n",
            "        [ 1.81675423e-02,  1.72032919e-02,  5.84057579e-03, ...,\n",
            "          1.70059409e-03,  2.25801338e-02,  1.26357218e-02],\n",
            "        [-3.14014195e-03, -1.40014887e-02, -1.58062093e-02, ...,\n",
            "          8.04249197e-03,  1.46340318e-02, -5.53627964e-03],\n",
            "        [-2.30646655e-02, -2.99636554e-02, -2.92466627e-03, ...,\n",
            "          2.36212555e-02,  2.07622740e-02,  7.91004021e-03]],\n",
            "\n",
            "       [[-1.89542584e-02,  9.83258476e-04,  1.06680428e-03, ...,\n",
            "          7.78212072e-03, -1.70409940e-02,  9.41188913e-03],\n",
            "        [-2.93656010e-02,  2.50628814e-02, -9.85890583e-05, ...,\n",
            "         -1.56601164e-02, -7.90176634e-03, -1.80559326e-02],\n",
            "        [ 1.86738663e-03, -1.04580428e-02,  1.62846651e-02, ...,\n",
            "         -6.42673764e-03,  7.55304750e-03,  5.48209948e-03],\n",
            "        ...,\n",
            "        [-2.87074354e-02,  3.97103559e-03,  3.14745354e-03, ...,\n",
            "         -1.89182162e-02,  1.97978262e-02, -1.82628110e-02],\n",
            "        [-2.21782438e-02, -2.23413054e-02,  6.40212838e-03, ...,\n",
            "         -3.23817763e-03,  1.11558912e-02,  1.31652877e-03],\n",
            "        [ 1.29670938e-02,  3.96818854e-02, -2.47360393e-02, ...,\n",
            "          3.05608986e-03, -1.23573141e-02, -2.34122667e-02]],\n",
            "\n",
            "       [[ 3.93721368e-03, -6.22343505e-03, -1.67135075e-02, ...,\n",
            "         -2.02565989e-03,  3.27886902e-02,  2.10023820e-02],\n",
            "        [ 8.53436359e-04, -2.69363192e-03,  2.13482548e-02, ...,\n",
            "          1.60068683e-02,  3.48903448e-03, -1.74190477e-02],\n",
            "        [-1.84255149e-02,  1.29095570e-03,  3.12763005e-02, ...,\n",
            "         -1.35626504e-02,  1.64682213e-02,  3.23714502e-02],\n",
            "        ...,\n",
            "        [-9.55464505e-03,  1.41280890e-02, -3.93667491e-03, ...,\n",
            "          1.79092295e-03,  7.48779718e-03,  1.53351696e-02],\n",
            "        [-1.11390622e-02, -5.94576169e-03, -6.59489864e-03, ...,\n",
            "          4.98601189e-03, -1.27253318e-02, -3.48562486e-02],\n",
            "        [-1.07522542e-02, -2.80369655e-03, -1.58854127e-02, ...,\n",
            "         -2.24374384e-02,  5.65494690e-03,  1.71667971e-02]],\n",
            "\n",
            "       ...,\n",
            "\n",
            "       [[-1.17936796e-02, -2.24961415e-02, -1.33291585e-02, ...,\n",
            "         -7.93317147e-03, -1.80994142e-02, -1.17330644e-02],\n",
            "        [-1.17922146e-02, -9.62985586e-03, -5.94212022e-03, ...,\n",
            "          7.00848736e-03, -1.93863083e-02, -1.21886982e-02],\n",
            "        [ 4.30417527e-03, -7.63789751e-04,  7.00544799e-03, ...,\n",
            "         -2.37230062e-02,  1.09573733e-02, -5.08598760e-07],\n",
            "        ...,\n",
            "        [ 4.85895362e-05, -1.36023099e-02,  4.49510245e-03, ...,\n",
            "          3.42504531e-02,  4.18175617e-03,  1.00832600e-02],\n",
            "        [ 7.87730236e-03,  9.20936093e-03,  2.30921078e-02, ...,\n",
            "          2.68299691e-02,  7.73138925e-03, -6.55751210e-03],\n",
            "        [ 6.30382739e-04,  3.57231125e-02,  1.85214486e-02, ...,\n",
            "         -3.00486386e-02,  1.75537113e-02,  2.40657851e-02]],\n",
            "\n",
            "       [[ 4.38686321e-03, -9.84153803e-03,  5.16536226e-03, ...,\n",
            "          6.59687794e-04,  2.69337669e-02, -9.59488272e-04],\n",
            "        [-1.49631279e-03, -3.72978928e-03,  2.14357227e-02, ...,\n",
            "         -1.04941465e-02, -2.88516823e-02, -1.08845886e-02],\n",
            "        [ 1.60050262e-02, -1.37291653e-02,  1.06426664e-02, ...,\n",
            "          8.39753356e-03,  5.38459513e-03, -3.03354143e-04],\n",
            "        ...,\n",
            "        [-2.77683474e-02, -2.23048441e-02, -1.86795443e-02, ...,\n",
            "          1.94649193e-02,  6.20374130e-03,  2.31030658e-02],\n",
            "        [-1.52403973e-02,  1.93622406e-03,  1.18264873e-02, ...,\n",
            "          5.37120877e-03,  8.33409838e-04, -2.00413540e-02],\n",
            "        [-1.56240887e-03,  8.64176638e-03, -1.27826855e-02, ...,\n",
            "          2.40251627e-02, -2.09068749e-02, -6.74094167e-03]],\n",
            "\n",
            "       [[ 5.22170402e-03, -4.59499611e-03, -1.14999749e-02, ...,\n",
            "          8.83076154e-03,  2.43443083e-02, -4.31963801e-03],\n",
            "        [ 1.60264745e-02,  3.36838048e-03,  1.98274720e-02, ...,\n",
            "          1.17232557e-02,  7.45004835e-03,  9.91942734e-03],\n",
            "        [-3.07072531e-02,  1.08166803e-02, -4.39245580e-03, ...,\n",
            "         -1.03275692e-02, -1.10353325e-02, -1.19550759e-02],\n",
            "        ...,\n",
            "        [-1.18021220e-02,  1.67080446e-03,  3.74088921e-02, ...,\n",
            "         -1.30825266e-02, -1.29558919e-02,  1.84876844e-02],\n",
            "        [ 7.74208037e-03,  9.28160176e-03, -3.21971998e-03, ...,\n",
            "          8.87963641e-03, -1.79704260e-02,  3.15604638e-03],\n",
            "        [-3.03003788e-02, -2.01226287e-02,  1.34916771e-02, ...,\n",
            "         -1.86498761e-02, -2.85586412e-03,  3.67923379e-02]]],\n",
            "      dtype=float32)>, <tf.Variable 'transformer/layer_11/self_attention_layer_norm/gamma:0' shape=(768,) dtype=float32, numpy=\n",
            "array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1.], dtype=float32)>, <tf.Variable 'transformer/layer_2/self_attention/value/bias:0' shape=(12, 64) dtype=float32, numpy=\n",
            "array([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
            "      dtype=float32)>, <tf.Variable 'transformer/layer_7/output/kernel:0' shape=(3072, 768) dtype=float32, numpy=\n",
            "array([[-0.01191127, -0.01174582, -0.00026439, ..., -0.00143017,\n",
            "         0.02532438,  0.00861738],\n",
            "       [ 0.00489793,  0.03165279, -0.01015916, ..., -0.01861534,\n",
            "        -0.01184009, -0.00109204],\n",
            "       [-0.01319231, -0.00894991,  0.00356911, ..., -0.00640343,\n",
            "        -0.00182406, -0.02171696],\n",
            "       ...,\n",
            "       [-0.02222435, -0.00060672, -0.00296086, ...,  0.00268318,\n",
            "         0.01141246, -0.00361022],\n",
            "       [-0.01534252,  0.02298198, -0.00202476, ...,  0.00884899,\n",
            "         0.00154356,  0.00416722],\n",
            "       [ 0.01007786, -0.00187043,  0.0126686 , ...,  0.02125293,\n",
            "        -0.01610312, -0.01733059]], dtype=float32)>, <tf.Variable 'transformer/layer_10/self_attention/value/bias:0' shape=(12, 64) dtype=float32, numpy=\n",
            "array([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
            "      dtype=float32)>, <tf.Variable 'transformer/layer_8/output_layer_norm/beta:0' shape=(768,) dtype=float32, numpy=\n",
            "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0.], dtype=float32)>, <tf.Variable 'transformer/layer_1/output/kernel:0' shape=(3072, 768) dtype=float32, numpy=\n",
            "array([[ 0.02889059, -0.01270026, -0.00349018, ...,  0.03161564,\n",
            "         0.01298964,  0.01468888],\n",
            "       [-0.01045816,  0.01199845, -0.01447204, ..., -0.00989028,\n",
            "        -0.0284503 , -0.02125826],\n",
            "       [ 0.00292209,  0.01976826,  0.00503027, ..., -0.02638194,\n",
            "         0.02708321,  0.00595144],\n",
            "       ...,\n",
            "       [ 0.02019107,  0.00683083, -0.03992076, ...,  0.01330451,\n",
            "        -0.00927108, -0.02412904],\n",
            "       [-0.00671431, -0.01237864, -0.0101228 , ..., -0.02258772,\n",
            "        -0.00944722, -0.00750388],\n",
            "       [-0.02099977, -0.02993535, -0.00349645, ..., -0.03224445,\n",
            "        -0.00406329, -0.02277105]], dtype=float32)>, <tf.Variable 'transformer/layer_7/output_layer_norm/beta:0' shape=(768,) dtype=float32, numpy=\n",
            "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0.], dtype=float32)>, <tf.Variable 'transformer/layer_4/self_attention/query/kernel:0' shape=(768, 12, 64) dtype=float32, numpy=\n",
            "array([[[-1.27319619e-02, -2.45462079e-02, -5.67097543e-03, ...,\n",
            "         -2.08375547e-02, -9.53064021e-03, -1.39138484e-02],\n",
            "        [ 9.25902650e-03, -1.81856509e-02, -8.33847001e-03, ...,\n",
            "         -4.16389620e-03, -3.39853344e-03,  3.39168101e-03],\n",
            "        [ 1.43641494e-02, -3.69989425e-02,  2.48526428e-02, ...,\n",
            "         -8.11837986e-03, -3.06128170e-02, -3.19946222e-02],\n",
            "        ...,\n",
            "        [ 2.14043446e-02,  1.11035174e-02, -9.18617006e-03, ...,\n",
            "          1.64390225e-02, -2.71054800e-03,  2.73659732e-02],\n",
            "        [-2.13886192e-03,  5.59779932e-04,  3.29030183e-04, ...,\n",
            "          1.17566623e-03, -1.96680380e-03,  5.25486469e-03],\n",
            "        [ 1.23308199e-02, -3.09567228e-02, -4.39317431e-03, ...,\n",
            "          3.46892863e-03, -9.19340964e-05, -2.83000246e-02]],\n",
            "\n",
            "       [[ 5.15068229e-03, -1.46113979e-02, -1.66256528e-03, ...,\n",
            "         -2.59541497e-02,  7.19188014e-03, -2.44379342e-02],\n",
            "        [ 2.12489208e-03,  1.04645751e-02, -2.75223535e-02, ...,\n",
            "          1.25070000e-02, -2.09435336e-02,  9.11824405e-03],\n",
            "        [ 1.97710171e-02,  5.87972812e-03, -1.56274121e-02, ...,\n",
            "         -1.22301548e-03,  2.00192444e-02,  3.73144336e-02],\n",
            "        ...,\n",
            "        [ 1.20807858e-02, -2.34665275e-02,  3.23471688e-02, ...,\n",
            "         -1.73967034e-02, -2.44698394e-02, -1.05869863e-02],\n",
            "        [ 1.59495343e-02, -2.62664817e-02, -1.33830095e-02, ...,\n",
            "          1.83040835e-02, -3.19327116e-02, -2.16767192e-02],\n",
            "        [ 1.35356619e-03,  2.11603567e-02, -3.46291810e-02, ...,\n",
            "         -1.48509508e-02,  5.23980195e-03,  3.39849219e-02]],\n",
            "\n",
            "       [[ 5.22589684e-03,  1.63484234e-02,  7.42287561e-03, ...,\n",
            "         -1.29540460e-02,  2.73581930e-02,  6.01978274e-04],\n",
            "        [-8.21859110e-03, -6.79469155e-03, -2.62774415e-02, ...,\n",
            "         -3.82052325e-02,  3.74662504e-02,  1.54649727e-02],\n",
            "        [-7.36479787e-03, -1.63240004e-02, -4.53504268e-03, ...,\n",
            "         -1.11728013e-02,  3.87366414e-02, -3.53192911e-03],\n",
            "        ...,\n",
            "        [-3.54772806e-02, -2.12647747e-02, -3.34022418e-02, ...,\n",
            "          2.28388142e-02,  2.46533323e-02,  2.61645485e-02],\n",
            "        [-2.85774842e-02,  2.87825288e-03,  1.74304023e-02, ...,\n",
            "         -1.81521308e-02, -1.25814080e-02,  3.20597962e-02],\n",
            "        [-1.33864516e-02,  5.58751216e-03, -8.01992044e-03, ...,\n",
            "         -1.50753921e-02, -4.20324923e-03,  2.47318437e-03]],\n",
            "\n",
            "       ...,\n",
            "\n",
            "       [[ 1.17645180e-02, -1.74268447e-02, -1.17562264e-02, ...,\n",
            "          7.19026476e-03, -2.67659556e-02,  3.67133804e-02],\n",
            "        [ 2.67249197e-02, -2.22766609e-03, -3.08797099e-02, ...,\n",
            "          1.17411735e-02, -1.39071385e-03,  7.28765642e-03],\n",
            "        [ 1.86658255e-03, -1.27911381e-02, -2.03507841e-02, ...,\n",
            "         -6.19202713e-03,  7.43893720e-03, -1.29088908e-02],\n",
            "        ...,\n",
            "        [-3.47260572e-02, -9.38840490e-03,  2.01089494e-02, ...,\n",
            "         -2.11087335e-03, -4.35827998e-03, -2.73468578e-03],\n",
            "        [-3.25873028e-03, -3.06758843e-02, -1.46849472e-02, ...,\n",
            "          2.55845375e-02, -7.20940856e-03,  1.70105870e-03],\n",
            "        [-1.60526466e-02, -1.96672678e-02, -3.69693451e-02, ...,\n",
            "         -1.99425854e-02, -2.68420540e-02,  2.70356294e-02]],\n",
            "\n",
            "       [[-2.41287947e-02,  3.06307413e-02,  4.23125969e-03, ...,\n",
            "         -5.49057778e-03,  4.93066432e-03,  1.97872836e-02],\n",
            "        [-1.17926265e-03, -3.55090015e-03, -1.02030020e-02, ...,\n",
            "         -7.02509377e-03, -1.30665386e-02,  3.30037461e-03],\n",
            "        [-7.36703444e-03, -1.74631812e-02,  1.16258245e-02, ...,\n",
            "          1.40706189e-02,  1.46573142e-03, -2.20477842e-02],\n",
            "        ...,\n",
            "        [-1.45900557e-02,  3.33775952e-02,  1.23616890e-04, ...,\n",
            "          4.96360334e-03,  1.59967020e-02,  2.82675419e-02],\n",
            "        [-2.69421954e-02, -1.69939380e-02,  2.23144647e-02, ...,\n",
            "         -1.09225127e-03, -1.44652352e-02,  1.21721681e-02],\n",
            "        [ 9.35913995e-03, -1.36763025e-02,  2.22837068e-02, ...,\n",
            "         -2.08794661e-02,  2.87580956e-02, -1.83304455e-02]],\n",
            "\n",
            "       [[ 2.25893594e-02, -3.23594664e-04, -2.17627585e-02, ...,\n",
            "         -2.06338260e-02,  4.32121148e-03,  1.29459007e-02],\n",
            "        [ 7.79828336e-03, -1.99125288e-03, -6.74245460e-03, ...,\n",
            "         -3.06131020e-02, -2.74593546e-03, -1.14284763e-02],\n",
            "        [ 1.69237275e-02, -6.84643211e-03, -3.96556174e-03, ...,\n",
            "         -1.84415188e-02,  1.42044863e-02,  1.81215871e-02],\n",
            "        ...,\n",
            "        [ 1.61978137e-02,  2.15952266e-02,  1.26243168e-02, ...,\n",
            "         -1.36993285e-02, -1.60288028e-02,  4.89707431e-03],\n",
            "        [ 3.08720600e-02,  3.47067565e-02, -3.12237963e-02, ...,\n",
            "          3.11796684e-02,  2.78821439e-02, -5.86944399e-03],\n",
            "        [ 2.98087439e-03, -2.41218740e-03, -2.16190554e-02, ...,\n",
            "         -3.10849734e-02, -1.04815997e-02, -2.33502500e-02]]],\n",
            "      dtype=float32)>, <tf.Variable 'transformer/layer_3/self_attention/query/kernel:0' shape=(768, 12, 64) dtype=float32, numpy=\n",
            "array([[[ 3.41645069e-02, -2.14624786e-04,  3.22964182e-03, ...,\n",
            "          1.22528644e-02,  1.04206940e-02, -1.76669601e-02],\n",
            "        [ 1.84806846e-02, -9.61898640e-03,  1.67730637e-02, ...,\n",
            "          1.94856226e-02,  3.51967686e-03, -8.91323388e-03],\n",
            "        [-2.10590214e-02, -6.99245604e-03, -2.10762536e-03, ...,\n",
            "         -7.85859651e-04, -2.03518104e-02,  1.27307242e-02],\n",
            "        ...,\n",
            "        [ 1.34231901e-04, -4.29894822e-03,  7.39005301e-03, ...,\n",
            "          1.10762501e-02, -3.28461938e-02, -9.81129240e-03],\n",
            "        [-2.68737357e-02,  1.28416643e-02,  7.13509554e-03, ...,\n",
            "         -3.24373227e-03,  2.81519983e-02, -2.66570970e-02],\n",
            "        [-3.39238928e-03,  1.66263469e-02, -3.01001337e-03, ...,\n",
            "         -1.41048841e-02, -2.25427486e-02, -3.54259298e-03]],\n",
            "\n",
            "       [[ 1.03307627e-02, -2.28775516e-02, -6.55593816e-03, ...,\n",
            "          8.54706950e-03,  2.09018011e-02, -1.33970343e-02],\n",
            "        [ 1.93431024e-02, -2.38934765e-03,  3.39744403e-03, ...,\n",
            "         -2.68159956e-02,  1.18832244e-02, -1.29579259e-02],\n",
            "        [-3.65366484e-03,  3.07907374e-03,  7.59195909e-03, ...,\n",
            "         -1.42236874e-02, -8.17715889e-04,  1.17426580e-02],\n",
            "        ...,\n",
            "        [ 9.12673306e-03, -1.02914674e-02, -1.70105009e-03, ...,\n",
            "          1.33434392e-03,  2.08526254e-02,  2.99213473e-02],\n",
            "        [ 3.24501586e-03, -2.96383202e-02, -1.67592708e-02, ...,\n",
            "          1.64027754e-02, -2.80432752e-03, -6.13938691e-03],\n",
            "        [ 2.53330654e-04, -1.78391337e-02, -7.99851771e-03, ...,\n",
            "         -2.21708301e-03, -4.98592202e-03, -5.95032750e-03]],\n",
            "\n",
            "       [[-8.31811223e-03, -1.50340945e-02, -4.45894850e-03, ...,\n",
            "         -2.40520593e-02,  1.40299536e-02,  2.01304164e-02],\n",
            "        [ 6.42949855e-03, -1.02483751e-02,  1.52054382e-02, ...,\n",
            "         -1.86257195e-02, -2.51782476e-03,  2.15517115e-02],\n",
            "        [ 1.41005320e-02, -2.08192002e-02, -7.42524222e-04, ...,\n",
            "         -1.02083404e-02, -1.44476769e-03, -3.51372059e-03],\n",
            "        ...,\n",
            "        [-1.33749396e-02,  2.85178013e-02, -1.39505439e-03, ...,\n",
            "         -1.74821913e-02,  1.48851273e-03,  7.35060265e-03],\n",
            "        [ 1.12453522e-02, -2.80829072e-02, -1.26192151e-02, ...,\n",
            "          1.92824844e-02,  2.75883079e-02,  3.61736026e-03],\n",
            "        [-3.96204367e-03,  3.01340665e-03, -1.85580812e-02, ...,\n",
            "          2.01324672e-02,  2.32681725e-02, -1.67122092e-02]],\n",
            "\n",
            "       ...,\n",
            "\n",
            "       [[ 3.52162383e-02,  6.36477442e-03, -2.11367907e-04, ...,\n",
            "          6.50114240e-03, -1.75902881e-02,  3.57549116e-02],\n",
            "        [ 2.38666385e-02, -1.33215757e-02, -8.07537697e-03, ...,\n",
            "          8.20767693e-03, -3.17951739e-02, -2.15758681e-02],\n",
            "        [-2.70687304e-02, -3.35202459e-03, -2.66529224e-03, ...,\n",
            "         -5.19344630e-03,  1.29115181e-02,  1.06107341e-02],\n",
            "        ...,\n",
            "        [ 6.93851057e-03,  4.93379962e-03,  4.32017114e-04, ...,\n",
            "         -8.75042100e-03, -1.96401384e-02, -5.72147255e-04],\n",
            "        [ 3.07004284e-02, -2.48164330e-02,  1.68776140e-02, ...,\n",
            "         -1.32025895e-03,  2.16591656e-02, -8.82581901e-03],\n",
            "        [ 9.83329490e-03, -3.25497426e-02,  7.75137777e-03, ...,\n",
            "          9.51181818e-03, -6.92352047e-03,  1.69924404e-02]],\n",
            "\n",
            "       [[ 2.63723861e-02,  1.76290367e-02,  2.47401595e-02, ...,\n",
            "          1.30613018e-02,  3.48238871e-02,  1.21873152e-02],\n",
            "        [-3.33223515e-03, -9.71895177e-03, -5.66144940e-03, ...,\n",
            "         -1.20664081e-02,  1.96175035e-02,  8.98659695e-03],\n",
            "        [-2.60026753e-02, -3.06516234e-02, -2.41326950e-02, ...,\n",
            "          1.21943364e-02,  1.96289122e-02,  3.30950916e-02],\n",
            "        ...,\n",
            "        [-1.58340652e-02,  7.25831557e-03,  1.32876346e-02, ...,\n",
            "         -6.19459152e-03,  3.11448681e-03,  4.52510430e-04],\n",
            "        [ 2.40390580e-02, -3.51492614e-02,  1.46711282e-02, ...,\n",
            "          3.38571146e-02, -3.89652513e-02,  8.35456140e-03],\n",
            "        [ 9.92149653e-05,  2.39832746e-03,  1.64481252e-02, ...,\n",
            "          1.82483729e-03, -2.03387812e-02,  2.90391094e-04]],\n",
            "\n",
            "       [[-7.76699279e-03,  1.85513794e-02, -1.24319345e-02, ...,\n",
            "          1.35617191e-02,  1.98247135e-02, -1.56620257e-02],\n",
            "        [ 9.30520240e-03, -1.09715678e-03,  2.75573321e-02, ...,\n",
            "         -1.57027412e-02,  2.12924313e-02, -5.41015202e-03],\n",
            "        [ 4.86039976e-03,  7.50411488e-03, -1.97318606e-02, ...,\n",
            "         -5.78790950e-03, -1.24286385e-02, -1.76588595e-02],\n",
            "        ...,\n",
            "        [ 2.98582334e-02, -2.40460001e-02, -1.15361391e-02, ...,\n",
            "         -1.81579869e-02, -9.84757557e-04, -1.15018198e-02],\n",
            "        [ 5.98955806e-03, -6.70684269e-03, -4.49411236e-05, ...,\n",
            "          1.66612566e-02,  2.12266352e-02, -1.06218942e-02],\n",
            "        [ 2.25943625e-02, -2.16615647e-02,  8.34785588e-03, ...,\n",
            "         -1.24428105e-02,  1.45440334e-02, -1.94095727e-02]]],\n",
            "      dtype=float32)>, <tf.Variable 'transformer/layer_10/self_attention/key/bias:0' shape=(12, 64) dtype=float32, numpy=\n",
            "array([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
            "      dtype=float32)>, <tf.Variable 'transformer/layer_8/self_attention/value/kernel:0' shape=(768, 12, 64) dtype=float32, numpy=\n",
            "array([[[-9.59453359e-03, -1.08594550e-02,  2.21454576e-02, ...,\n",
            "         -2.49770321e-02, -1.78398527e-02,  5.40644629e-03],\n",
            "        [ 1.36458622e-02,  2.07346007e-02,  3.65239196e-02, ...,\n",
            "          1.44071679e-03, -1.39013736e-03,  2.13283934e-02],\n",
            "        [ 2.35192627e-02, -2.02633496e-02, -7.62643805e-03, ...,\n",
            "         -8.46302719e-04, -1.73756182e-02, -3.65747064e-02],\n",
            "        ...,\n",
            "        [ 1.28319003e-02,  1.57930749e-03, -2.78679207e-02, ...,\n",
            "         -9.11396090e-03, -1.94618024e-03, -2.84739528e-02],\n",
            "        [-4.01090784e-03, -1.01191802e-02, -2.84461901e-02, ...,\n",
            "         -1.69332772e-02, -1.01191364e-02, -3.10629942e-02],\n",
            "        [ 4.51211166e-03, -1.58874728e-02,  3.56054641e-02, ...,\n",
            "         -1.90282445e-02, -2.06095316e-02, -2.55847350e-03]],\n",
            "\n",
            "       [[-6.30509667e-03,  3.66043635e-02,  7.02926191e-03, ...,\n",
            "         -1.47253321e-02,  2.49590129e-02, -1.07388701e-02],\n",
            "        [ 9.32529941e-03,  1.51507696e-02,  1.09588541e-02, ...,\n",
            "         -2.58095395e-02,  2.56416593e-02, -8.99658538e-03],\n",
            "        [ 2.81381272e-02,  6.01749029e-03, -7.43762497e-03, ...,\n",
            "         -6.24107942e-03,  2.42101513e-02,  2.22022124e-02],\n",
            "        ...,\n",
            "        [ 1.85783766e-02, -1.65095869e-02,  6.82113896e-05, ...,\n",
            "         -5.39514050e-03,  1.37969814e-02,  3.14143524e-02],\n",
            "        [-6.09110994e-03,  2.12971643e-02, -2.58762352e-02, ...,\n",
            "         -2.10209750e-02, -1.73049178e-02,  2.10323278e-02],\n",
            "        [ 1.23540582e-02,  1.41024301e-02, -1.58638842e-02, ...,\n",
            "         -1.77739076e-02,  1.43785654e-02, -2.80499756e-02]],\n",
            "\n",
            "       [[-8.32721032e-03, -1.23968720e-02,  1.51807186e-03, ...,\n",
            "          1.55875366e-02, -2.02539912e-03,  8.54655262e-03],\n",
            "        [ 3.18277581e-03,  2.12536659e-03, -3.98332365e-02, ...,\n",
            "          2.60003489e-02,  4.00303677e-03, -2.71467734e-02],\n",
            "        [-2.08320115e-02,  2.71260925e-02,  2.81430501e-02, ...,\n",
            "          3.76657583e-02,  3.60081196e-02,  2.74291560e-02],\n",
            "        ...,\n",
            "        [-2.27938537e-02,  1.80333331e-02, -3.80575880e-02, ...,\n",
            "         -1.19087147e-02, -1.34772714e-02,  1.47033865e-02],\n",
            "        [-1.21132471e-02, -2.77320687e-02,  3.96752643e-04, ...,\n",
            "          1.75848119e-02, -3.92363071e-02, -1.32566690e-03],\n",
            "        [ 1.56055233e-02, -3.19790952e-02, -1.63063016e-02, ...,\n",
            "         -1.60207711e-02, -3.12588573e-03, -3.03532928e-02]],\n",
            "\n",
            "       ...,\n",
            "\n",
            "       [[-1.43785886e-02,  2.14535673e-03, -3.28754215e-03, ...,\n",
            "          8.32115766e-03,  1.51190371e-03,  1.29546504e-02],\n",
            "        [ 1.29296659e-02, -1.09818869e-03, -3.79799632e-03, ...,\n",
            "         -3.02085653e-02, -1.10163055e-02,  7.39920943e-04],\n",
            "        [-3.30239832e-02,  2.69403253e-02,  1.81098022e-02, ...,\n",
            "          3.29679139e-02,  3.48677188e-02,  2.02286933e-02],\n",
            "        ...,\n",
            "        [-7.64668966e-03, -1.93661824e-02,  3.45478393e-02, ...,\n",
            "          1.41109191e-02,  8.13681260e-03,  3.85337807e-02],\n",
            "        [ 2.09032744e-02,  9.87118948e-03, -1.18065495e-02, ...,\n",
            "         -8.29241890e-03,  7.85201322e-04, -9.73022077e-03],\n",
            "        [ 5.62332990e-03,  8.66978150e-03,  4.38692281e-03, ...,\n",
            "          2.46986449e-02,  3.56596224e-02, -4.22905991e-03]],\n",
            "\n",
            "       [[ 8.63247644e-03, -1.45909274e-02, -1.12431152e-02, ...,\n",
            "         -1.39469868e-02,  3.39713693e-02, -1.74684096e-02],\n",
            "        [-2.20003892e-02, -5.32906083e-03,  6.56415988e-03, ...,\n",
            "         -1.11121107e-02,  5.25406189e-03,  1.22615919e-02],\n",
            "        [-2.42475979e-02,  2.02470794e-02, -1.66519713e-02, ...,\n",
            "          2.90527269e-02,  1.39266867e-02, -1.50263356e-02],\n",
            "        ...,\n",
            "        [-1.73576828e-02,  1.74389128e-02, -4.88193333e-03, ...,\n",
            "         -1.08177969e-02,  2.81884801e-02,  2.61694379e-02],\n",
            "        [ 2.78002098e-02,  2.86681410e-02, -1.73478201e-02, ...,\n",
            "         -2.50088014e-02, -1.30480621e-02,  7.34613324e-03],\n",
            "        [ 3.51084620e-02,  8.44682474e-03, -5.22999046e-03, ...,\n",
            "          8.75253347e-04, -9.19971522e-03, -1.08436691e-02]],\n",
            "\n",
            "       [[-8.09326302e-03, -1.43557237e-02,  2.40580717e-04, ...,\n",
            "          1.81221943e-02, -1.82823685e-03,  3.02266446e-03],\n",
            "        [-1.50825065e-02, -2.12016795e-02, -5.11776982e-03, ...,\n",
            "         -1.88653090e-03,  2.19898690e-02, -1.15534766e-02],\n",
            "        [ 2.52271257e-02,  7.63149175e-04,  1.81802139e-02, ...,\n",
            "         -1.63539872e-02,  2.72562429e-02, -9.15475469e-03],\n",
            "        ...,\n",
            "        [ 2.68503539e-02, -1.33669134e-02, -1.91636160e-02, ...,\n",
            "          2.94638798e-02, -7.74062192e-03, -5.31769404e-03],\n",
            "        [ 2.59760953e-02,  3.85913663e-02, -7.24202814e-03, ...,\n",
            "         -5.08023566e-03,  2.10967865e-02,  1.07319877e-02],\n",
            "        [ 1.28467763e-02,  7.08956830e-03,  2.27226354e-02, ...,\n",
            "          1.59434881e-02,  2.12699715e-02,  3.51795442e-02]]],\n",
            "      dtype=float32)>, <tf.Variable 'transformer/layer_6/output_layer_norm/gamma:0' shape=(768,) dtype=float32, numpy=\n",
            "array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1.], dtype=float32)>, <tf.Variable 'transformer/layer_6/intermediate/kernel:0' shape=(768, 3072) dtype=float32, numpy=\n",
            "array([[ 0.01374928,  0.0027433 , -0.02709134, ..., -0.03008121,\n",
            "         0.01669941, -0.01938249],\n",
            "       [ 0.02644299, -0.00058434,  0.01359891, ...,  0.0324789 ,\n",
            "         0.01153434, -0.01586422],\n",
            "       [-0.03522141, -0.00422468, -0.03025248, ...,  0.03853311,\n",
            "        -0.00346082,  0.00041918],\n",
            "       ...,\n",
            "       [ 0.02122233, -0.01431246, -0.02148616, ..., -0.00165696,\n",
            "        -0.01814157, -0.00557863],\n",
            "       [ 0.02049199,  0.01474764, -0.00665512, ..., -0.00542627,\n",
            "         0.02262868, -0.03381642],\n",
            "       [-0.01722709,  0.03676417, -0.00308934, ...,  0.00608156,\n",
            "        -0.01147934,  0.02563346]], dtype=float32)>, <tf.Variable 'transformer/layer_1/self_attention/value/bias:0' shape=(12, 64) dtype=float32, numpy=\n",
            "array([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
            "      dtype=float32)>, <tf.Variable 'transformer/layer_8/output/bias:0' shape=(768,) dtype=float32, numpy=\n",
            "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0.], dtype=float32)>, <tf.Variable 'transformer/layer_2/self_attention/query/bias:0' shape=(12, 64) dtype=float32, numpy=\n",
            "array([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
            "      dtype=float32)>, <tf.Variable 'transformer/layer_10/self_attention/query/bias:0' shape=(12, 64) dtype=float32, numpy=\n",
            "array([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
            "      dtype=float32)>, <tf.Variable 'transformer/layer_11/self_attention_layer_norm/beta:0' shape=(768,) dtype=float32, numpy=\n",
            "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0.], dtype=float32)>, <tf.Variable 'transformer/layer_11/self_attention_output/bias:0' shape=(768,) dtype=float32, numpy=\n",
            "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0.], dtype=float32)>, <tf.Variable 'transformer/layer_2/self_attention_layer_norm/gamma:0' shape=(768,) dtype=float32, numpy=\n",
            "array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "       1., 1., 1.], dtype=float32)>]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jphe0AYidv5j",
        "colab_type": "text"
      },
      "source": [
        "# **Action Item 2**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GPG1-8JcZvZg",
        "colab_type": "text"
      },
      "source": [
        "# Run predictions using the fine-tuned model with external test set from Proxzar"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PbqiifJUYv_3",
        "colab_type": "text"
      },
      "source": [
        "Predictions were done in another Colab file as running with the current version of **run_squad.py** from the *Tensorflow official* github repo is throwing attribute errors. I've to switch the official repo to *kamalkraj's github repo* that has modified version of **run_squad.py** file.\n",
        "The colab URL is: [Roll#2019900090 Project - Predictions](https://colab.research.google.com/drive/1rAwGpApMhJeOkoePWMO-rZS9dfMoQOQd#scrollTo=t4PVfgAGuida)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Gi67iTsdoi2",
        "colab_type": "text"
      },
      "source": [
        "# Performance Metrics Of Fine-tuned Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yw505n9za0Na",
        "colab_type": "text"
      },
      "source": [
        "On SQuAD 1.1 dev set, the fine-tuned model performed as measured with the below metrics:\n",
        "**EM score**: **79.60** , \n",
        "**F1 score**: **87.47**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N3gNUV5zd_yr",
        "colab_type": "text"
      },
      "source": [
        "# **Action Item 3**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "7uDdcwpGeSZk"
      },
      "source": [
        "# Generate qualitative comparision report with existing implementation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xa-JmFftcDMq",
        "colab_type": "text"
      },
      "source": [
        "Even after switching to different github repo that purpotedly addresses the issues with Tensorflow's official **run_squad.py**, there are still issues with the new repo files in *writing the predictions to disk*.\n",
        "\n",
        "Hence, could not generate performance summary report on the final predictions to compare this fine-tuned BERT model with Proxzar's current implementation that uses Stanford Core NLP and IIITB Research Labs SIML framework. "
      ]
    }
  ]
}